{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d07a2b",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d359699",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ae8f9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95155ae7-020e-48f9-bad1-53f3000e2228",
   "metadata": {},
   "source": [
    "# Continuous Optimization (Chapter 7: MML Book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813359e-2d5b-48a9-bf5b-c5bd5ec4a0b8",
   "metadata": {},
   "source": [
    "Latex macros\n",
    "$$\n",
    "\\newcommand{\\calA}{{\\cal A}}\n",
    "\\newcommand{\\calB}{{\\cal B}}\n",
    "\\newcommand{\\calC}{{\\cal C}}\n",
    "\\newcommand{\\calD}{{\\cal D}}\n",
    "\\newcommand{\\calE}{{\\cal E}}\n",
    "\\newcommand{\\calF}{{\\cal F}}\n",
    "\\newcommand{\\calG}{{\\cal G}}\n",
    "\\newcommand{\\calH}{{\\cal H}}\n",
    "\\newcommand{\\calI}{{\\cal I}}\n",
    "\\newcommand{\\calJ}{{\\cal J}}\n",
    "\\newcommand{\\calK}{{\\cal K}}\n",
    "\\newcommand{\\calL}{{\\cal L}}\n",
    "\\newcommand{\\calM}{{\\cal M}}\n",
    "\\newcommand{\\calN}{{\\cal N}}\n",
    "\\newcommand{\\calO}{{\\cal O}}\n",
    "\\newcommand{\\calP}{{\\cal P}}\n",
    "\\newcommand{\\calQ}{{\\cal Q}}\n",
    "\\newcommand{\\calR}{{\\cal R}}\n",
    "\\newcommand{\\calS}{{\\cal S}}\n",
    "\\newcommand{\\calT}{{\\cal T}}\n",
    "\\newcommand{\\calU}{{\\cal U}}\n",
    "\\newcommand{\\calV}{{\\cal V}}\n",
    "\\newcommand{\\calW}{{\\cal W}}\n",
    "\\newcommand{\\calX}{{\\cal X}}\n",
    "\\newcommand{\\calY}{{\\cal Y}}\n",
    "\\newcommand{\\calZ}{{\\cal Z}}\n",
    "\\newcommand{\\setA}{\\textsf{A}}\n",
    "\\newcommand{\\setB}{\\textsf{B}}\n",
    "\\newcommand{\\setC}{\\textsf{C}}\n",
    "\\newcommand{\\setD}{\\textsf{D}}\n",
    "\\newcommand{\\setE}{\\textsf{E}}\n",
    "\\newcommand{\\setF}{\\textsf{F}}\n",
    "\\newcommand{\\setG}{\\textsf{G}}\n",
    "\\newcommand{\\setH}{\\textsf{H}}\n",
    "\\newcommand{\\setI}{\\textsf{I}}\n",
    "\\newcommand{\\setJ}{\\textsf{J}}\n",
    "\\newcommand{\\setK}{\\textsf{K}}\n",
    "\\newcommand{\\setL}{\\textsf{L}}\n",
    "\\newcommand{\\setM}{\\textsf{M}}\n",
    "\\newcommand{\\setN}{\\textsf{N}}\n",
    "\\newcommand{\\setO}{\\textsf{O}}\n",
    "\\newcommand{\\setP}{\\textsf{P}}\n",
    "\\newcommand{\\setQ}{\\textsf{Q}}\n",
    "\\newcommand{\\setR}{\\textsf{R}}\n",
    "\\newcommand{\\setS}{\\textsf{S}}\n",
    "\\newcommand{\\setT}{\\textsf{T}}\n",
    "\\newcommand{\\setU}{\\textsf{U}}\n",
    "\\newcommand{\\setV}{\\textsf{V}}\n",
    "\\newcommand{\\setW}{\\textsf{W}}\n",
    "\\newcommand{\\setX}{\\textsf{X}}\n",
    "\\newcommand{\\setY}{\\textsf{Y}}\n",
    "\\newcommand{\\setZ}{\\textsf{Z}}\n",
    "\\newcommand{\\bfa}{\\mathbf{a}}\n",
    "\\newcommand{\\bfb}{\\mathbf{b}}\n",
    "\\newcommand{\\bfc}{\\mathbf{c}}\n",
    "\\newcommand{\\bfd}{\\mathbf{d}}\n",
    "\\newcommand{\\bfe}{\\mathbf{e}}\n",
    "\\newcommand{\\bff}{\\mathbf{f}}\n",
    "\\newcommand{\\bfg}{\\mathbf{g}}\n",
    "\\newcommand{\\bfh}{\\mathbf{h}}\n",
    "\\newcommand{\\bfi}{\\mathbf{i}}\n",
    "\\newcommand{\\bfj}{\\mathbf{j}}\n",
    "\\newcommand{\\bfk}{\\mathbf{k}}\n",
    "\\newcommand{\\bfl}{\\mathbf{l}}\n",
    "\\newcommand{\\bfm}{\\mathbf{m}}\n",
    "\\newcommand{\\bfn}{\\mathbf{n}}\n",
    "\\newcommand{\\bfo}{\\mathbf{o}}\n",
    "\\newcommand{\\bfp}{\\mathbf{p}}\n",
    "\\newcommand{\\bfq}{\\mathbf{q}}\n",
    "\\newcommand{\\bfr}{\\mathbf{r}}\n",
    "\\newcommand{\\bfs}{\\mathbf{s}}\n",
    "\\newcommand{\\bft}{\\mathbf{t}}\n",
    "\\newcommand{\\bfu}{\\mathbf{u}}\n",
    "\\newcommand{\\bfv}{\\mathbf{v}}\n",
    "\\newcommand{\\bfw}{\\mathbf{w}}\n",
    "\\newcommand{\\bfx}{\\mathbf{x}}\n",
    "\\newcommand{\\bfy}{\\mathbf{y}}\n",
    "\\newcommand{\\bfz}{\\mathbf{z}}\n",
    "\\newcommand{\\bfalpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\bfgamma}{\\boldsymbol{\\gamma}}\n",
    "\\newcommand{\\bfdelta}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\bfepsilon}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\bfzeta}{\\boldsymbol{\\zeta}}\n",
    "\\newcommand{\\bfeta}{\\boldsymbol{\\eta}}\n",
    "\\newcommand{\\bftheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\bfiota}{\\boldsymbol{\\iota}}\n",
    "\\newcommand{\\bfkappa}{\\boldsymbol{\\kappa}}\n",
    "\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\bfmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\bfnu}{\\boldsymbol{\\nu}}\n",
    "\\newcommand{\\bfomicron}{\\boldsymbol{\\omicron}}\n",
    "\\newcommand{\\bfpi}{\\boldsymbol{\\pi}}\n",
    "\\newcommand{\\bfrho}{\\boldsymbol{\\rho}}\n",
    "\\newcommand{\\bfsigma}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\bftau}{\\boldsymbol{\\tau}}\n",
    "\\newcommand{\\bfupsilon}{\\boldsymbol{\\upsilon}}\n",
    "\\newcommand{\\bfphi}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\bfpsi}{\\boldsymbol{\\psi}}\n",
    "\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n",
    "\\newcommand{\\bfxi}{\\boldsymbol{\\xi}}\n",
    "\\newcommand{\\bfell}{\\boldsymbol{\\ell}}\n",
    "\\newcommand{\\bfA}{\\mathbf{A}}\n",
    "\\newcommand{\\bfB}{\\mathbf{B}}\n",
    "\\newcommand{\\bfC}{\\mathbf{C}}\n",
    "\\newcommand{\\bfD}{\\mathbf{D}}\n",
    "\\newcommand{\\bfE}{\\mathbf{E}}\n",
    "\\newcommand{\\bfF}{\\mathbf{F}}\n",
    "\\newcommand{\\bfG}{\\mathbf{G}}\n",
    "\\newcommand{\\bfH}{\\mathbf{H}}\n",
    "\\newcommand{\\bfI}{\\mathbf{I}}\n",
    "\\newcommand{\\bfJ}{\\mathbf{J}}\n",
    "\\newcommand{\\bfK}{\\mathbf{K}}\n",
    "\\newcommand{\\bfL}{\\mathbf{L}}\n",
    "\\newcommand{\\bfM}{\\mathbf{M}}\n",
    "\\newcommand{\\bfN}{\\mathbf{N}}\n",
    "\\newcommand{\\bfO}{\\mathbf{O}}\n",
    "\\newcommand{\\bfP}{\\mathbf{P}}\n",
    "\\newcommand{\\bfQ}{\\mathbf{Q}}\n",
    "\\newcommand{\\bfR}{\\mathbf{R}}\n",
    "\\newcommand{\\bfS}{\\mathbf{S}}\n",
    "\\newcommand{\\bfT}{\\mathbf{T}}\n",
    "\\newcommand{\\bfU}{\\mathbf{U}}\n",
    "\\newcommand{\\bfV}{\\mathbf{V}}\n",
    "\\newcommand{\\bfW}{\\mathbf{W}}\n",
    "\\newcommand{\\bfX}{\\mathbf{X}}\n",
    "\\newcommand{\\bfY}{\\mathbf{Y}}\n",
    "\\newcommand{\\bfZ}{\\mathbf{Z}}\n",
    "\\newcommand{\\bfGamma}{\\boldsymbol{\\Gamma}}\n",
    "\\newcommand{\\bfDelta}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\bfTheta}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\bfPi}{\\boldsymbol{\\Pi}}\n",
    "\\newcommand{\\bfSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\bfUpsilon}{\\boldsymbol{\\Upsilon}}\n",
    "\\newcommand{\\bfPhi}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\bfPsi}{\\boldsymbol{\\Psi}}\n",
    "\\newcommand{\\bfOmega}{\\boldsymbol{\\Omega}}\n",
    "\\newcommand{\\bbA}{\\mathbb{A}}\n",
    "\\newcommand{\\bbB}{\\mathbb{B}}\n",
    "\\newcommand{\\bbC}{\\mathbb{C}}\n",
    "\\newcommand{\\bbD}{\\mathbb{D}}\n",
    "\\newcommand{\\bbE}{\\mathbb{E}}\n",
    "\\newcommand{\\bbF}{\\mathbb{F}}\n",
    "\\newcommand{\\bbG}{\\mathbb{G}}\n",
    "\\newcommand{\\bbH}{\\mathbb{H}}\n",
    "\\newcommand{\\bbI}{\\mathbb{I}}\n",
    "\\newcommand{\\bbJ}{\\mathbb{J}}\n",
    "\\newcommand{\\bbK}{\\mathbb{K}}\n",
    "\\newcommand{\\bbL}{\\mathbb{L}}\n",
    "\\newcommand{\\bbM}{\\mathbb{M}}\n",
    "\\newcommand{\\bbN}{\\mathbb{N}}\n",
    "\\newcommand{\\bbO}{\\mathbb{O}}\n",
    "\\newcommand{\\bbP}{\\mathbb{P}}\n",
    "\\newcommand{\\bbQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\bbR}{\\mathbb{R}}\n",
    "\\newcommand{\\bbS}{\\mathbb{S}}\n",
    "\\newcommand{\\bbT}{\\mathbb{T}}\n",
    "\\newcommand{\\bbU}{\\mathbb{U}}\n",
    "\\newcommand{\\bbV}{\\mathbb{V}}\n",
    "\\newcommand{\\bbW}{\\mathbb{W}}\n",
    "\\newcommand{\\bbX}{\\mathbb{X}}\n",
    "\\newcommand{\\bbY}{\\mathbb{Y}}\n",
    "\\newcommand{\\bbZ}{\\mathbb{Z}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e3a7c-51d7-434b-a531-6bb694e72375",
   "metadata": {},
   "source": [
    "### Recall geometry of a derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c9129-79c3-4977-9458-eb91821082e9",
   "metadata": {},
   "source": [
    "![](imgs/geometry-of-derivative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fc205-f7f6-4798-abac-e1564b009ede",
   "metadata": {},
   "source": [
    "![](imgs/minimas-of-non-convex-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0af884-e63e-4ac3-b28f-4cec292a662e",
   "metadata": {},
   "source": [
    "## Minimizing general functions\n",
    "\n",
    "We cannot minimize general functions by solving\n",
    "\n",
    "$$ \\frac{\\p f(\\bfx)}{\\p \\bfx} = \\mathbf{0}^\\top$$ \n",
    "\n",
    "because the equation might not have a formula for it.\n",
    "\n",
    "Instead we use iterative methods like gradient descent minimize general function $f(\\bfx)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f7b4b5-82f3-4673-a3ba-8b41f5169961",
   "metadata": {},
   "source": [
    "#### Definition (Directional derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c92135-827b-43ee-b211-95e3041b58cc",
   "metadata": {},
   "source": [
    "$\\newcommand{\\p}{\\partial}$\n",
    "\n",
    "Directional dervivative of a function $f(\\bfx) : \\bbR^n \\to \\bbR$ with respect to a given vector $\\bfu \\in \\bbR$ is defined as\n",
    "\n",
    "$$ D_\\bfu f(\\bfx) = \\lim_{\\epsilon \\to 0} \\frac{f(\\bfx + \\epsilon \\bfu) - f(\\bfx)}{\\epsilon} $$\n",
    "\n",
    "[Ref Khan Academy](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/directional-derivative-introduction)\n",
    "\n",
    "[Ref Libretexts](https://math.libretexts.org/Bookshelves/Calculus/Calculus_3e_(Apex)/12%3A_Functions_of_Several_Variables/12.06%3A_Directional_Derivatives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5d5c1-832c-4956-8381-c480a2ccba23",
   "metadata": {},
   "source": [
    "##### Vector calculus chain rule (a theorem)\n",
    "\n",
    "Given a function composition $\\bff(\\bfx) = \\bfg(\\bfh(\\bfx)) = (\\bfg \\circ \\bfh)(\\bfx)$ where $\\bfh : \\bbR^n \\to \\bbR^m$, $\\bfg : \\bbR^m \\to \\bbR^p$ and $\\bfh: \\bbR^m \\to \\bbR^p$\n",
    "\n",
    "$$ \\frac{\\p \\bff}{\\p \\bfx} = \\frac{\\p \\bff}{\\p \\bfh}\\frac{\\p \\bfh}{\\p \\bfx} $$\n",
    "or denoting the derivatives as Jacobian matrices we have,\n",
    "$$ \\calJ_\\bfx \\bff = \\calJ_\\bfh[\\bff] \\calJ_\\bfx [ \\bfh ] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13321db-38bf-4c2c-9916-c04ff741e356",
   "metadata": {},
   "source": [
    "##### Theorem (Directional derivative is gradient dot product with the direction)\n",
    "\n",
    "Express the trajectory in the direction $\\bfu$ as a function of time $t$ as\n",
    "$$\\bfg(t) = \\bfx + t \\bfu $$\n",
    "Note that the Jacobian of $\\bfg(t)$ wrt $t$ is simply $\\bfu$,\n",
    "$$\\calJ_t \\bfg(t) = \\bfu$$\n",
    "Recall the definition of directional derivative,\n",
    "$$ D_\\bfu f(\\bfx) = \\lim_{\\epsilon \\to 0} \\frac{f(\\bfx + \\epsilon \\bfu) - f(\\bfx)}{\\epsilon}. $$\n",
    "\n",
    "Compare it with the derivative of $f(\\bfg(t))$ with respect to $t$ at $t=0$\n",
    "$$ \\frac{\\p f(\\bfg(t))}{\\p t} = \\lim_{\\epsilon \\to 0} \\frac{f(\\bfx + (t + \\epsilon) \\bfu) - f(\\bfx + t\\bfu)}{\\epsilon} \\Big\\vert_{t = 0}.$$\n",
    "$$ \\frac{\\p f(\\bfg(t))}{\\p t} = \\lim_{\\epsilon \\to 0} \\frac{f(\\bfx + \\epsilon \\bfu) - f(\\bfx )}{\\epsilon} = D_\\bfu f(\\bfx).$$\n",
    "\n",
    "We can compute $\\frac{\\p f(\\bfg(t))}{\\p t}$ by chain rule,\n",
    "\n",
    "$$  D_\\bfu f(\\bfx) = \\calJ_t f(\\bfg(t)) = \\calJ_\\bfx f(\\bfx) \\calJ_t \\bfg = \\nabla_\\bfx^\\top f(\\bfx) \\bfu$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880212ea-3c7b-4464-8a8e-ca4b407033d4",
   "metadata": {},
   "source": [
    "#### Theorem : The direction of stepest ascent and descent\n",
    "\n",
    "Let $\\hat{\\bfu}$ be of unit magnitude. The directional derivative represents how the function changes in the direction $\\hat{\\bfu}$.\n",
    "\n",
    "$$ D_{\\hat{\\bfu}} f(\\bfx) =  \\nabla_\\bfx^\\top f(\\bfx) \\hat{\\bfu} = \\|\\nabla_\\bfx f(\\bfx)\\|\\cos(\\theta),$$ \n",
    "where $\\theta$ is the angle between $\\nabla_\\bfx f(\\bfx)$ and $\\hat{\\bfu}$. The change is maximum when $\\theta = 0$ and $\\cos(\\theta) = 1$ and the change is minimum when $\\theta = 180^\\circ$ and $\\cos(\\theta) = -1$. \n",
    "\n",
    "In other words the function $f$ increases the most (stepest ascent) when $\\hat{\\bfu}  \\propto \\nabla_\\bfx f(\\bfx)$ and decreases the most (steepest descent) when $\\hat{\\bfu} \\propto - \\nabla_\\bfx f(\\bfx)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2aebd-bf43-4f7a-b367-0d07be189134",
   "metadata": {},
   "source": [
    "### Gradient descent method\n",
    "(Section 9.3 of [Convex Optimization by Stephen Boyd and Lieven Vandenberghe](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09e5b2-847a-449d-b2c9-af2c73ea1058",
   "metadata": {},
   "source": [
    "\n",
    "1. Start from a random point $\\bfx_0$, $\\bfx_t \\leftarrow \\bfx_0$.\n",
    "2. Move in the direction opposite to $\\nabla_\\bfx f(\\bfx)$. If we were at $\\bfx_t$, then the next point is at $\\bfx_{t+1} = \\bfx_t - \\alpha_t \\nabla_\\bfx f(\\bfx)$, where $\\alpha_t > 0$ is a positive scalar, called the step size or the learning rate.\n",
    "3. Stop when the gradient is almost zero $\\|\\nabla_\\bfx f(\\bfx)\\| < 10^{-4}$.\n",
    "\n",
    "This corresponds to the following algorithm:\n",
    "\n",
    "$\\bfx_t = \\bfx_0$<br/>\n",
    "while ($\\|\\nabla_\\bfx f(\\bfx)\\| > 10^{-4}$) {\n",
    "    $\\bfx_t \\leftarrow \\bfx_t - \\alpha_t \\nabla_\\bfx f(\\bfx)$\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008bce4c-b5b1-4680-85da-03147f044596",
   "metadata": {},
   "source": [
    "Algorithm 9.3 Gradient descent method.\n",
    "\n",
    "**given** a starting point $\\bfx \\in \\text{dom}{f}$ .\n",
    "\n",
    "**repeat**\n",
    "1. $\\Delta x = -\\nabla f(\\bfx)$.\n",
    "2. Choose step size $\\alpha$ \n",
    "3. Update. $\\bfx := \\bfx + \\alpha \\bfx$\n",
    "\n",
    "**until** stopping criterion is satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3735d6-83d2-403a-beee-b49141e5bb74",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/f/ff/Gradient_descent.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a1c26-f575-45aa-848b-3b6a09e4afa3",
   "metadata": {},
   "source": [
    "\n",
    "<video src=\"https://upload.wikimedia.org/wikipedia/commons/transcoded/4/4c/Gradient_Descent_in_2D.webm/Gradient_Descent_in_2D.webm.720p.vp9.webm\"  controls>\n",
    "\tThis is fallback content to display for user agents that do not support the video tag.\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c053c-a7d2-4efa-a685-2ccc05e91f6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9157c1-debb-4641-9e78-4bfab56b0e1d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import animation, rc\n",
    "rc('animation', html='jshtml')\n",
    "def plot_gradients(func, gradfunc):\n",
    "    x, y = np.mgrid[-3:3:21j,\n",
    "                    -3:3:21j]\n",
    "    bfx = np.array([x, y])\n",
    "    f = func(x,y)\n",
    "    [dfdx, dfdy] = gradfunc(x,y)\n",
    "    fig, ax = plt.subplots()\n",
    "    ctr = ax.contour(x, y, f, 20, cmap='Blues_r')\n",
    "    ax.quiver(x, y, dfdx, dfdy)\n",
    "    ax.plot([0], [0], 'ro') \n",
    "    ax.text(0, 0, '$x^*$', color='r')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5349132-4673-4bc0-be9f-f94b41189d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def f(x, y): return  x**2 + y**2\n",
    "def gradf(x, y): return 2*x, 2*y\n",
    "plot_gradients(f, gradf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79bb11-1522-4a07-ae5d-07ff890a9992",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quadratic function example:\n",
    "\n",
    "A quadratic function $f(\\bfx) = x_1^2 + x_2^2$ has level sets as circles:\n",
    "$$ S(f, c) = \\{\\bfx : x_1^2 + x_2^2 = c\\} $$\n",
    "\n",
    "It has the parameteric form as\n",
    "$$ S(f, c) = \\{\\bfg(c, \\theta) = \\begin{bmatrix}\\sqrt{c} \\cos(\\theta) \\\\ \\sqrt{c} \\sin(\\theta)\\end{bmatrix} : \\theta \\in [0, 2\\pi)\\} $$\n",
    "\n",
    "The gradient is:\n",
    "$$ \\nabla_\\bfx f(\\bfx)^\\top = 2\\bfx^\\top \n",
    "= 2 \\begin{bmatrix} \\sqrt{c} \\cos(\\theta) & \\sqrt{c} \\sin(\\theta)\\end{bmatrix}$$\n",
    "\n",
    "and \n",
    "\n",
    "The derivative of curve with respect to $\\theta$ the tangent to the curve:\n",
    "$$\\calJ_\\theta \\bfg(c, \\theta) \n",
    "= \\begin{bmatrix} \\frac{\\p }{\\p \\theta} \\sqrt{c} \\cos(\\theta) \\\\ \\frac{\\p }{\\p \\theta} \\sqrt{c} \\sin(\\theta)\\end{bmatrix} \n",
    "= \\begin{bmatrix}-\\sqrt{c} \\sin(\\theta) \\\\ \\sqrt{c} \\cos(\\theta)\\end{bmatrix}$$\n",
    "\n",
    "$$ \\nabla_\\bfx f(\\bfx)^\\top\\calJ_\\theta \\bfg(c, \\theta) \n",
    "= 2 \\begin{bmatrix} \\sqrt{c} \\cos(\\theta) & \\sqrt{c} \\sin(\\theta)\\end{bmatrix}\n",
    "\\begin{bmatrix}-\\sqrt{c} \\sin(\\theta) \\\\ \\sqrt{c} \\cos(\\theta)\\end{bmatrix} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94294b-80e5-4f66-b804-136f8aa73767",
   "metadata": {},
   "source": [
    "### Taylor series approximation \n",
    "#### A bit more about the step size/learning rate\n",
    "\n",
    "Recall the Taylor series expansion of a function $f(x)$ around $x_0$\n",
    "\n",
    "$$f(x) = f(x_0) + \\frac{d f(x)}{d x} (x - x_0) + \\frac{1}{2!} \\frac{d^2 f(x)}{d x^2} (x - x_0)^2 + \\dots + \\frac{1}{n!} \\frac{d^n f(x)}{d x^n} (x - x_0)^n + \\dots $$\n",
    "\n",
    "Vectorized Taylor series expansion is\n",
    "\n",
    "$$f(\\bfx) = f(\\bfx_0) + \\nabla_\\bfx^\\top f(\\bfx) (\\bfx - \\bfx_0) + \\frac{1}{2!}  (\\bfx - \\bfx_0)^\\top \\calH f(\\bfx) (\\bfx - \\bfx_0)^\\top + \\dots \\infty$$\n",
    "\n",
    "While optimizing around the point $\\bfx_t$, we can use the Taylor series expansion to find a local quadratic approximation to find the next best minima:\n",
    "\n",
    "$$\\hat{f}(\\bfx_{t+1}) = f(\\bfx_t) + \\nabla_\\bfx^\\top f(\\bfx) (\\bfx_{t+1} - \\bfx_t) + \\frac{1}{2!}  (\\bfx_{t+1} - \\bfx_t)^\\top \\calH f(\\bfx) (\\bfx_{t+1} - \\bfx_t)^\\top $$\n",
    "\n",
    "At the optimal point of the quadratic approximation the derivative is zero:\n",
    "$$\\frac{\\p \\hat{f}(\\bfx_{t+1})}{\\p \\bfx_{t+1}} = \\mathbf{0}^\\top$$\n",
    "$$\\nabla_\\bfx^\\top f(\\bfx) + (\\bfx_{t+1} - \\bfx_t)^\\top \\calH  f(\\bfx) = \\mathbf{0}^\\top$$\n",
    "\n",
    "Taking transpose and rearranging the terms we get:\n",
    "$$ \\bfx_{t+1} - \\bfx_t = -[\\calH  f(\\bfx)]^{-1}\\nabla_\\bfx f(\\bfx) $$\n",
    "$$ \\bfx_{t+1} = \\bfx_t -[\\calH  f(\\bfx)]^{-1}\\nabla_\\bfx f(\\bfx) $$\n",
    "\n",
    "This is the update rule for the Newton's method for optimization. Note that the step size here is inversely proportional the second derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757b3de-ead1-48ce-bb7c-c07316670077",
   "metadata": {},
   "source": [
    "### Taylor series approximation\n",
    "\n",
    "Approximate the following function to a quadratic function near the point $\\bfx_0 = [-2, 3]$\n",
    "\n",
    "$f (x) = 0.06\\exp( 2x_1 +x_2) + 0.05\\exp(x_1−2 x_2) + \\exp(−x_1) $\n",
    "\n",
    "\n",
    "$$ f(\\bfx) = 0.06 \\exp([2, 1]\\bfx) + 0.05 \\exp([1, -2]\\bfx) + \\exp([-1, 0]\\bfx)$$\n",
    "\n",
    "$\\newcommand{\\bfb}{\\mathbf{b}}$\n",
    "Let $\\bfb_1^\\top = [2, 1]$, $\\bfb_2^\\top = [1, -2]$, $\\bfb_3^\\top = [-1, 0]$.\n",
    "\n",
    "$$\\nabla_\\bfx^\\top f(\\bfx) = 0.06 \\bfb_1^\\top \\exp(\\bfb_1^\\top\\bfx) + 0.05 \\bfb_2^\\top\\exp(\\bfb_2^\\top\\bfx) + \\bfb_3^\\top\\exp(\\bfb_3^\\top\\bfx)$$\n",
    "\n",
    "$$\\nabla_\\bfx f(\\bfx) = 0.06 \\bfb_1 \\exp(\\bfb_1^\\top\\bfx) + 0.05 \\bfb_2 \\exp(\\bfb_2^\\top\\bfx) + \\bfb_3\\exp(\\bfb_3^\\top\\bfx)$$\n",
    "\n",
    "$$H f(\\bfx) = \\nabla_\\bfx^\\top (\\nabla_\\bfx f(\\bfx)) = 0.06 \\bfb_1 \\exp(\\bfb_1^\\top\\bfx) \\bfb_1^\\top + 0.05 \\bfb_2 \\exp(\\bfb_2^\\top\\bfx) \\bfb_2^\\top + \\bfb_3\\exp(\\bfb_3^\\top\\bfx)\\bfb_3^\\top $$\n",
    "\n",
    "$$H f(\\bfx) =  0.06 \\exp(\\bfb_1^\\top\\bfx) \\bfb_1 \\bfb_1^\\top + 0.05 \\exp(\\bfb_2^\\top\\bfx) \\bfb_2 \\bfb_2^\\top + \\exp(\\bfb_3^\\top\\bfx)\\bfb_3\\bfb_3^\\top $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc06e3-d42a-4b56-aeee-22cbef18c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    1. For an input x of shape x.shape = (2,) \n",
    "        f(x) must return a scalar\n",
    "        \n",
    "    2. For an input x of shape x.shape = (m, 2), \n",
    "        f(x) must return an array of shape (m,) \n",
    "        which contains f(x) is computed for m values of x\n",
    "\n",
    "    3. For an input x of shape x.shape = (m, n, 2)\n",
    "        f(x) must return an array of shape (m, n) \n",
    "        which contains f(x) is computed for (m x n) values of x\n",
    "    \"\"\"\n",
    "    return (0.06 * np.exp(x @ [2, 1]) \n",
    "            + 0.05* np.exp(x @ [1, -2]) \n",
    "            + np.exp(x @ [-1, 0]))\n",
    "\n",
    "# Compute its derivative, the gradient function\n",
    "def grad_f(x):     \n",
    "    \"\"\"\n",
    "    1. For an input x of shape x.shape = (2,) \n",
    "        grad_f(x) must return a n array of shape (2,)\n",
    "        \n",
    "    2. For an input x of shape x.shape = (m, 2), \n",
    "        grad_f(x) must return an array of shape (m, 2) \n",
    "        which contains grad_f(x) is computed for m values of x\n",
    "\n",
    "    3. For an input x of shape x.shape = (m, n, 2)\n",
    "        grad_f(x) must return an array of shape (m, n, 2) \n",
    "        which contains grad_f(x) is computed for (m x n) values of x\n",
    "    \"\"\"\n",
    "    coeff1 = np.array([2, 1])\n",
    "    coeff2 = np.array([1, -2])\n",
    "    coeff3 = np.array([-1, 0])\n",
    "    # Slicing using np.newaxis or None, increases the dimension by 1.\n",
    "    # https://numpy.org/doc/stable/reference/constants.html#numpy.newaxis\n",
    "    return (0.06 * np.exp(x @ coeff1)[..., None] * coeff1\n",
    "            + 0.05 * np.exp(x @ coeff2)[..., None] * coeff2\n",
    "            + np.exp(x @ coeff3)[..., None] * coeff3)\n",
    "\n",
    "def numerical_jacobian(f, x, h=1e-10):\n",
    "    n = x.shape[-1]\n",
    "    eye = np.eye(n)\n",
    "    x_plus_dx = x + h * eye # n x n\n",
    "    num_jac = (f(x_plus_dx) - f(x)) / h # limit definition of the formula # n x m\n",
    "    if num_jac.ndim >= 2:\n",
    "        num_jac = num_jac.swapaxes(-1, -2) # m x n\n",
    "    return num_jac\n",
    "    \n",
    "# Compare our grad_f with numerical gradient\n",
    "def check_numerical_jacobian(f, jac_f,  nD=2, **kwargs):\n",
    "    x = np.random.rand(nD)\n",
    "    num_jac = numerical_jacobian(f, x, **kwargs)\n",
    "    return np.allclose(num_jac, jac_f(x), atol=1e-06, rtol=1e-4) # m x n\n",
    "\n",
    "## Throw error if grad_f is wrong\n",
    "assert check_numerical_jacobian(f, grad_f)\n",
    "\n",
    "## Gradient of gradient\n",
    "def hessian_f(x):\n",
    "    \"\"\"\n",
    "    1. For an input x of shape x.shape = (2,) \n",
    "        hessian_f(x) must return a n array of shape (2, 2)\n",
    "        \n",
    "    2. For an input x of shape x.shape = (m, 2), \n",
    "        hessian_f(x) must return an array of shape (m, 2, 2) \n",
    "        which contains hessian_f(x) is computed for m values of x\n",
    "\n",
    "    3. For an input x of shape x.shape = (m, n, 2)\n",
    "        hessian_f(x) must return an array of shape (m, n, 2, 2) \n",
    "        which contains hessian_f(x) is computed for (m x n) values of x \n",
    "    \"\"\"\n",
    "    coeff1 = np.array([2, 1])\n",
    "    coeff2 = np.array([1, -2])\n",
    "    coeff3 = np.array([-1, 0])\n",
    "    return (0.06 * np.exp(x @ coeff1)[..., None, None] * np.outer(coeff1, coeff1)\n",
    "            + 0.05 * np.exp(x @ coeff2)[..., None, None] * np.outer(coeff2, coeff2)\n",
    "            +  np.exp(x @ coeff3)[..., None, None] * np.outer(coeff3, coeff3))\n",
    "\n",
    "## Throw error if hessian_f is wrong\n",
    "assert check_numerical_jacobian(grad_f, hessian_f)\n",
    "\n",
    "\n",
    "def taylor_series_quad_approx(x0, func, grad_func, hessian_func):\n",
    "    def quad_func(x):\n",
    "        x_min_x0 = (x-x0)[..., None] # make column vectors\n",
    "        x_min_x0_T = (x-x0)[..., None, :] # make row vectors\n",
    "        grad_f_x0_T = grad_func(x0)[..., None, :] # make row vectors\n",
    "        return (func(x0) \n",
    "                + grad_f_x0_T @ x_min_x0\n",
    "                + x_min_x0_T @ hessian_func(x0) @ x_min_x0).squeeze(axis=(-1,-2))\n",
    "            \n",
    "    return quad_func\n",
    "\n",
    "def plot_contours(func, ax=None, cmap='Blues_r', levels=20, \n",
    "                  xrange=slice(-3,3,21j),\n",
    "                  yrange=slice(-3,3,21j)):\n",
    "    x, y = np.mgrid[xrange,\n",
    "                    yrange]\n",
    "    bfx = np.concatenate([x[..., None],\n",
    "                          y[..., None]], axis=-1)\n",
    "    f = func(bfx)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ctr = ax.contour(x, y, np.log(f), levels, cmap=cmap)\n",
    "    ax.clabel(ctr, ctr.levels, inline=True, fontsize=6)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    return ax\n",
    "    \n",
    "# Fit a quadratic curve around this curve:\n",
    "ax = plot_contours(f, \n",
    "                   levels=[0.01, 0.5, 0.8, 1.0, 1.2, 1.6, 1.8, 2.0])\n",
    "x0 = np.array([0, 2])\n",
    "ax.plot([x0[0]], [x0[1]], 'ro')\n",
    "ax.text(x0[0], x0[1], '$x_0$')\n",
    "quad_func = taylor_series_quad_approx(x0, f, grad_f, hessian_f)\n",
    "# x, y = np.mgrid[-3:3:21j,\n",
    "#                     -3:3:21j]\n",
    "# bfx = np.concatenate([x[..., None],\n",
    "#                       y[..., None]], axis=-1)\n",
    "# print(bfx.shape)\n",
    "# print(f(bfx).shape)\n",
    "# print(grad_f(bfx).shape)\n",
    "# print(quad_func(bfx).shape)\n",
    "plot_contours(quad_func, ax=ax, cmap='Reds_r', \n",
    "              levels=[0.1, 0.2, 0.5, 0.8, 1.0, 1.5],\n",
    "              xrange=slice(-1,2,21j),\n",
    "              yrange=slice(-2,3,21j))\n",
    "ax.set_title(\"Taylor series quadratic fit in red to the blue curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c45deb-f819-499f-8621-303a05d607fc",
   "metadata": {},
   "source": [
    "#### Example Taylor series 1\n",
    "\n",
    "Following the example above, fit a quadratic function to the function using Taylor series expansion near the points $\\bfx_0 = \\begin{bmatrix} -1 \\\\ 1\\end{bmatrix}$ then visualize the contour plots of the original function and the quadratic function (50 marks),\n",
    "\n",
    "$$f(\\bfx) = x_1 \\exp(-(x_1^2 + x_2^2)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b258eb4-c467-4e33-866c-eef8fe9f903b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "305256c6007607eb7ac224e48bb3bb17",
     "grade": false,
     "grade_id": "cell-4dccb852958c96b5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b7d1d5-d1be-4afd-a8da-533bd6eafc0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Minimization by gradient descent\n",
    "\n",
    "#### Example 1 : minimization by gradient descent\n",
    "\n",
    "Find the minimizer of $f (\\bfx) = 0.06\\exp( 2x_1 +x_2) + 0.05\\exp(x_1−2 x_2) + \\exp(−x_1) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a269cf-42e3-49be-8b12-2996fd2a0267",
   "metadata": {},
   "source": [
    "In vector form we can write it as:\n",
    "\n",
    "$$f (\\bfx) = 0.06\\exp( [2, 1] \\bfx ) + 0.05\\exp([1, -2] \\bfx) + \\exp([-1, 0] \\bfx) $$\n",
    "$$ \\nabla_\\bfx^\\top f(\\bfx) = 0.06\\exp( [2, 1] \\bfx )[2, 1] + 0.05\\exp([1, -2] \\bfx)[1, -2] + \\exp([-1, 0] \\bfx) [-1, 0] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c70f1-1a21-4de1-97c2-b853c1375568",
   "metadata": {},
   "source": [
    "Algorithm 9.3 Gradient descent method.\n",
    "\n",
    "**given** a starting point $\\bfx \\in \\text{dom}{f}$ .\n",
    "\n",
    "**repeat**\n",
    "1. Choose step size $\\alpha_t$ \n",
    "2. Update. $\\bfx := \\bfx  - \\alpha_t \\nabla f(\\bfx) $\n",
    "\n",
    "**until** stopping criterion is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e4615-78f2-48f9-9d4f-4fe5648a9f34",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(x):\n",
    "    return (0.06 * np.exp(x @ [2, 1]) \n",
    "            + 0.05* np.exp(x @ [1, -2]) \n",
    "            + np.exp(x @ [-1, 0]))\n",
    "\n",
    "# Compute its derivative, the gradient function\n",
    "def grad_f(x):    \n",
    "    coeff1 = np.array([2, 1])\n",
    "    coeff2 = np.array([1, -2])\n",
    "    coeff3 = np.array([-1, 0])\n",
    "    # Slicing using None, increases the dimension by 1.\n",
    "    # \n",
    "    return (0.06 * np.exp(x @ coeff1)[..., None] * coeff1\n",
    "            + 0.05 * np.exp(x @ coeff2)[..., None] * coeff2\n",
    "            + np.exp(x @ coeff3)[..., None] * coeff3)\n",
    "\n",
    "\n",
    "assert check_numerical_jacobian(f, grad_f)\n",
    "f(np.zeros((2,))), grad_f(np.zeros((2,))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787633f2-43b9-48de-bb79-0484222f6d44",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_gradients(func, gradfunc):\n",
    "    x, y = np.mgrid[-3:3:21j,\n",
    "                    -3:3:21j]\n",
    "    bfx = np.concatenate((x[..., None], y[..., None]), axis=-1)\n",
    "    f = func(bfx)\n",
    "    dfdx = gradfunc(bfx)\n",
    "    fig, ax = plt.subplots()\n",
    "    ctr = ax.contour(x, y, np.log(f), 20, cmap='Blues_r')\n",
    "    ax.quiver(bfx[..., 0], bfx[..., 1], dfdx[..., 0], dfdx[..., 1])\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.axis('equal')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e140853-3483-4e3b-adf8-730f64e0bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradients(f, grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36194560-aee0-4669-9268-7023feccf968",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the gradient descent algorithm\n",
    "def minimize(x0, f, grad_func, alpha_t=0.2, maxiter=100):\n",
    "    t = 0\n",
    "    xt = x0\n",
    "    grad_f_t = grad_func(xt)\n",
    "    list_of_xts, list_of_fs = [xt], [f(xt)] # for logging\n",
    "    while np.linalg.norm(grad_f_t) > 1e-4: # <-- Check for convergence\n",
    "        xt = xt - alpha_t * grad_f_t # <-- Main update step\n",
    "        grad_f_t = grad_func(xt) # Compute the next gradient\n",
    "        \n",
    "        if t >= maxiter:  # Failsafe, if the algorithm does not converge\n",
    "            break\n",
    "        else:\n",
    "            t += 1\n",
    "        list_of_xts.append(xt) # for logging\n",
    "        list_of_fs.append(f(xt)) # for logging\n",
    "    \n",
    "    return xt, list_of_xts, list_of_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825ce50-4174-4941-ae16-161863d26302",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x0 = np.array([-2, 2])\n",
    "#x0 = np.random.rand(2,2) * 4 - 2\n",
    "OPTIMAL_X, list_of_xts, list_of_fs = minimize(x0,\n",
    "                                              f, grad_f, \n",
    "                                              alpha_t=0.2,\n",
    "                                              maxiter=1000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(list_of_fs)\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864c26f-d9f5-40c4-872b-7a3dad7837ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "rc('animation', html='jshtml')\n",
    "\n",
    "class Anim:\n",
    "    def __init__(self, fig, ax, func):\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "        x, y = np.mgrid[-3:3:21j,\n",
    "                        -3:3:21j]\n",
    "        bfx = np.concatenate((x[..., None], y[..., None]), axis=-1)\n",
    "        f = func(bfx)\n",
    "        self.ctr = self.ax.contour(x, y, np.log(f), 20, cmap='Blues_r')\n",
    "        self.ax.set_xlabel('x')\n",
    "        self.ax.set_ylabel('y')\n",
    "        #self.ax.clabel(self.ctr, self.ctr.levels, inline=True, fontsize=6)\n",
    "        self.list_of_xs = []\n",
    "        self.list_of_ys = []\n",
    "        self.line2, = self.ax.plot([], [], 'r*-')\n",
    "\n",
    "        \n",
    "    def anim_init(self):\n",
    "        return (self.line2,)\n",
    "        \n",
    "    def update(self, xt):\n",
    "        self.list_of_xs.append(xt[0])\n",
    "        self.list_of_ys.append(xt[1])\n",
    "        self.line2.set_data(self.list_of_xs, self.list_of_ys)\n",
    "        return self.line2,\n",
    "    \n",
    "fig, ax = plt.subplots()        \n",
    "a = Anim(fig, ax, f)\n",
    "animation.FuncAnimation(fig, a.update, frames=list_of_xts[::5],\n",
    "                        init_func=a.anim_init, blit=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2fdc8-2d5f-47af-b90c-ac7ae2b62661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate can be sensitive to the starting points. \n",
    "# Observe the behavior for different starting points\n",
    "# For different starting points you might need different learning rate scheme\n",
    "\n",
    "x0 = np.random.rand(2) * 6 - 3\n",
    "OPTIMAL_X, list_of_xts, list_of_fs = minimize(x0,\n",
    "                                              f, grad_f, \n",
    "                                              alpha_t=0.2,\n",
    "                                              maxiter=200)\n",
    "fig, ax = plt.subplots()        \n",
    "a = Anim(fig, ax, f)\n",
    "animation.FuncAnimation(fig, a.update, frames=list_of_xts[::10],\n",
    "                        init_func=a.anim_init, blit=True, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7405f05-779b-4d27-9ddf-b686f72b0a61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Homework 4: Problem 3 (20 marks)\n",
    "\n",
    "Following the example above, *implement your own* gradient descent algorithm that minimizes the following function (50 marks),\n",
    "\n",
    "$$f(\\bfx) = x_1 \\exp(-(x_1^2 + x_2^2)) $$\n",
    "\n",
    "Test your algorithm with the starting points of $\\bfx_0 = \\begin{bmatrix} -1 \\\\ 1\\end{bmatrix}$ and $\\bfx_0 = \\begin{bmatrix} -1 \\\\ -1\\end{bmatrix}$ and learning rate of $\\alpha_t = 0.25$. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457a057-3968-4d44-902c-45653b93336a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76a1ae78bf015c1fa89cbc761288c8ee",
     "grade": false,
     "grade_id": "cell-62edd01f5331c45a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x0 = np.array([-1, 1])\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def grad_f(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "## Throw error if grad_f is wrong\n",
    "assert check_numerical_jacobian(f, grad_f)\n",
    "\n",
    "# Implement the gradient descent algorithm\n",
    "def minimize(x0, f, grad_func, alpha_t=0.2, maxiter=100):\n",
    "    t = 0\n",
    "    xt = x0\n",
    "    grad_f_t = grad_func(xt)\n",
    "    list_of_xts, list_of_fs = [xt], [f(xt)] # for logging\n",
    "    while np.linalg.norm(grad_f_t) > 1e-4: # <-- Check for convergence\n",
    "        # 1. Update xt using gradient descent update\n",
    "        # 2. Compute grad_f_t with new xt\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        if t >= maxiter:  # Failsafe, if the algorithm does not converge\n",
    "            break\n",
    "        else:\n",
    "            t += 1\n",
    "        list_of_xts.append(xt) # for logging\n",
    "        list_of_fs.append(f(xt)) # for logging\n",
    "    \n",
    "    return xt, list_of_xts, list_of_fs\n",
    "\n",
    "\n",
    "OPTIMAL_X, list_of_xts, list_of_fs = minimize(x0,\n",
    "                                              f, grad_f, \n",
    "                                              alpha_t=0.25,\n",
    "                                              maxiter=200)\n",
    "\n",
    "class Anim:\n",
    "    def __init__(self, fig, ax, func):\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "        x, y = np.mgrid[-2:1:21j,\n",
    "                        -1:1:21j]\n",
    "        bfx = np.concatenate((x[..., None], y[..., None]), axis=-1)\n",
    "        f = func(bfx)\n",
    "        self.ctr = self.ax.contour(x, y, f, \n",
    "                                   levels=[-4e-1, -3e-1, -2e-1, -1e-1, -7e-2, -5e-2, -1e-2, -1e-3, 0, 1e-3], \n",
    "                                   cmap='Blues_r')\n",
    "        self.ax.set_xlabel('x')\n",
    "        self.ax.set_ylabel('y')\n",
    "        #self.ax.clabel(self.ctr, self.ctr.levels, inline=True, fontsize=6)\n",
    "        self.list_of_xs = []\n",
    "        self.list_of_ys = []\n",
    "        self.line2, = self.ax.plot([], [], 'r*-')\n",
    "\n",
    "        \n",
    "    def anim_init(self):\n",
    "        return (self.line2,)\n",
    "        \n",
    "    def update(self, xt):\n",
    "        self.list_of_xs.append(xt[0])\n",
    "        self.list_of_ys.append(xt[1])\n",
    "        self.line2.set_data(self.list_of_xs, self.list_of_ys)\n",
    "        return self.line2,\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots()        \n",
    "a = Anim(fig, ax, f)\n",
    "anim1 = animation.FuncAnimation(fig, a.update, frames=list_of_xts[::10],\n",
    "                                init_func=a.anim_init, blit=True, repeat=False)\n",
    "anim1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac2759-a529-445c-9963-9f0a24cd9228",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7f7e7497530d5e64013121053591359",
     "grade": true,
     "grade_id": "cell-2802bfb4b2c6edaf",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.allclose(OPTIMAL_X, [-7.07e-01,  1.06e-04], atol=1e-3, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976836f-9066-4486-b126-9c4420647617",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e81f268fd8cfd63940122f8659d83b1",
     "grade": false,
     "grade_id": "cell-729ffc121c39078d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x0 = np.array([-1, -1])\n",
    "# Repeat minmimization with a different starting point x0\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed9574-ad23-4a9d-8e67-8a97dbaa15b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9319c3ecad782c6613461634c95cfbf1",
     "grade": true,
     "grade_id": "cell-e58b107f406f3236",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.allclose(OPTIMAL_X, [-7.07e-01,  1.06e-04], atol=1e-3, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f41f43-e565-4f49-921f-e6d3a65e42cd",
   "metadata": {},
   "source": [
    "#### Convergence of gradient descent \n",
    "(Section 9.3 of [Convex Optimization by Stephen Boyd and Lieven Vandenberghe](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb97f05-0e9a-402c-a064-d209a94faa73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc58b6-b88c-4b16-abcd-2c7b10edd0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
