{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f41cb6",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918119e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b671b5d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689f1c8-dc50-4491-ab60-a9e4a5442a0d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# we will need matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd93ecb-8121-44f4-8344-91fff8d95d75",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear algebra: (Chapter 2 of MML Book by Deisenroth et al)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d317de-44df-41f3-a358-f70126f56039",
   "metadata": {},
   "source": [
    "1. Linear algebra is the study of vectors and certain\n",
    "rules to manipulate vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49da13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Equation of a 2D line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ee8cb",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "There are two equations of lines\n",
    "\n",
    "1. $y = m x + c$\n",
    "2. $a x + b y + c = 0$\n",
    "\n",
    "What are the advantages or disadvantages of both of those?\n",
    "\n",
    "\n",
    "A 2D line is the set of all points (x, y) that satisfy the an equation $a x + b y + c = 0$ for given $a, b, c$. For the same line there can be multiple valid parameters $a, b , c$. Either $a$ or $b$ can be zero. But both $a$ and $b$ cannot be zero at the same time. \n",
    "\n",
    "Ideally the equation of line must  be written in the set notation:\n",
    "\n",
    "$\\newcommand{\\bbR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\calL}{\\mathcal{L}}$\n",
    "$\\newcommand{\\calC}{\\mathcal{C}}$\n",
    "$$\\calL(a, b, c) = \\{(x, y): ax + by + c = 0, x \\in \\bbR, y \\in \\bbR \\}$$\n",
    "\n",
    "This equation is read as: the line $\\mathcal{L}$ defined by the paramters $a,b,c$ is the set of all points $(x, y)$ such that it the $x,y$ satisfy the equation $ax + by + c = 0$ and $x$ and $y$ are in the set of all real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0a829",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Implicit form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e335584",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Line is a type of curve. Other curves can be parabola, circle etc. Every curve can be represented in implicit form like we did for the line above. In implicit form, the points are *constrained* by one or more equations to lie on the curve. The equations define a test whether the point lies on the curve or not.\n",
    "\n",
    "$$\\calC(p_1, p_2, \\dots, p_n) = \\{(x, y): f(x, y; p_1, p_2, \\dots, p_n) = 0, x \\in \\bbR, y \\in \\bbR\\}$$\n",
    "\n",
    "This equation is read as the curve $\\calC$ defined by the paramters $p_1, p_2, \\dots, p_n$ is the set of all points $(x, y)$ such that it the $x,y$ satisfy the equation $f(x, y; p_1, p_2, \\dots, p_n) = 0$ and $x$ and $y$ are in the set of all real numbers.\n",
    "\n",
    "Take circle with center $(x_0, y_0)$ and radius $r$, as an example. The implicit form is:\n",
    "\n",
    "$$\\calC(x_0, y_0, r) = \\{(x, y): (x-x_0)^2 + (y-y_0)^2 - r^2 = 0, x \\in \\bbR, y \\in \\bbR\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e00327",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Paramteric form of 2D Line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a5567-1a2a-4fab-935b-6747260fca54",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A parameteric form defines how the points on the curve are generated from a free paramters. For a line:\n",
    "\n",
    "$$\\calL(d_x, d_y, x_0, y_0) = \\{(d_xr+x_0, d_yr+y_0): r \\in \\bbR \\}$$\n",
    "\n",
    "This equation is read as: the line $\\calL$ defined by the paramters $d_x,d_y,x_0,y_0$ is the set of all points $(d_xr+x_0, d_yr+y_0)$ such that $r$ is any real number.\n",
    "\n",
    "Take circle with center $(x_0, y_0)$ and radius $r$, as an example. The parameteric form is:\n",
    "\n",
    "$$\\calC(x_0, y_0, r) = \\{(r\\cos(\\theta) + x_0, r\\sin(\\theta) + y_0): \\theta \\in [0, 2\\pi)\\}$$\n",
    "\n",
    "In general, the paramteric form of a curve depend on some free paramters and the points are defined as functions of the free parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5b0e7-f6e3-4745-a46b-34159eaa8169",
   "metadata": {},
   "source": [
    "# Linear system of equations and Matrices\n",
    "\n",
    "\\begin{align}\n",
    "4 x_1 + 4 x_2 &= 5 \\\\\n",
    "2 x_1 - 4 x_2 &= 1\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93045182-d020-4a27-bcf1-d07fbc25a3e6",
   "metadata": {},
   "source": [
    "<img src=\"imgs/mml-fig-2.3-two-lines.png\" width=\"200px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c360701-0975-492d-87c3-01cfd134c383",
   "metadata": {},
   "source": [
    "The equation \n",
    "\\begin{align}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "4 & 4 \\\\\n",
    "2 & -4\n",
    "\\end{bmatrix}}_{A \\in \\mathbb{R}^{2 \\times 2}}\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2\n",
    "\\end{bmatrix}}_{\\mathbf{x} \\in \\mathbb{R}^2}\n",
    "=\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "5 \\\\ 1\n",
    "\\end{bmatrix}}_{\\mathbf{b} \\in \\mathbb{R}^2},\n",
    "\\end{align}\n",
    "\n",
    "can be written in matrix form as,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eacf91-159f-4cbf-9e26-c130c5e711c6",
   "metadata": {},
   "source": [
    "$$ A \\mathbf{x} = \\mathbf{b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9bb284-9dbd-4c72-9279-42bdaf7e1d06",
   "metadata": {},
   "source": [
    "In general a system of $m$ equations with $n$ unknowns,\n",
    "\\begin{align}\n",
    "a_{11} x_1 + a_{12} x_2 + \\dots + a_{1n} x_n &= b_1 \\\\\n",
    "\\vdots \\\\\n",
    "a_{m1} x_1 + a_{m2} x_2 + \\dots + a_{mn} x_n &= b_m\n",
    "\\end{align}\n",
    "\n",
    " can be written in matrix form as\n",
    " \n",
    "\\begin{align}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "a_{11} & \\dots & a_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & \\dots & a_{mn}\n",
    "\\end{bmatrix}}_{A \\in \\mathbb{R}^{m \\times n}}\n",
    "\\underbrace{\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ \\vdots \\\\ x_n\n",
    "\\end{bmatrix}}_{\\mathbf{x} \\in \\mathbb{R}^n}\n",
    "=\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "b_1 \\\\ \\vdots \\\\ b_m\n",
    "\\end{bmatrix}}_{\\mathbf{b} \\in \\mathbb{R}^m}.\n",
    "\\end{align}\n",
    "\n",
    "Or $$ A \\mathbf{x} = \\mathbf{b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11e2e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88918d7-0d7a-4e93-b40d-ad9a0e963afb",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\newcommand{\\bfz}{\\mathbf{z}}\n",
    "\\newcommand{\\bfy}{\\mathbf{y}}\n",
    "\\newcommand{\\bfx}{\\mathbf{x}}\n",
    "\\newcommand{\\bfw}{\\mathbf{w}}\n",
    "\\newcommand{\\bfv}{\\mathbf{v}}\n",
    "\\newcommand{\\bfu}{\\mathbf{u}}$\n",
    " We will denote vectors with bold font notations instead of the usualy $\\vec{x}$ notation. The arrow notation vectors are sometimes called geometric vectors. We will make no such distinction. The set of all real numbers will be denoted as $\\bbR$. The set of all real 2D vectors is written as $\\bbR^2$. When we write $\\bfx \\in \\bbR^2$, it means that $\\bfx$ is in the set of real 2D vectors, hence a 2D real vector. We will write $\\|\\bfx\\|$ for the magnitude of the vector, and $\\bfx \\cdot \\bfy$ for dot product between two vector $\\bfx$ and $\\bfy$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611384df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## n-D vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790fc5e-8b33-4a49-9c7a-fc869a7a638a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A n-D vector is also written as $\\bfx \\in \\bbR^n$ and the vector has $\\bfx = [x_1; \\dots; x_n]$ $n$ real components. Every vector has a magnitude $\\|\\bfx\\|$ and a direction $\\hat{\\bfx}$. The magnitude and direction are given by:\n",
    "\n",
    "$$ \\|\\bfx\\| = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^n} \\in \\bbR$$ \n",
    "$$ \\hat{\\bfx} = \\frac{1}{\\|\\bfx\\|} \\bfx \\in \\bbR^n$$\n",
    "\n",
    "The direction vector $\\hat{\\bfx}$ is a unit vector because its magnitide is one i.e. $\\|\\hat{\\bfx}\\| = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bd514-eca8-4940-bf9d-2d7ea941ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2, 3, 4, 5, 9, 10])\n",
    "xmag = np.linalg.norm(x)\n",
    "xunit = x / xmag\n",
    "\n",
    "# For many many x vectors you can do this:\n",
    "xs = np.array([[2, 3, 4, 5, 9, 10],\n",
    "               [-3, 2, -5, 4, -10, 9]]) # 2 x 6\n",
    "xmags = np.linalg.norm(xs, axis=-1, keepdims=True) # 2 x 1\n",
    "xunits = xs / xmags # 2 x 6\n",
    "print(xunit, xunits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2627ae-960a-4c66-8382-0e1f2e3d0ea9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Vector addition\n",
    "\n",
    "Vector addition is element-wise addition\n",
    "\n",
    "$$ \\bfv + \\bfw \n",
    "= \\begin{bmatrix} v_1 \\\\ \\vdots \\\\ v_n \\end{bmatrix} + \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_n \\end{bmatrix}\n",
    "= \\begin{bmatrix} v_1 + w_1 \\\\ \\vdots \\\\ v_n + w_n \\end{bmatrix} $$\n",
    "\n",
    "Geometrically the resulting vector can be obtained by triangle law or the parallelogram law.\n",
    "\n",
    "![](https://openstax.org/apps/archive/20221219.191545/resources/8c95eeee388ee88ecb81b6026404d02273fbcb84)\n",
    "\n",
    "Reference: \\[[1](https://openstax.org/books/calculus-volume-3/pages/2-1-vectors-in-the-plane)\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71c934-1df4-42a2-b7ef-5b39fcf5cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.arange(0, 10)\n",
    "w = np.arange(10, 20)\n",
    "v+w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb6d8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dot product of vectors\n",
    "\n",
    "Dot product of two vectors is a scalar given by sum of element-wise product.\n",
    "\n",
    "$$ \\bfv \\cdot \\bfu \n",
    "= \\begin{bmatrix} v_1 \\\\ \\vdots \\\\ v_n \\end{bmatrix} \\cdot \\begin{bmatrix} u_1 \\\\ \\vdots \\\\ u_n \\end{bmatrix}\n",
    "= v_1 u_1 + v_2 u_2 + \\cdots + v_n u_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c7e4e-340a-40af-a9f1-08655199b233",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Geometrically, dot product is closely related to the projection. Projection of vector $\\bfv$ on $\\bfu$ is the dot product of $\\bfv$ with the direction of $\\bfu$ \n",
    "\n",
    "$$\\text{proj}_{\\bfu}\\bfv = \\bfv \\cdot \\hat{\\bfu}$$\n",
    "\n",
    "![](https://openstax.org/apps/archive/20221219.191545/resources/263b8d95f699470f4cf6d49170b85118906c5ede)\n",
    "\n",
    "Dot product of vector with itself gives the square of the magnitude $\\bfv \\cdot \\bfv = \\|\\bfv\\|^2$.\n",
    "\n",
    "Reference: \\[[2](https://openstax.org/books/calculus-volume-3/pages/2-3-the-dot-product)\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35c92c-ce82-433c-94b3-24ed872f9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.arange(10)\n",
    "u = np.arange(10, 20)\n",
    "v @ u # v . u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb281755-6e4d-40cf-8e19-18c483754855",
   "metadata": {},
   "outputs": [],
   "source": [
    "v @ u / np.linalg.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ae487-a3c5-47c8-8fb8-1360ecc1b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(v @ v , np.linalg.norm(v)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ec792",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5e6e9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\newcommand{\\bfX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bfV}{\\mathbf{V}}$\n",
    "$\\newcommand{\\bfU}{\\mathbf{U}}$\n",
    "Matrices are a group of vectors. A matrix can be obtained by vertical stacking of row (horizontal) vectors or horizontal stacking of column (vertical) vectors. It is common to represent all vectors as column vectors unless specified otherwise, so we consider a matrix $\\bfV$ as a horizontal concatenation of column vectors $\\bfv_1, \\bfv_2, \\dots, \\bfv_n$. Let each vector be m-dimensional $\\bfv_i \\in \\bbR^m$.\n",
    "\n",
    "$$\\bfV = \\begin{bmatrix}\\bfv_1 & \\bfv_2 & \\dots & \\bfv_n\\end{bmatrix} \n",
    "= \\begin{bmatrix}\n",
    "\\bfv_1[1]& \\bfv_2[1] & \\dots & \\bfv_n[1]\\\\\n",
    "\\bfv_1[2]& \\bfv_2[2] & \\dots & \\bfv_n[2]\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\bfv_1[m] & \\bfv_2[m] & \\dots & \\bfv_n[m]\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Such a matrix is said to be a $m \\times n$ matrix. It is also written as $\\bfV \\in \\bbR^{m \\times n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37828eb7-e0dd-4f31-b42d-c460f8d3cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.arange(5)\n",
    "v2 = np.arange(5, 10)\n",
    "v3 = np.arange(10, 15)\n",
    "V = np.hstack((v1[:, None], v2[:, None], v3[:, None]))\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4a07d-526c-4026-9a83-1c4de1e7c6c1",
   "metadata": {},
   "source": [
    "# Linear functions\n",
    "\n",
    "What are linear functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197dbcb4-9285-4ead-a885-66c6408527fd",
   "metadata": {},
   "source": [
    "#### Definition of Linear combination (2.11 in MML book)\n",
    "\n",
    "Given $m$ vectors $\\bfx_1, \\bfx_2, \\dots, \\bfx_m \\in \\bbR^n$, a linear combination of vectors is defined as,\n",
    "$ \\mathbf{v} := \\alpha_1 \\bfx_1 + \\alpha_2 \\bfx_2 + \\dots + \\alpha_n \\bfx_m $ for every any scalar $\\alpha_1, \\alpha_2, \\dots \\alpha_n \\in \\bbR$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86279039-e544-407a-92d9-8dd6edff88e6",
   "metadata": {},
   "source": [
    "#### Definition of Linear function\n",
    "\n",
    "A function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ is linear in $\\mathbf{x}$ when \n",
    "\n",
    "1.  $f(\\alpha\\mathbf{x}) = \\alpha f(\\mathbf{x})$ for all $\\alpha \\in \\mathbb{R}$ and $\\mathbf{x} \\in \\mathbb{R}^n$\n",
    "2.  $f(\\mathbf{x} + \\mathbf{y}) = f(\\mathbf{x}) + f(\\mathbf{y})$ for all $\\alpha, \\beta \\in \\mathbb{R}$ and $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$\n",
    "\n",
    "The above two properties can be combined in a single test \n",
    "$$ f(\\alpha \\mathbf{x} + \\beta \\mathbf{y}) = \\alpha f(\\mathbf{x}) + \\beta f(\\mathbf{y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1a9e4-8d7e-405a-99cc-061b572a5aec",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bfx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bfb}{\\mathbb{R}}$\n",
    "\n",
    "#### Examples of linear functions\n",
    "\n",
    "1. $ f([x_1, x_2]) = 3x_1 + 2x_2$\n",
    "2. $ f([x_1, x_2, x_3]) = 4(3x_1 + 2x_2) + x_3$\n",
    "3. $ f([x_1, x_2, x_3], [w_1, w_2, w_3]) = w_1x_1 + w_2x_2 + w_3 x_3$\n",
    "4. $ f([x_1, x_2]) = \\exp(\\log(3x_1 + 3x_2))$\n",
    "\n",
    "##### Homework 3. Problem 1: Show that the above functions are linear.\n",
    "\n",
    "#### Examples of non-linear functions\n",
    "\n",
    "1. $ f([x_1, x_2]) = x_1^2$\n",
    "2. $ f([x_1, x_2]) = x_1^3$\n",
    "3. $ f([x_1, x_2]) = \\sin(x_1)$\n",
    "4. $ f([x_1, x_2]) = \\exp(x_1)$\n",
    "5. $ f([x_1, x_2]) = \\log(x_1)$\n",
    "\n",
    "##### Homework 3. Problem 2: Show that the above functions are non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a29d8-d7a4-45d1-a205-a7a9d60b512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 4*(3*x[0] + 2*x[1]) + x[2]\n",
    "x = np.random.rand(3)\n",
    "y = np.random.rand(3)\n",
    "alpha = np.random.rand()\n",
    "beta = np.random.rand()\n",
    "np.allclose(f(alpha * x + beta * y) ,\n",
    "            alpha * f(x) + beta * f(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e483e29-dc83-4a8c-8108-2d35399bcab9",
   "metadata": {},
   "source": [
    "# Properties of Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb45f5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transpose of a Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70000e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Transpose of a matrix $\\bfV$ (denoted as $\\bfV^\\top$) is an operation that swaps rows with columns and columns with rows. For example, the transpose of the above matrix will make it a vertical concatenation of row vectors.\n",
    "\n",
    "$$\\bfV^\\top = \\begin{bmatrix}\\bfv_1^\\top \\\\ \\bfv_2^\\top \\\\ \\vdots \\\\ \\bfv_n^\\top\\end{bmatrix} \n",
    "= \\begin{bmatrix}\n",
    "\\bfv_1[1]& \\bfv_1[2] & \\dots & \\bfv_n[m]\\\\\n",
    "\\bfv_2[1]& \\bfv_2[2] & \\dots & \\bfv_n[m]\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\bfv_n[1] & \\bfv_2[2] & \\dots & \\bfv_n[m]\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "If $\\bfV \\in \\bbR^{m \\times n}$, then $\\bfV^\\top \\in \\bbR^{n \\times m}$. The two dimensions get swapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83340f4-c6bf-42dc-a6b2-600da8b7372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.arange(12).reshape(3, 4)\n",
    "V, V.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c37ec4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tranpose of a column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855cfbb2",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "All vectors are also matrices. By convention, all vectors are considered column matrices and hence called column vectors. A n-D vector $\\bfv = [v_1; \\dots; v_n] \\in \\bbR^n$ is by convention considered a $n \\times 1$ column matrix i.e. $\\bfv \\in \\bbR^{n \\times 1}$.\n",
    "\n",
    "$$ \\bfv = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n  \\end{bmatrix} \\in \\bbR^{n\\times 1}$$\n",
    "\n",
    "The transpose of a column vector is a row vector\n",
    "\n",
    "$$\\bfv^\\top = \\begin{bmatrix} v_1 & v_2 & \\dots & v_n  \\end{bmatrix} \\in \\bbR^{1\\times n}$$\n",
    "\n",
    "Row vectors are always denoted with a tranpose of their corresponding column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c9dbc3-11e3-4925-9829-2e6b18c8fc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.arange(5).reshape(5, 1)\n",
    "v.shape, v.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38a91c-bd66-49ea-8bf0-3504eafb2fd7",
   "metadata": {},
   "source": [
    "## Matrix addition\n",
    "\n",
    "You can add two matrices element-wise.\n",
    "\n",
    "\\begin{align}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "a_{11} & \\dots & a_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & \\dots & a_{mn}\n",
    "\\end{bmatrix}}_{A \\in \\mathbb{R}^{m \\times n}}\n",
    "+\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "b_{11} & \\dots & b_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "b_{m1} & \\dots & b_{mn}\n",
    "\\end{bmatrix}}_{B \\in \\mathbb{R}^{m \\times n}}\n",
    ":=\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "a_{11} + b_{11} & \\dots & a_{1n} + b_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} + b_{m1} & \\dots & a_{mn} + b_{mn}\n",
    "\\end{bmatrix}}_{A + B \\in \\mathbb{R}^{m \\times n}}\n",
    "\\end{align}\n",
    "\n",
    "The matrix addition is commutative\n",
    "$ A + B = B + A $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699f1b0-7baa-474d-817d-150fe01be9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(3, 4)\n",
    "B = np.random.rand(3, 4)\n",
    "np.allclose(A + B, B+ A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd9201-c9de-4e49-9bb7-3dccb8fd7ace",
   "metadata": {},
   "source": [
    "## Matrix-scalar product\n",
    "\n",
    "You can multiply matrices with a scalar (a real number)\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha \\underbrace{\\begin{bmatrix}\n",
    "a_{11} & \\dots & a_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & \\dots & a_{mn}\n",
    "\\end{bmatrix}}_{A \\in \\mathbb{R}^{m \\times n}} \n",
    ":= \n",
    "\\underbrace{\\begin{bmatrix}\n",
    "\\alpha a_{11} & \\dots & \\alpha  a_{1n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\alpha  a_{m1} & \\dots & \\alpha a_{mn}\n",
    "\\end{bmatrix}}_{\\alpha A \\in \\mathbb{R}^{m \\times n}} \n",
    "\\end{align}\n",
    "\n",
    "Matrix scalar product is commutative\n",
    "$$ \\alpha A = A \\alpha $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5077e6a-d742-4b6a-90b0-cbe82405143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.random.rand()\n",
    "A = np.random.rand(3, 4)\n",
    "np.allclose(alpha * A, A * alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131bd2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix-vector product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd4baca",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For those who know dot product, matrix-vector product is best defined as a collection of dot products. Define a matrix $\\bfV \\in \\bbR^{m \\times n}$ as a vertical concatenation of $m$ n-dimensional row-vectors $\\bfv_1^\\top, \\dots \\bfv_m$\n",
    "\n",
    "$$\\bfV = \\begin{bmatrix}\\bfv_1^\\top \\\\ \\bfv_2^\\top \\\\ \\vdots \\\\ \\bfv_m^\\top\\end{bmatrix} $$\n",
    "\n",
    "The matrix $\\bfV \\in \\bbR^{m \\times n}$ can be multiplied by a n-dimensional column vector $\\bfu \\in  \\bbR^{n}$ with the product defined as the vector-wise dot product vertically concatenated to result in another column vector:\n",
    "\n",
    "$$ \\bfV \\bfu = \\begin{bmatrix}\\bfv_1 \\cdot \\bfu \\\\ \\bfv_2 \\cdot \\bfu \\\\ \\vdots \\\\ \\bfv_m \\cdot \\bfu\\end{bmatrix} \\in \\bbR^{m}$$\n",
    "\n",
    "When $m = 1$, then $\\bfV = \\bfv_1^\\top$ and the matrix product is $\\bfv_1^\\top \\bfu = \\bfv_1 \\cdot \\bfu$. Dot product between two vectors $\\bfv$ and $\\bfu$ is also written as $\\bfv^\\top \\bfu$. Going forward we will prefer $\\bfv^\\top \\bfu$ notation for dot product instead of $\\bfv \\cdot \\bfu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414431f-59ad-4218-82ac-bf74818e4aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.random.rand(3, 4)\n",
    "u = np.random.rand(4)\n",
    "V @ u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dfa0de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix-matrix product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6600e16",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Matrix-matrix product between two matrices $\\bfV \\in \\bbR^{m \\times n}$ and $\\bfU \\in \\bbR^{n \\times p}$ can be defined in terms of matrix-vector product by writing $\\bfU$ as a horizontal concatenation of $p$ $n$-dimensional column vectors $\\bfu_1, \\bfu_2, \\dots, \\bfu_p$.\n",
    "\n",
    "$$ \\bfV \\bfU = \\begin{bmatrix} \\bfV\\bfu_1 & \\bfV\\bfu_2 & \\dots & \\bfV\\bfu_p \\end{bmatrix} \\in \\bbR^{m \\times p}\\\\\n",
    "= \\begin{bmatrix}\n",
    "\\bfv_1^\\top \\bfu_1& \\bfv_1^\\top \\bfu_2 & \\dots & \\bfv_1^\\top \\bfu_p\\\\\n",
    "\\bfv_2^\\top \\bfu_1& \\bfv_2^\\top \\bfu_2 & \\dots & \\bfv_2^\\top \\bfu_p\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\bfv_m^\\top \\bfu_1 & \\bfv_m^\\top \\bfu_2 & \\dots & \\bfv_m^\\top \\bfu_p\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The result is a horizontal concatenation of matrix-vector products, where the left matrix $\\bfV$ gets multiplied with each column vector of right matrix $\\bfU$.\n",
    "\n",
    "Another interpretation is that the matrix-matrix product are all possible  dot products between left matrices' row vectors with right matrices' column vectors.\n",
    "\n",
    "Matrix-matrix product or short matrix products do not commute i.e $\\bfV \\bfU \\ne \\bfU \\bfV$ in general.\n",
    "\n",
    "Matrix-matrix product does follow distributive property\n",
    "$A(B + C) = AB + AC$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bec4c7-4b2d-4531-b65a-37fb885272ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.random.rand(3, 4)\n",
    "U = np.random.rand(4, 5)\n",
    "V @ U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2dae50-c0a9-4217-b606-e592456fc826",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Identity matrix\n",
    "\n",
    "$$ \\newcommand{\\bfI}{\\mathbf{I}}\\bfI_n = \\begin{bmatrix} \n",
    "    1 & 0 & \\dots & 0 \\\\\n",
    "    0 & 1 & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\dots & 1 \\\\\n",
    " \\end{bmatrix}$$\n",
    " \n",
    " ### Square matrix\n",
    " \n",
    " A square matrix is a matrix with number of rows equal to the number of columns.\n",
    " \n",
    " ### Inverse of a square matrix\n",
    " \n",
    " A matrix $\\bfV^{-1}$ is called the inverse of a square matrix $\\bfV$ if $\\bfV^{-1} \\bfV = \\bfV^{-1} = \\bfI_n$. The inverse of a square matrix exists only when it is singular i.e the determinant of the matrix is non-zero $\\det(\\bfV) \\ne 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6bfdc-06c7-494f-8a72-c7d61b73b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(10)\n",
    "I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf467656-66e0-405b-91e0-2b88dc4f8f10",
   "metadata": {},
   "source": [
    "# Linearity of matrix multiplication\n",
    "$\\newcommand{\\bfb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bfy}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bfA}{A}$\n",
    "\n",
    "Consider a function $f_A : \\bbR^n \\to \\bbR^m$ defined as\n",
    "$$ f_A(\\bfx) := \\bfA \\bfx$$.\n",
    "\n",
    "Show that the function $f_A$ is linear.\n",
    "\n",
    "#### Proof\n",
    "The function $f_A(\\bfx)$ is linear if for any $\\bfx,\\bfy \\in \\bbR^n$ and $\\alpha, \\beta \\in \\bbR$,\n",
    "$$f_A(\\alpha \\bfx + \\beta \\bfy) = \\alpha f_A(\\bfx) + \\beta f_A(\\bfy)$$\n",
    "\n",
    "$$\\bfA (\\alpha \\bfx + \\beta \\bfy) =  A (\\alpha\\bfx) +  A (\\beta\\bfy) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b9044-0b41-431d-a111-6716ca785c44",
   "metadata": {},
   "source": [
    "#### Theorem \n",
    "All linear functions of the form $f : \\bbR^m \\to \\bbR^n$ can be written as matrix multplications of the form $f(\\bfx) = F \\bfx$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a566b6-09e7-49ee-bbad-0cea02557109",
   "metadata": {},
   "source": [
    "#### Proof:\n",
    "$\\newcommand{\\bfe}{\\mathbf{e}}$\n",
    "\n",
    "Consider any given linear function $f : \\bbR^m \\to \\bbR^n$ which is linear. Because of linearity it satisifies $f(\\alpha \\bfx + \\beta \\bfy) = \\alpha f(\\bfx) + \\beta f(\\bfy)$.\n",
    "\n",
    "Consider special vectors, called the standard basis vectors, that have only one element as 1 and the rest are 0.\n",
    "$$\\bfe_1 := [1, 0, \\dots, 0]^\\top \\in \\bbR^n$$\n",
    "$$\\bfe_2 := [0, 1, \\dots, 0]^\\top \\in \\bbR^n$$\n",
    "$$\\bfe_n := [0, 0, \\dots, 1]^\\top \\in \\bbR^n$$\n",
    "\n",
    "Any vector $\\bfx = [x_1, x_2, \\dots, x_n]^\\top \\in \\bbR^n$ can be written as a linear combination of standard basis vectors,\n",
    "\n",
    "$\\bfx = x_1 \\bfe_1 + x_2 \\bfe_2 + \\dots + x_n \\bfe_n$\n",
    "\n",
    "Hence the function evaluation at $\\bfx$, can also be written as the linear combination of function evaluations at standard basis vectors,\n",
    "$$f(\\bfx) = f(x_1 \\bfe_1 + x_2 \\bfe_2 + \\dots + x_n \\bfe_n) $$\n",
    "$$ f(\\bfx) = x_1 f(\\bfe_1) + x_2 f(\\bfe_2) + \\dots + x_n f(\\bfe_n) $$\n",
    "Note that $f(\\bfe_1)$, $f(\\bfe_2)$, are constants with respect to $\\bfx$. So you can arrange them into a constant matrix, independent of $\\bfx$,\n",
    "\n",
    "$$ f(\\bfx) = \\underbrace{\\begin{bmatrix}\n",
    "f(\\bfe_1) &  f(\\bfe_2) & \\dots & f(\\bfe_n)\n",
    "\\end{bmatrix}}_{F}\n",
    "\\underbrace{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}}_{\\bfx} $$\n",
    "\n",
    "$$ f(\\bfx) = F\\bfx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f16799-43ae-4d3a-85d3-927be35f918e",
   "metadata": {},
   "source": [
    "#### Theorem\n",
    "Show that composition $f(g(\\bfx))$ of two linear functions, $f$ and $g$ is also a linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d3859-9b7e-420d-8f2a-d309ab2f4717",
   "metadata": {},
   "source": [
    "Proof is left as Homework 3: Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d5ebcd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using vectors for 2D line notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aeabd0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def stylizeax(ax, limits):\n",
    "    \"\"\"Set ax style\"\"\"\n",
    "    minx, maxx, miny, maxy = limits\n",
    "     # x-axis, y=0\n",
    "    ax.annotate(\"\", \n",
    "                xy=(minx, 0),\n",
    "                xytext=(maxx, 0),\n",
    "                arrowprops=dict(arrowstyle=\"<->\"),\n",
    "                color='k')\n",
    "    ax.text(maxx, 0, \"x\")\n",
    "     # y-axis, x=0\n",
    "    ax.annotate(\"\", \n",
    "                xy=(0, miny),\n",
    "                xytext=(0, maxy),\n",
    "                arrowprops=dict(arrowstyle=\"<->\"),\n",
    "                color='k')\n",
    "    ax.text(0, maxy, \"y\")\n",
    "    \n",
    "\n",
    "    ax.grid(True, which='both') # show the grid\n",
    "    ax.set_aspect('equal') # set aspect ratio of the grid to 1:1\n",
    "    \n",
    "    \n",
    "def points_on_line(a, b, c, Npts=6, scale=10):\n",
    "    \"\"\"Generate points on the line ax + by + c = 0\"\"\"\n",
    "    # ax + by + c = 0\n",
    "    # In parameteric form with free parameter r\n",
    "    #   (x, y) = (-b*r + x0, a*r + y0)\n",
    "    # where \n",
    "    #   x0 = - a*c / (a*a + b*b)\n",
    "    #   y0 = - b*c / (a*a + b*b)\n",
    "    # (x0, y0) is the point on the line closest to the origin\n",
    "    uniformgrid = [i/Npts for i in range(-scale*Npts//2, scale*Npts//2, scale)]\n",
    "    x0 = -a*c/(a*a + b*b)\n",
    "    y0 = -b*c/(a*a + b*b)\n",
    "    x = [-b*r + x0 for r in uniformgrid]\n",
    "    y = [ a*r + y0 for r in uniformgrid]\n",
    "    return x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106eb5fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6bb7f-4da0-4576-97ed-0a833ff8e4f9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a line ax + by + c = 0\n",
    "# a, b, c = 2.5, -1, -5 # pick numbers by hand\n",
    "\n",
    "# pick a, b, c at random\n",
    "import random\n",
    "scale = 10\n",
    "a, b, c = [scale*(random.random()-0.5) for _ in range(3)] # random number from -10 to 10\n",
    "\n",
    "# Generate some sample points on a line\n",
    "x, y = points_on_line(a, b, c, scale=scale)\n",
    "\n",
    "# Plot the points\n",
    "fig, ax = plt.subplots()\n",
    "stylizeax(ax, (min(x), max(x), min(y), max(y)))\n",
    "ax.plot(x, y, '*-') # the line\n",
    "ax.set_title(f'{a:.1f}x{b:+.1f}y{c:+.1f} = 0') # print the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef40787-74c4-438b-9f87-ecfab586d7fa",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We started from 2D linear models, but we want to work with N-D models where N can be even in thousands or millions. It makes sense to simplify the notation by using vector notation. \n",
    "\n",
    "Recall that the implicit equation for a line is\n",
    "\n",
    "$$\\calL(a, b, c) = \\{(x, y): ax + by + c = 0, x \\in \\bbR, y \\in \\bbR \\}$$\n",
    "\n",
    "We will represent a 2D point (x, y) by a 2D vector $\\bfx = [x; y]$ and the parameters $(a, b)$ with weight vector $\\bfw = [a; b]$.\n",
    "Let's compute the dot product between the two newly defined vectors :\n",
    "\n",
    "$$\\bfw \\cdot \\bfx = ax + by$$\n",
    "\n",
    "The equation of the line under new notation in full its full glory is \n",
    "\n",
    "$$\\calL(\\bfw, c) = \\{\\bfx: \\bfw \\cdot \\bfx + c = 0, \\bfx \\in \\bbR^2 \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685879cc-26ad-4a9b-822a-f11037a7bf07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Unique line notation\n",
    "\n",
    "Same line $ax + by + c = 0$ can be represented by multiple equations for the same form. This representation of line is not unique. For example, equations $5x + 2y + 10 = 0$ and $10x + 4y + 20 =0$ represent the same line. In general, for any real number $\\alpha \\ne 0$ all equations $\\alpha a x + \\alpha b y + \\alpha c = 0$ represent the same line. Once can choose an arbitrary non-zero \n",
    "$\\alpha $ for making the equation unique. \n",
    "\n",
    "In vector notation, all non-zero $\\alpha \\bfw \\cdot \\bfx + \\alpha c = 0$ represent the same line. One good candidate for $\\alpha$ is  $\\alpha = \\frac{1}{\\|\\bfw\\|}$, because this changes $\\alpha \\bfw$ to $\\hat{\\bfw}$ a unit vector.\n",
    "\n",
    "$$\\calL(\\hat{\\bfw}, w_0) = \\{\\bfx: \\hat{\\bfw} \\cdot \\bfx + w_0 = 0, \\bfx \\in \\bbR^2 \\}$$\n",
    "\n",
    "where $w_0 = \\frac{c}{\\|\\bfw\\|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2257c17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Geometric interpretaion\n",
    "\n",
    "<img src=\"imgs/line-eq-geometric-interpretations.svg\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a788750c-1f56-4175-a997-bb896da5abbb",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "The new equation of the line has a convinient geometric interpretation. Recall that $\\hat{\\bfw} \\cdot \\bfx$ is the projection of $\\bfx$ on $\\hat{\\bfw}$. In other words, the equation $\\text{proj}_{\\hat{\\bfw}}\\bfx + w_0 = 0$ constrains all vectors on the line to have a constant projection $\\text{proj}_{\\hat{\\bfw}}\\bfx = -w_0$.\n",
    "\n",
    "This means that vector $\\hat{\\bfw}$ is perpendicular to the line and cuts the line a distance $|w_0|$ from the origin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686cb638",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # a vector algebra library\n",
    "\n",
    "a = np.array([0, 1, 2, 3]) # a vector\n",
    "print(\"a=\", a)\n",
    "b = np.array([4, 5, 6, 7]) # another vector\n",
    "print(\"b=\", b)\n",
    "C = np.array([[0, 1, 2, 3],\n",
    "              [4, 5, 6, 7]]) # A matrix\n",
    "print(\"C=\", C)\n",
    "D = np.zeros((2, 4)) # a 2x4 matrix of zeros\n",
    "print(\"D=\", D)\n",
    "E = np.random.rand(2,5) # Random 2x5 matrix of numbers between 0 and 1\n",
    "print(\"E=\", E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cf4507",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"a*0.1 = \", a * 0.1) # element-wise multiplication\n",
    "print(\"C*0.2 = \", C * 0.2) # element-wise multiplication\n",
    "print(\"a*b = \", a * b)   # element-wise multiplication (Note: different from Matlab)\n",
    "print(\"a*b*0.2 = \", a * b * 0.2)   # element-wise multiplication\n",
    "print(\"C @ a = \", C @ a)   # matrix-vector product\n",
    "print(\"C.T = \", C.T)     # matrix transpose\n",
    "print(\"C.T @ D = \", C.T @ D) # matrix-matrix product\n",
    "print(\"a * C = \", a * C)   # so called broadcasting; numpy specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880d09b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Numpy: General Broadcasting Rules\n",
    "\n",
    "When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimension and works its way left. Two dimensions are compatible when\n",
    "\n",
    "   1. they are equal, or\n",
    "\n",
    "   2. one of them is 1.\n",
    "  \n",
    "Otherwise a ValueError is raised\n",
    "\n",
    "Ref: https://numpy.org/doc/stable/user/basics.broadcasting.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e50d67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the following example, both the A and B arrays have axes with length one that are expanded to a larger size during the broadcast operation:\n",
    "\n",
    "    A      (4d array):  8 x 1 x 6 x 1\n",
    "    B      (3d array):      7 x 1 x 5\n",
    "    Result (4d array):  8 x 7 x 6 x 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2565c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "A = np.random.rand(8, 1, 6, 1)\n",
    "B = np.random.rand(7, 1, 5)\n",
    "(A * B).shape # Returns the shape of the multi dimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a378e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are some more examples:\n",
    "\n",
    "    A      (2d array):  5 x 4\n",
    "    B      (1d array):      1\n",
    "    Result (2d array):  ?\n",
    "\n",
    "    A      (2d array):  5 x 4\n",
    "    B      (1d array):      4\n",
    "    Result (2d array):  ?\n",
    "\n",
    "    A      (3d array):  15 x 3 x 5\n",
    "    B      (3d array):  15 x 1 x 5\n",
    "    Result (3d array):  \n",
    "\n",
    "    A      (3d array):  15 x 3 x 5\n",
    "    B      (2d array):       3 x 5\n",
    "    Result (3d array):  ?\n",
    "\n",
    "    A      (3d array):  15 x 3 x 5\n",
    "    B      (2d array):       3 x 1\n",
    "    Result (3d array):  ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00746db",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def points_on_line(hatw, w0, Npts=6, scale=10):\n",
    "    \"\"\" Generate some sample points on a line \"\"\"\n",
    "    assert hatw.shape == (2,) # only works for 2D\n",
    "    perp_hatw = np.array([-hatw[1], hatw[0]])# vector perpendicular to hatw\n",
    "    uniformgrid = np.linspace(-scale//2, scale//2, Npts)\n",
    "    return perp_hatw * uniformgrid[:, None] - w0*hatw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637614d-ec65-44d8-930c-749b550d4a8b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a line ax + by + c = 0\n",
    "scale = 10\n",
    "# a, b, c = [scale*(random.random()-0.5) for _ in range(3)] # random number from -10 to 10\n",
    "# abc = scale*(np.random.rand(3)-0.5)  # random number from -10 to 10\n",
    "abc = [3, 2, -6] # pick your favorite line\n",
    "w = abc[:2]\n",
    "hatw = w / np.linalg.norm(w) # What does np.linalg.norm do?\n",
    "w0 = abc[2] / np.linalg.norm(w)\n",
    "\n",
    "# Generate some sample points on a line\n",
    "x = points_on_line(hatw, w0, Npts=6, scale=scale) # Npts x 2 array\n",
    "\n",
    "# Plot the points\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('equal')\n",
    "stylizeax(ax, (x[:, 0].min(), x[:, 0].max(), x[:, 1].min(), x[:, 1].max())) # numpy allows for multi-dimensional slicing\n",
    "ax.plot(x[:, 0], x[:, 1], '*-') # the line\n",
    "pt0 = -w0*hatw\n",
    "ax.annotate(\"\", xytext=(0, 0), xy=(pt0[0], pt0[1]),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color='r'))\n",
    "ax.text(pt0[0], pt0[1], r\"$-w_0\\hat{\\mathbf{w}}$\", color='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db131636-ca0f-42a9-b1ea-90ad09cf35f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear regression: review\n",
    "\n",
    "Let's take the simple linear regression example from STS332 textbook (uploaded on brightspace;page 300; Table 6-1). \n",
    "\n",
    "\"As an illustration, consider the data in Table 6-1. In this table, y is the salt concentration\n",
    "(milligrams/liter) found in surface streams in a particular watershed and x is the percentage of\n",
    "the watershed area consisting of paved roads.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49806742-d179-421d-9cb9-ed9c0721883d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile saltconcentration.tsv\n",
    "#Observation\tSaltConcentration\tRoadwayArea\n",
    "1\t3.8\t0.19\n",
    "2\t5.9\t0.15\n",
    "3\t14.1\t0.57\n",
    "4\t10.4\t0.4\n",
    "5\t14.6\t0.7\n",
    "6\t14.5\t0.67\n",
    "7\t15.1\t0.63\n",
    "8\t11.9\t0.47\n",
    "9\t15.5\t0.75\n",
    "10\t9.3\t0.6\n",
    "11\t15.6\t0.78\n",
    "12\t20.8\t0.81\n",
    "13\t14.6\t0.78\n",
    "14\t16.6\t0.69\n",
    "15\t25.6\t1.3\n",
    "16\t20.9\t1.05\n",
    "17\t29.9\t1.52\n",
    "18\t19.6\t1.06\n",
    "19\t31.3\t1.74\n",
    "20\t32.7\t1.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0951c0-91fd-48b3-b399-9216a78302ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# numpy can import text files separated by seprator like tab or comma\n",
    "salt_concentration_data = np.loadtxt(\"saltconcentration.tsv\")\n",
    "salt_concentration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d06dda-e251-4260-a440-5b33f4905fea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the points\n",
    "fig, ax = plt.subplots()\n",
    "# Scatter plot using matplotlib\n",
    "ax.scatter(salt_concentration_data[:, 2], salt_concentration_data[:, 1])\n",
    "ax.set_xlabel(r\"Roadway area %\")\n",
    "ax.set_ylabel(r\"Salt concentration (mg/L)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d65d58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least squares regression\n",
    "\n",
    "\n",
    "![](imgs/least-sq-stubs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d237e-b0da-4538-8aa9-49eee15c0a8b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The problem of linear regression is to find a line that \"best fits\" the given data. That is we want all the points $\\{(x_1, y_1), \\dots, (x_n, y_n)\\}$ to satisfy the equation of the line $y = mx + c$.  Since we know that there exists no such line, so we will try to make $y \\approxeq mx + c$, by minimizing some error/distance/cost/loss function between $y$ and $mx + c$ for every point $(x_i, y_i)$ in the dataset. The simplest error function that results in nice answers is squared distance:\n",
    "\n",
    "$$e(x_i,y_i) = (y_i - (mx_i + c))^2$$\n",
    "\n",
    "Then we can minimize the total error to find the line:\n",
    "\n",
    "$$m^*, c^* = \\arg~\\min_{m, c} \\sum_{i=1}^n e(x_i,y_i)$$\n",
    "\n",
    "\n",
    "Geometrically, this error minimization corresponds to minimizing the stubs in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bb5b44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectorization of Least square regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ecd64-ecb6-4ec2-9100-785c5286e610",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$\\newcommand{\\bfe}{\\mathbf{e}}$\n",
    "$\\newcommand{\\bfm}{\\mathbf{m}}$\n",
    "Recall that the magnitude of a vector $ \\|\\bfv\\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^n}$ has a similar form to the error function. This suggests that we can define an error vector with the signed error for each data point as it's elements\n",
    "\n",
    "$$ \\bfe = \\begin{bmatrix}y_1 - (mx_1 + c)\\\\ y_2 - (mx_2 + c)\\\\ \\vdots \\\\ y_n - (mx_n + c)\\end{bmatrix}$$\n",
    "\n",
    "Minimizing the total error is same as minimizing the square of error vector magnitude \n",
    "\n",
    "$$m^*, c^* = \\arg~\\min_{m, c} \\|\\bfe\\|^2$$\n",
    "\n",
    "While we are at at it let us define $\\bfx = [x_1; \\dots; x_n]$ to denote the vector of all x coordinates of the dataset and $\\bfy = [y_1; \\dots; y_n]$ to denote y coordinates. Then the error vector is:\n",
    "$$ \\bfe = \\bfy - (\\bfx m +  \\mathbf{1}_n c)$$ \n",
    "\n",
    "where $\\mathbf{1}_n$ is a n-D vector of all ones. Finally, we vectorize parameters of the line $\\bfm = [m; c]$. We will also need to horizontally concatenate $\\bfx$ and $\\mathbf{1}_n$. Let's call the result $\\bfX = [\\bfx, \\mathbf{1}_n] \\in \\bbR^{n \\times 2}$. Now, the error vector looks like this:\n",
    "\n",
    "$$ \\bfe = \\bfy - \\bfX \\bfm$$ \n",
    "\n",
    "Expanding the error magnitude:\n",
    "\n",
    "$$ \\|\\bfe\\|^2 = (\\bfy - \\bfX \\bfm)^\\top (\\bfy - \\bfX \\bfm)\n",
    "\\\\\n",
    "= \\bfy^\\top\\bfy + \\bfm^\\top \\bfX^\\top \\bfX \\bfm - 2\\bfy^\\top \\bfX \\bfm \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3625e-a558-4710-a818-d3ad99b19b38",
   "metadata": {},
   "source": [
    "---\n",
    "#### Homework 3: Problem 4 \n",
    "\n",
    "Expand $$(\\bfy - \\bfX \\bfm)^\\top (\\bfy - \\bfX \\bfm)$$ and show that it is equal to $$ \\bfy^\\top\\bfy + \\bfm^\\top \\bfX^\\top \\bfX \\bfm - 2\\bfy^\\top \\bfX \\bfm $$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfc6b7-df4c-4477-95c7-da34232de2ee",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Our minimization problem in vectorized form is:\n",
    "\n",
    "$$\\bfm^* = \\arg~\\min_{\\bfm} \\bfy^\\top\\bfy + \\bfm^\\top \\bfX^\\top \\bfX \\bfm - 2\\bfy^\\top \\bfX \\bfm $$\n",
    "\n",
    "This is a quadratic equation in $\\bfm$ that can be minimized by equating the derivate to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180abd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two rules of vector derivatives\n",
    "\n",
    "There are two conventions in vector derivatives:\n",
    "1. Gradient convention\n",
    "2. Jacobian convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66158d29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Gradient convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8784e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Under gradient convention the derivative of scalar-valued vector function function $f(\\bfx): \\bbR^n \\to \\bbR$ is\n",
    "defined as vertical stacking of element-wise derivatives\n",
    "\n",
    "$$\n",
    "\\newcommand{\\p}{\\partial}\n",
    "\\frac{\\p }{ \\p \\bfx} f(\\bfx) = \n",
    "\\begin{bmatrix}\\frac{\\p f(\\bfx)}{\\p x_1} \\\\ \\vdots \\\\ \\frac{\\p f(\\bfx)}{\\p x_n}\\end{bmatrix} \\in \\bbR^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaceefb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Jacobian convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d75410",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Under gradient convention the derivative of scalar-valued vector function function $f(\\bfx):  \\bbR^n \\to \\bbR$ is\n",
    "defined as horizontal stacking of element-wise derivatives\n",
    "\n",
    "$$\n",
    "\\newcommand{\\bff}{\\mathbf{f}}\n",
    "\\frac{\\p }{ \\p \\bfx} f(\\bfx) = \n",
    "\\begin{bmatrix}\\frac{\\p f(\\bfx)}{\\p x_1} & \\dots & \\frac{\\p f(\\bfx)}{\\p x_n}\\end{bmatrix} \\in \\bbR^{1 \\times n}$$\n",
    "\n",
    "For a vector-value vector function $\\bff(\\bfx): \\bbR^n \\to \\bbR^m$, Jacobian of $\\bff(\\bfx)$ is the vertical concatentation of gradients transposed, resulting in $m \\times n$ matrix\n",
    "$$\\newcommand{\\bfJ}{\\mathbf{J}}\n",
    "\\bfJ_\\bfx (\\bff(\\bfx)) = \\frac{\\p }{ \\p \\bfx} \\bff(\\bfx) = \n",
    "\\begin{bmatrix}\\frac{\\p f_1(\\bfx)}{\\p \\bfx} \\\\ \\dots \\\\ \\frac{\\p f_m(\\bfx)}{\\p \\bfx}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We will use Jacobian convention in this course, because it works nicely with chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99911060",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Derivative of a linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f62bfb",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "All scalar-valued linear functions of $\\bfx$ can be written in the form $f(\\bfx) = \\bfc^\\top \\bfx$.\n",
    "\n",
    "\\begin{align}\n",
    "\\newcommand{\\bfc}{\\mathbf{c}}\n",
    "\\newcommand{\\bfA}{\\mathbf{A}}\n",
    "\\frac{\\p }{ \\p \\bfx} \\bfc^\\top \\bfx = \\bfc^\\top\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f34c61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Derivative of a quadratic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577db0ce-de88-45f1-8b3d-fd6aded1231d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "All scalar-valued homogeneous quadratic functions of $\\bfx$ can be written in the form $f(\\bfx) = \\bfx^\\top \\bfA \\bfx$.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\p }{ \\p \\bfx} \\bfx^\\top \\bfA \\bfx = \\bfx^\\top (\\bfA + \\bfA^\\top)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ac7e5-39d7-4e49-9a8d-03dd1ae21c46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Homework 3: Problem 5\n",
    "\n",
    "Proof of above two derivatives is left as an exercises.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3f02d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Back to Least square regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635fe62-e801-43e3-9e87-13ddf67fb586",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\\begin{align}\n",
    "\\mathbf{0}^\\top &= \\frac{\\p }{\\p \\bfm} ( \\bfy^\\top\\bfy + \\bfm^\\top \\bfX^\\top \\bfX \\bfm - 2\\bfy^\\top \\bfX \\bfm)\\\\\n",
    "      &= 2 {\\bfm^*}^\\top \\bfX^\\top \\bfX  - 2\\bfy^\\top \\bfX\n",
    "\\end{align}\n",
    "\n",
    "This gives us the solution\n",
    "$$ \\bfm^* = (\\bfX^\\top \\bfX)^{-1} \\bfX^\\top \\bfy $$\n",
    "\n",
    "The symbol $\\bfV^{-1}$ is called inverse of matrix $\\bfV$.\n",
    "\n",
    "The term $(\\bfX^\\top \\bfX)^{-1} \\bfX^\\top$ is also called the pseudo-inverse of a matrix $\\bfX$, denoted as $\\bfX^\\dagger$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd5ac8-3607-4a2a-bb6b-5690a9a73aab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = salt_concentration_data.shape[0]\n",
    "bfx = salt_concentration_data[:, 2:3]\n",
    "bfy = salt_concentration_data[:, 1]\n",
    "bfX = np.hstack((bfx, np.ones((bfx.shape[0], 1))))\n",
    "bfX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb0a68-d642-4461-a025-f2442b89ab3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "bfm = np.linalg.inv(bfX.T @ bfX) @ bfX.T @ bfy\n",
    "print(bfm)\n",
    "bfm, *_ = np.linalg.lstsq(bfX, bfy, rcond=None)\n",
    "print(bfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a620f13-0f8c-4515-94fc-66b80eca067d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m = bfm.flatten()[0]\n",
    "c = bfm.flatten()[1]\n",
    "\n",
    "# Plot the points\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(salt_concentration_data[:, 2], salt_concentration_data[:, 1])\n",
    "ax.set_xlabel(r\"Roadway area $\\%$\")\n",
    "ax.set_ylabel(r\"Salt concentration (mg/L)\")\n",
    "x = salt_concentration_data[:, 2]\n",
    "y = m * x + c\n",
    "# Plot the points\n",
    "ax.plot(x, y, 'r-') # the line"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "center": false,
   "enable_chalkboard": true,
   "showNotes": true
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
