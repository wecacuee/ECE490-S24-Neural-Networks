{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "NTkGmFniOLXZ"
      },
      "outputs": [],
      "source": [
        "import scipy.signal as ss\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 3, 5],\n",
        "               [2, 7, 9],\n",
        "               [11, 13, 17]], dtype='f8')\n",
        "W = np.array([[19, 23],\n",
        "              [29, 31]], dtype='f8')\n",
        "# X = np.ones(12, dtype='f8').reshape(3,4) + 1\n",
        "# W = np.ones(6, dtype='f8').reshape(2,3) + 1"
      ],
      "metadata": {
        "id": "VidZ_XGXORLI"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\newcommand{\\bbR}{\\mathbb{R}}$\n",
        "A 2D convolution operation between the finite dimensional kernel weights $W \\in \\bbR^{K \\times L}$ and image $X\\in\\bbR^{I \\times J}$ is denoted as $Y = X * W\\in \\bbR^{I-K+1,J-L+1}$. Let the convolution kernel be smaller than the image in both the dimensions, $K < I$ and $L<J$.\n",
        "Then the 2D convolution operation for all $i \\in \\{1, \\dots, I-K+1\\}, j\\in \\{1, \\dots, J-K+1\\}$ is  defined as,\n",
        "\\begin{align}\n",
        "Y &= W * X\\\\\n",
        " Y_{i,j} &= \\sum_{k=1}^K \\sum_{l=1}^L W_{k,l} X_{i+K-k,j+L-l} \\\\\n",
        " &= \\sum_{p=i}^{K+i-1} \\sum_{q=j}^{L+j-1} X_{p, q} W_{i+K-p,j+L-q}\n",
        "\\end{align}\n",
        "The two expressions are equivalent and can obtained by variable substitution of $p=i+K-k$ and $q=j+L-l$."
      ],
      "metadata": {
        "id": "Qc4jF-jBgUXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_padding(padding):\n",
        "  if padding is None:\n",
        "    padding = [(0, s-1) for s in W.shape]\n",
        "  return padding\n",
        "def t_convolve2d(X, W, padding=None):\n",
        "  padding = handle_padding(padding)\n",
        "  Xpadded = np.pad(X, padding)\n",
        "  Y = ss.convolve2d(Xpadded, W, mode='valid')\n",
        "  return Y\n",
        "\n",
        "Y = t_convolve2d(X, W)\n",
        "Y, (X[:2, :2] * W).sum(), (X[:2, :2] * W[::-1, ::-1]).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY96TG2mOSiJ",
        "outputId": "28520a1b-6724-43dd-e6bf-c78f7f6eebf3"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 297.,  570.,  362.],\n",
              "        [ 765., 1100.,  670.],\n",
              "        [ 718.,  896.,  527.]]),\n",
              " 363.0,\n",
              " 297.0)"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding vector jacobian product\n",
        "$\\newcommand{\\p }{\\partial}$\n",
        "$\\newcommand{\\pfrac}[2]{\\frac{\\p {#1}}{\\p {#2}}}$\n",
        "Let the eventual loss function be $l(Y)$ and we are given the backpropagated gradient $\\pfrac{l}{Y}$. We want to find the vector jacobian products, $\\pfrac{l}{X}$ and $\\pfrac{l}{W}$.\n",
        "\n",
        "\\begin{align}\n",
        "\\pfrac{}{W} l(Y) &= \\sum_{i=1}^{I-K+1} \\sum_{j=1}^{J-L+1} \\pfrac{l}{Y_{i,j}} \\pfrac{Y_{i,j}}{W}\\\\\n",
        "\\pfrac{}{X} l(Y) &= \\sum_{i=1}^{I-K+1} \\sum_{j=1}^{J-L+1} \\pfrac{l}{Y_{i,j}} \\pfrac{Y_{i,j}}{X}\n",
        "\\end{align}\n",
        "\n",
        "### Finding $\\pfrac{l}{W}$\n",
        "Our main task is to find $\\pfrac{Y_{i,j}}{W}$. Let's find the $(k,l)$th element of this derivative,\n",
        "\n",
        "\\begin{align}\n",
        "\\pfrac{Y_{i,j}}{W_{k,l}}= \\pfrac{}{W_{k,l}} \\sum_{k=1}^K \\sum_{l=1}^L W_{k,l} X_{i+K-k,j+L-l} &=\n",
        "X_{i+K-k,j+L-l} &\\text{ for all} i \\le I, k \\le K, j \\le J, l \\le L\n",
        "\\end{align}\n",
        "\n",
        "That was easy. Let's put it back in the equation one above to find $\\pfrac{l}{W_{k,l}}$\n",
        "\\begin{align}\n",
        "\\pfrac{}{W_{k,l}} l(Y) &= \\sum_{i=1}^{I-K+1} \\sum_{j=1}^{J-L+1} \\pfrac{l}{Y_{i,j}}X_{i+K-k,j+L-l}.\n",
        "\\end{align}\n",
        "\n",
        "$\\newcommand{\\calM}{\\mathcal{M}}$\n",
        "Define mirroring operator of an image $Z \\in \\bbR^{I \\times J}$ as $\\calM(Z)$ as,\n",
        "\\begin{align}\n",
        "\\calM(Z)_{i,j} &= Z_{I+1-i,J+1-j}\n",
        "\\end{align}\n",
        "Rewrite $\\pfrac{l}{W_{k,l}}$ in terms of mirrored $\\calM(\\pfrac{l}{W})_{m,n}$,\n",
        "\\begin{align}\n",
        "\\calM\\left(\\pfrac{}{W} l(Y)\\right)_{m,n}\n",
        "&= \\sum_{i=1}^{I-K+1} \\sum_{j=1}^{J-L+1} \\pfrac{l}{Y_{i,j}}X_{i+K-(K+1-m),j+L-(L+1-n)}\\\\\n",
        "&= \\sum_{i=1}^{I-K+1} \\sum_{j=1}^{J-L+1} \\pfrac{l}{Y_{i,j}}X_{m-1+i,n-1+j}\\\\\n",
        "\\end{align}\n",
        "Change the indexing variables $p=I-K+2-i$ and $q=J-L+2-j$,\n",
        "\\begin{align}\n",
        "\\calM\\left(\\pfrac{}{W} l(Y)\\right)_{m,n}\n",
        " &= \\sum_{p=1}^{I-K+1} \\sum_{q=1}^{J-L+1} \\calM\\left(\\pfrac{l}{Y}\\right)_{p,q} X_{m+I-K+1-p,n+J-L+1-n}.\n",
        "\\end{align}\n",
        "Left hand side of the above equation is a convolution operation, with $\\pfrac{l}{Y}$ as the kernel,\n",
        "\\begin{align}\n",
        "\\calM\\left(\\pfrac{}{W} l(Y)\\right) &=  \\calM(\\pfrac{l}{Y}) * X\\\\\n",
        "\\implies \\pfrac{}{W} l(Y) &= \\calM\\left( \\calM(\\pfrac{l}{Y}) * X \\right)\n",
        "\\end{align}\n",
        "\n",
        "It turns out the following is equivalent,\n",
        "\\begin{align}\n",
        "\\pfrac{}{W} l(Y) &= \\pfrac{l}{Y} * \\calM(X)\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "EbnfvJmagck9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding $\\pfrac{l}{X}$\n",
        "Let's try to do the same with $\\pfrac{l}{X}$ by finding $\\pfrac{Y_{i,j}}{X_{p,q}}$ first,\n",
        "\\begin{align}\n",
        "\\pfrac{Y_{i,j}}{X_{p,q}} &= \\pfrac{}{X_{p,q}} \\sum_{p=i}^{K+i-1} \\sum_{q=j}^{L+j-1} X_{p, q} W_{i+K-p,j+L-q}\\\\\n",
        "&= \\begin{cases}\n",
        "W_{i+K-p,j+L-q} & \\text{ if } 1 \\le i+K-p \\le K \\text { and } 1 \\le j+L-q \\le J\\\\\n",
        "0 & \\text{ otherwise }\n",
        "\\end{cases}.\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Using this we can find the $(p,q)$th element of $\\pfrac{l}{X}$ as,\n",
        "\\begin{align}\n",
        "\\pfrac{}{X_{p,q}} l(Y) &= \\sum_{i=1}^{I-K+1} \\sum_{j=1}^{J-L+1} \\pfrac{l}{Y_{i,j}} \\pfrac{Y_{i,j}}{X_{p,q}}\\\\\n",
        "&= \\sum_{i=1}^{p-K} \\sum_{j=1}^{p-L} \\pfrac{l}{Y_{i,j}} . 0\n",
        "+ \\sum_{i=p-K+1}^{p} \\sum_{j=q-L+1}^{q} \\pfrac{l}{Y_{i,j}}  W_{i+K-p,j+L-q}\\\\\n",
        "&\\qquad + \\sum_{i=p+1}^{I-K+1} \\sum_{j=q+1}^{J-L+1} \\pfrac{l}{Y_{i,j}} . 0\\\\\n",
        "&= \\sum_{i=p-K+1}^{p} \\sum_{j=q-L+1}^{q} \\pfrac{l}{Y_{i,j}}  W_{i+K-p,j+L-q}.\n",
        "\\end{align}\n",
        "In the last step we reduced the scope  of summation where $\\pfrac{Y_{i,j}}{X_{p,q}}$ is non-zero. However, due to the change of variables, some of the indexing on $\\pfrac{l}{Y_{i,j}}$ is invalid. To remedy that define a padded gradient $G \\in \\bbR^{I+K-1 \\times J+L-1}$ that is defined as,\n",
        "\\begin{align}\n",
        "G_{m, n} &= \\begin{cases}\n",
        "0 & \\text{ if } n \\le K-1 \\text{ or } m \\le L-1 \\\\\n",
        "\\pfrac{l}{Y_{m-K+1,n-L+1}} & \\text{ if } K \\le m \\le I \\text{ and } L \\le n \\le J\\\\\n",
        "0 & \\text{ if } I+1 \\le n \\le I+K-1 \\text{ or } J+1 \\le m \\le J+L-1 \\\\\n",
        "\\end{cases}\\\\\n",
        "\\implies \\pfrac{l}{Y_{i,j}} &= G_{i+K-1,j+L-1}\n",
        "\\end{align}\n",
        "Put this back into $\\pfrac{l}{X_{p,q}}$,\n",
        "\\begin{align}\n",
        "\\pfrac{}{X_{p,q}} l(Y)\n",
        "&= \\sum_{i=p-K+1}^{p} \\sum_{j=q-L+1}^{q} G_{i+K-1,j+L-1}  W_{i+K-p,j+L-q}.\n",
        "\\end{align}\n",
        "Let us do a change of variable to write the above sum in terms of $W$ indices $k = i+K-p$ or $i=p-K+k$. Similarly, $l = j+L-q$ or $j=q-L+l$. This leads to,\n",
        "\\begin{align}\n",
        "\\pfrac{}{X_{p,q}} l(Y) &= \\sum_{k=1}^{K} \\sum_{l=1}^{L}  W_{k,l}G_{p+k-1,q+l-1}\n",
        "\\end{align}\n",
        "The above is almost a convolution except that the $k$ and $l$ indices appear with a positive sign. To remedy that we use the mirror operator of $W$,\n",
        "\\begin{align}\n",
        "\\calM(W)_{k,l} &= W_{K+1-k,L+1-l} \\text{ for all } k \\in \\{1, \\dots, K\\}, i \\in \\{ 1, \\dots, L\\}\n",
        "\\end{align}\n",
        "Use this to write $\\pfrac{l}{X_{p,q}}$ in terms of $\\calM(G)$,\n",
        "\\begin{align}\n",
        "\\pfrac{}{X_{p,q}} l(Y) &= \\sum_{k=1}^{K} \\sum_{l=1}^{L}  \\calM(W)_{K+1-k,L+1-l} G_{p+k-1,q+l-1}\\\\\n",
        "&= \\sum_{m=1}^{K} \\sum_{n=1}^{L}  \\calM(W)_{m,n} G_{p+K-m,q+L-n}\n",
        "\\end{align}\n",
        "\n",
        "The  right hand side the convolution oeprator,\n",
        "\\begin{align}\n",
        "\\pfrac{}{X} l(Y) &= G * \\calM(W)\n",
        "\\end{align}\n",
        "\n",
        "Equivalently,\n",
        "\\begin{align}\n",
        "\\implies \\pfrac{}{X} l(Y) &= \\calM\\left( \\calM(G) * X \\right)\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "siQ3ph2tS3jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unpad(X, padding):\n",
        "  return X[tuple(slice(s, -e) for s, e in padding)]\n",
        "\n",
        "def mirror(X):\n",
        "  return X[::-1, ::-1]\n",
        "\n",
        "def convolve2d_vjp(X, W, dl__dY, padding=None):\n",
        "  padding = handle_padding(padding)\n",
        "  Xpadded = np.pad(X, padding)\n",
        "\n",
        "  # Additional zero padding\n",
        "  b_padding = [(s-1, s-1)  for s in W.shape]\n",
        "  # The default padding needs to be reversed for dl__dY\n",
        "  dl__dY_bpadded = np.pad(dl__dY, b_padding)\n",
        "\n",
        "  dl__dW = ss.convolve2d(dl__dY, mirror(Xpadded), mode='valid')\n",
        "  assert dl__dW.shape == W.shape\n",
        "\n",
        "  # use the valid convolutions with custom padded X and W\n",
        "  dl__dXp = ss.convolve2d(dl__dY_bpadded, mirror(W), mode='valid')\n",
        "  assert dl__dXp.shape == Xpadded.shape\n",
        "  dl__dX = unpad(dl__dXp, padding)\n",
        "\n",
        "  return (dl__dX, dl__dW)"
      ],
      "metadata": {
        "id": "CEBeX6K0Om_9"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_convolve2d_vjp(X, W, dl__dY, h=1e-12, **kw):\n",
        "  # We are going to perturb each pixel of X by a small amount h\n",
        "  # and observe the corresponding change in Y\n",
        "  dl__dX = np.empty_like(X)\n",
        "  Y = t_convolve2d(X, W, **kw) # Y before any perturbation\n",
        "  for i in range(X.shape[0]):\n",
        "    for j in range(X.shape[1]):\n",
        "      Xp = X.copy()\n",
        "      Xp[i, j] += h # perturbed image at pixel (i,j)\n",
        "      Yp = t_convolve2d(Xp, W, **kw) # convolution result when perturbed\n",
        "      # numerical derivative of image Y with respect to pixel X[i,j]\n",
        "      dY__dXij = (Yp - Y) / h\n",
        "      # numerical derivative of loss l with respect to pixel X[i,j]\n",
        "      dl__dX[i, j] = (dl__dY * dY__dXij).sum()\n",
        "\n",
        "  dl__dW = np.empty_like(W)\n",
        "  for i in range(W.shape[0]):\n",
        "    for j in range(W.shape[1]):\n",
        "      Wp = W.copy()\n",
        "      Wp[i, j] += h # perturbed kernel at pixel (i,j)\n",
        "      Yp = t_convolve2d(X, Wp, **kw) # convolution result when perturbed\n",
        "      # numerical derivative of image Y with respect to pixel X[i,j]\n",
        "      dY__dWij = (Yp - Y) / h\n",
        "      # numerical derivative of loss l with respect to pixel X[i,j]\n",
        "      dl__dW[i, j] = (dl__dY * dY__dWij).sum()\n",
        "\n",
        "  return dl__dX, dl__dW\n",
        "\n",
        "random_dl__dY = np.ones(Y.shape)\n",
        "nvjp_X, nvjp_W =numerical_convolve2d_vjp(X, W, random_dl__dY)\n",
        "nvjp_X, nvjp_W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d5dHJq6Onhg",
        "outputId": "6af95e31-4ecb-4666-be36-c177215b4889"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 30.97966328,  60.02665032,  60.02665032],\n",
              "        [ 54.00124792, 101.8634066 , 101.97709344],\n",
              "        [ 53.88756108, 101.8634066 , 101.8634066 ]]),\n",
              " array([[45.98632586, 58.94662536],\n",
              "        [54.17177817, 67.87104212]]))"
            ]
          },
          "metadata": {},
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vjp_X, vjp_W = convolve2d_vjp(X, W, random_dl__dY)\n",
        "vjp_X, vjp_W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyEXtLAHSFKB",
        "outputId": "54afd40e-0814-49b0-852b-2cc8745203b1"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 31.,  60.,  60.],\n",
              "        [ 54., 102., 102.],\n",
              "        [ 54., 102., 102.]]),\n",
              " array([[46., 59.],\n",
              "        [54., 68.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert np.allclose(nvjp_X, nvjp_X, atol=1e-1, rtol=1e-2)\n",
        "assert np.allclose(nvjp_W, nvjp_W, atol=1e-1, rtol=1e-2)"
      ],
      "metadata": {
        "id": "EPxf-J3w5q2u"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actually try a random one\n",
        "random_dl__dY = np.random.rand(*Y.shape)\n",
        "all([\n",
        "    np.allclose(nvjp, cvjp, atol=1e-1, rtol=1e-2)\n",
        "    for nvjp, cvjp in zip(\n",
        "        numerical_convolve2d_vjp(X, W, random_dl__dY),\n",
        "        convolve2d_vjp(X, W, random_dl__dY))\n",
        "    ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wev22BiPSTrT",
        "outputId": "4707eda3-6d97-4103-e652-53efe7d7ea70"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All random values\n",
        "X = np.random.rand(3,4)\n",
        "W = np.random.rand(2,2)\n",
        "Y = t_convolve2d(X, W)\n",
        "random_dl__dY = np.random.rand(*Y.shape)\n",
        "all([\n",
        "    np.allclose(nvjp, cvjp, rtol=1e-2)\n",
        "    for nvjp, cvjp in zip(\n",
        "        numerical_convolve2d_vjp(X, W, random_dl__dY),\n",
        "        convolve2d_vjp(X, W, random_dl__dY))\n",
        "    ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86JiitfudkUL",
        "outputId": "a0bf45e9-08c6-4fa5-9863-66f3ef15dde1"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_convolve2d_vjp():\n",
        "    # With random shapes\n",
        "    W_shape = np.random.randint(1, 9, size=(2,))\n",
        "    maxW = max(W_shape)\n",
        "    X_shape = np.random.randint(maxW, 40, size=(2,))\n",
        "    # All random values\n",
        "    X = np.random.rand(*X_shape)\n",
        "    W = np.random.rand(*W_shape)\n",
        "    Y = t_convolve2d(X, W)\n",
        "    random_dl__dY = np.random.rand(*Y.shape)\n",
        "    return all([\n",
        "    np.allclose(nvjp, cvjp, rtol=1e-2, atol=1e-3)\n",
        "        for nvjp, cvjp in zip(\n",
        "            numerical_convolve2d_vjp(X, W, random_dl__dY),\n",
        "            convolve2d_vjp(X, W, random_dl__dY))\n",
        "        ])\n",
        "assert test_convolve2d_vjp()"
      ],
      "metadata": {
        "id": "o7P8vcd-d0NN"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the test a hundred times just to make sure\n",
        "for _ in range(100):\n",
        "  assert test_convolve2d_vjp()"
      ],
      "metadata": {
        "id": "mKivfmYohwzw"
      },
      "execution_count": 259,
      "outputs": []
    }
  ]
}