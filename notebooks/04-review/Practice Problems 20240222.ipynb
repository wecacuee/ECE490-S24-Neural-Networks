{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c40374-8b85-45b9-a5d8-490fb5ddb0ac",
   "metadata": {},
   "source": [
    "# Practice problems for Midterm 1 ECE 490/590 Spring 2024\n",
    "\n",
    "#### Date: Feb 29, 2024\n",
    "#### Instructor: Vikas Dhiman (vikas.dhiman@maine.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa633e3-568b-4959-b69f-8976f2891214",
   "metadata": {},
   "source": [
    "1. Total marks are 75.\n",
    "2. Total time allowed is 75 min.\n",
    "3. One page 8\"x11\" cheatsheet is allowed.\n",
    "4. Calculators are allowed.\n",
    "5. Computers are not allowed. You must know approximately know what the Python code will output. Minor formatting errors will not be penalized.\n",
    "\n",
    "<style>\n",
    "@media print {\n",
    "    .pagebreak { page-break-before: always; } /* page-break-after works, as well */\n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c26784d-8694-4566-ab99-c626b1eaf047",
   "metadata": {},
   "source": [
    "#### 1. Write your name here: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5072b60-7feb-4b4e-8e7e-403c6847b51b",
   "metadata": {},
   "source": [
    "#### 2. Write your email here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f159f-8957-425b-ba8a-8d0d4152a1fd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\calA}{{\\cal A}}\n",
    "\\newcommand{\\calB}{{\\cal B}}\n",
    "\\newcommand{\\calC}{{\\cal C}}\n",
    "\\newcommand{\\calD}{{\\cal D}}\n",
    "\\newcommand{\\calE}{{\\cal E}}\n",
    "\\newcommand{\\calF}{{\\cal F}}\n",
    "\\newcommand{\\calG}{{\\cal G}}\n",
    "\\newcommand{\\calH}{{\\cal H}}\n",
    "\\newcommand{\\calI}{{\\cal I}}\n",
    "\\newcommand{\\calJ}{{\\cal J}}\n",
    "\\newcommand{\\calK}{{\\cal K}}\n",
    "\\newcommand{\\calL}{{\\cal L}}\n",
    "\\newcommand{\\calM}{{\\cal M}}\n",
    "\\newcommand{\\calN}{{\\cal N}}\n",
    "\\newcommand{\\calO}{{\\cal O}}\n",
    "\\newcommand{\\calP}{{\\cal P}}\n",
    "\\newcommand{\\calQ}{{\\cal Q}}\n",
    "\\newcommand{\\calR}{{\\cal R}}\n",
    "\\newcommand{\\calS}{{\\cal S}}\n",
    "\\newcommand{\\calT}{{\\cal T}}\n",
    "\\newcommand{\\calU}{{\\cal U}}\n",
    "\\newcommand{\\calV}{{\\cal V}}\n",
    "\\newcommand{\\calW}{{\\cal W}}\n",
    "\\newcommand{\\calX}{{\\cal X}}\n",
    "\\newcommand{\\calY}{{\\cal Y}}\n",
    "\\newcommand{\\calZ}{{\\cal Z}}\n",
    "\\newcommand{\\setA}{\\textsf{A}}\n",
    "\\newcommand{\\setB}{\\textsf{B}}\n",
    "\\newcommand{\\setC}{\\textsf{C}}\n",
    "\\newcommand{\\setD}{\\textsf{D}}\n",
    "\\newcommand{\\setE}{\\textsf{E}}\n",
    "\\newcommand{\\setF}{\\textsf{F}}\n",
    "\\newcommand{\\setG}{\\textsf{G}}\n",
    "\\newcommand{\\setH}{\\textsf{H}}\n",
    "\\newcommand{\\setI}{\\textsf{I}}\n",
    "\\newcommand{\\setJ}{\\textsf{J}}\n",
    "\\newcommand{\\setK}{\\textsf{K}}\n",
    "\\newcommand{\\setL}{\\textsf{L}}\n",
    "\\newcommand{\\setM}{\\textsf{M}}\n",
    "\\newcommand{\\setN}{\\textsf{N}}\n",
    "\\newcommand{\\setO}{\\textsf{O}}\n",
    "\\newcommand{\\setP}{\\textsf{P}}\n",
    "\\newcommand{\\setQ}{\\textsf{Q}}\n",
    "\\newcommand{\\setR}{\\textsf{R}}\n",
    "\\newcommand{\\setS}{\\textsf{S}}\n",
    "\\newcommand{\\setT}{\\textsf{T}}\n",
    "\\newcommand{\\setU}{\\textsf{U}}\n",
    "\\newcommand{\\setV}{\\textsf{V}}\n",
    "\\newcommand{\\setW}{\\textsf{W}}\n",
    "\\newcommand{\\setX}{\\textsf{X}}\n",
    "\\newcommand{\\setY}{\\textsf{Y}}\n",
    "\\newcommand{\\setZ}{\\textsf{Z}}\n",
    "\\newcommand{\\bfa}{\\mathbf{a}}\n",
    "\\newcommand{\\bfb}{\\mathbf{b}}\n",
    "\\newcommand{\\bfc}{\\mathbf{c}}\n",
    "\\newcommand{\\bfd}{\\mathbf{d}}\n",
    "\\newcommand{\\bfe}{\\mathbf{e}}\n",
    "\\newcommand{\\bff}{\\mathbf{f}}\n",
    "\\newcommand{\\bfg}{\\mathbf{g}}\n",
    "\\newcommand{\\bfh}{\\mathbf{h}}\n",
    "\\newcommand{\\bfi}{\\mathbf{i}}\n",
    "\\newcommand{\\bfj}{\\mathbf{j}}\n",
    "\\newcommand{\\bfk}{\\mathbf{k}}\n",
    "\\newcommand{\\bfl}{\\mathbf{l}}\n",
    "\\newcommand{\\bfm}{\\mathbf{m}}\n",
    "\\newcommand{\\bfn}{\\mathbf{n}}\n",
    "\\newcommand{\\bfo}{\\mathbf{o}}\n",
    "\\newcommand{\\bfp}{\\mathbf{p}}\n",
    "\\newcommand{\\bfq}{\\mathbf{q}}\n",
    "\\newcommand{\\bfr}{\\mathbf{r}}\n",
    "\\newcommand{\\bfs}{\\mathbf{s}}\n",
    "\\newcommand{\\bft}{\\mathbf{t}}\n",
    "\\newcommand{\\bfu}{\\mathbf{u}}\n",
    "\\newcommand{\\bfv}{\\mathbf{v}}\n",
    "\\newcommand{\\bfw}{\\mathbf{w}}\n",
    "\\newcommand{\\bfx}{\\mathbf{x}}\n",
    "\\newcommand{\\bfy}{\\mathbf{y}}\n",
    "\\newcommand{\\bfz}{\\mathbf{z}}\n",
    "\\newcommand{\\bfalpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\bfgamma}{\\boldsymbol{\\gamma}}\n",
    "\\newcommand{\\bfdelta}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\bfepsilon}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\bfzeta}{\\boldsymbol{\\zeta}}\n",
    "\\newcommand{\\bfeta}{\\boldsymbol{\\eta}}\n",
    "\\newcommand{\\bftheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\bfiota}{\\boldsymbol{\\iota}}\n",
    "\\newcommand{\\bfkappa}{\\boldsymbol{\\kappa}}\n",
    "\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\bfmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\bfnu}{\\boldsymbol{\\nu}}\n",
    "\\newcommand{\\bfomicron}{\\boldsymbol{\\omicron}}\n",
    "\\newcommand{\\bfpi}{\\boldsymbol{\\pi}}\n",
    "\\newcommand{\\bfrho}{\\boldsymbol{\\rho}}\n",
    "\\newcommand{\\bfsigma}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\bftau}{\\boldsymbol{\\tau}}\n",
    "\\newcommand{\\bfupsilon}{\\boldsymbol{\\upsilon}}\n",
    "\\newcommand{\\bfphi}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\bfpsi}{\\boldsymbol{\\psi}}\n",
    "\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n",
    "\\newcommand{\\bfxi}{\\boldsymbol{\\xi}}\n",
    "\\newcommand{\\bfell}{\\boldsymbol{\\ell}}\n",
    "\\newcommand{\\bfA}{\\mathbf{A}}\n",
    "\\newcommand{\\bfB}{\\mathbf{B}}\n",
    "\\newcommand{\\bfC}{\\mathbf{C}}\n",
    "\\newcommand{\\bfD}{\\mathbf{D}}\n",
    "\\newcommand{\\bfE}{\\mathbf{E}}\n",
    "\\newcommand{\\bfF}{\\mathbf{F}}\n",
    "\\newcommand{\\bfG}{\\mathbf{G}}\n",
    "\\newcommand{\\bfH}{\\mathbf{H}}\n",
    "\\newcommand{\\bfI}{\\mathbf{I}}\n",
    "\\newcommand{\\bfJ}{\\mathbf{J}}\n",
    "\\newcommand{\\bfK}{\\mathbf{K}}\n",
    "\\newcommand{\\bfL}{\\mathbf{L}}\n",
    "\\newcommand{\\bfM}{\\mathbf{M}}\n",
    "\\newcommand{\\bfN}{\\mathbf{N}}\n",
    "\\newcommand{\\bfO}{\\mathbf{O}}\n",
    "\\newcommand{\\bfP}{\\mathbf{P}}\n",
    "\\newcommand{\\bfQ}{\\mathbf{Q}}\n",
    "\\newcommand{\\bfR}{\\mathbf{R}}\n",
    "\\newcommand{\\bfS}{\\mathbf{S}}\n",
    "\\newcommand{\\bfT}{\\mathbf{T}}\n",
    "\\newcommand{\\bfU}{\\mathbf{U}}\n",
    "\\newcommand{\\bfV}{\\mathbf{V}}\n",
    "\\newcommand{\\bfW}{\\mathbf{W}}\n",
    "\\newcommand{\\bfX}{\\mathbf{X}}\n",
    "\\newcommand{\\bfY}{\\mathbf{Y}}\n",
    "\\newcommand{\\bfZ}{\\mathbf{Z}}\n",
    "\\newcommand{\\bfGamma}{\\boldsymbol{\\Gamma}}\n",
    "\\newcommand{\\bfDelta}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\bfTheta}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\bfPi}{\\boldsymbol{\\Pi}}\n",
    "\\newcommand{\\bfSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\bfUpsilon}{\\boldsymbol{\\Upsilon}}\n",
    "\\newcommand{\\bfPhi}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\bfPsi}{\\boldsymbol{\\Psi}}\n",
    "\\newcommand{\\bfOmega}{\\boldsymbol{\\Omega}}\n",
    "\\newcommand{\\bbA}{\\mathbb{A}}\n",
    "\\newcommand{\\bbB}{\\mathbb{B}}\n",
    "\\newcommand{\\bbC}{\\mathbb{C}}\n",
    "\\newcommand{\\bbD}{\\mathbb{D}}\n",
    "\\newcommand{\\bbE}{\\mathbb{E}}\n",
    "\\newcommand{\\bbF}{\\mathbb{F}}\n",
    "\\newcommand{\\bbG}{\\mathbb{G}}\n",
    "\\newcommand{\\bbH}{\\mathbb{H}}\n",
    "\\newcommand{\\bbI}{\\mathbb{I}}\n",
    "\\newcommand{\\bbJ}{\\mathbb{J}}\n",
    "\\newcommand{\\bbK}{\\mathbb{K}}\n",
    "\\newcommand{\\bbL}{\\mathbb{L}}\n",
    "\\newcommand{\\bbM}{\\mathbb{M}}\n",
    "\\newcommand{\\bbN}{\\mathbb{N}}\n",
    "\\newcommand{\\bbO}{\\mathbb{O}}\n",
    "\\newcommand{\\bbP}{\\mathbb{P}}\n",
    "\\newcommand{\\bbQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\bbR}{\\mathbb{R}}\n",
    "\\newcommand{\\bbS}{\\mathbb{S}}\n",
    "\\newcommand{\\bbT}{\\mathbb{T}}\n",
    "\\newcommand{\\bbU}{\\mathbb{U}}\n",
    "\\newcommand{\\bbV}{\\mathbb{V}}\n",
    "\\newcommand{\\bbW}{\\mathbb{W}}\n",
    "\\newcommand{\\bbX}{\\mathbb{X}}\n",
    "\\newcommand{\\bbY}{\\mathbb{Y}}\n",
    "\\newcommand{\\bbZ}{\\mathbb{Z}}\n",
    "$$\n",
    "$\\newenvironment{bm}{\\begin{bmatrix}}{\\end{bmatrix}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d559973",
   "metadata": {},
   "source": [
    "## Python Basics\n",
    "\n",
    "The midterm will be on paper, no computers will be allowed. Make sure you know what the python code output should be.\n",
    "\n",
    "Python questions will be restriced to content covered in Python_1.ipynb,  Python_2.ipynb, NumpyTutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8525873-b1ad-4be1-94d1-606095e85df0",
   "metadata": {},
   "source": [
    "#### Q1. What will the following code print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7ae58c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello' \"ECE\". pi is 3.142\n"
     ]
    }
   ],
   "source": [
    "hello = \"'Hello'\"\n",
    "name = '\"ECE\"'\n",
    "pi = 3.1419\n",
    "print(f'{hello:s} {name}. pi is {pi:.03f}')  # string formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a4622-b2b2-419c-a603-d41cd3003119",
   "metadata": {},
   "source": [
    "#### Q2. What will the following code print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1905200-14e2-4f55-a04b-cd296dd4ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "xs = [1, 2, 3, 'hello', [4, 5, 6]]    # Create a list\n",
    "print(xs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e178701-325e-4e3a-ba20-a03403cd3134",
   "metadata": {},
   "source": [
    "#### Q3. What will the following code print?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "682f2ecd-cf1c-4b69-948e-772b2d8511da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4]\n"
     ]
    }
   ],
   "source": [
    "nums = list(range(5))    # range is a built-in function that creates a list of integers\n",
    "print(nums[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f98fa-ba06-4d9b-b7b8-93e676175036",
   "metadata": {},
   "source": [
    "#### Q4. Which code is faster for very large lists and dictionaries ? Option 1 or Option 2? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "419ef45b-3085-4a5d-82d9-05f1b517a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furry\n",
      "furry\n"
     ]
    }
   ],
   "source": [
    "# Code Option 1:\n",
    "d = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data\n",
    "print(d['dog'])\n",
    "# Code option 2:\n",
    "keys = ['cat', 'dog'] # Create the dictionary with keys as lists\n",
    "values = ['cute', 'furry'] # # Create the dictionary with values as lists\n",
    "print(values[keys.index('dog')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e2535-e051-4582-aef6-b6521259aa84",
   "metadata": {},
   "source": [
    "#### Q5. Which code is faster for very large lists and dictionaries ? Option 1 or Option 2? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d166522-961f-40f5-aed3-699fad4938bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "furry\n",
      "furry\n"
     ]
    }
   ],
   "source": [
    "# Code Option 1:\n",
    "d = {0: 'cute', 1: 'furry'}  # Create a new dictionary with some data\n",
    "print(d[1])\n",
    "# Code option 2:\n",
    "values = ['cute', 'furry'] # # Create the dictionary with values as lists\n",
    "print(values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003dba8-090c-443c-8639-597a8c0f97b2",
   "metadata": {},
   "source": [
    "#### Q6. What is the output of the following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86765df1-5e0f-424d-98e6-9629892823f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "class Value:\n",
    "    def __init__(self, v):\n",
    "        self.v = v\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return self.v * other\n",
    "\n",
    "print(Value(3) + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ac162-6e58-43fe-86b0-7bee84f07fdd",
   "metadata": {},
   "source": [
    "## Numpy basics\n",
    "\n",
    "Python questions will be restriced to content covered in NumpyTutorial.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5df094-3c63-4754-aaae-a0c01975145f",
   "metadata": {},
   "source": [
    "#### Q7: What is the output of the following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a6c0649-167a-4956-9720-0d3e0dc867e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 5],\n",
       "       [2, 4, 6]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([[5, 6]])\n",
    "np.concatenate((x.T, y.T), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb596d3-9422-4328-948a-f885eddcec8a",
   "metadata": {},
   "source": [
    "#### Q8. What is the output of the following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2daf1b89-bf3f-479c-b563-a0c6de7bee65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17],\n",
       "       [39]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([[5, 6]])\n",
    "x @ y.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331dfd8f-cdff-498c-b0f3-ba1bd2c9d154",
   "metadata": {},
   "source": [
    "#### Q9. What is the output of the following code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0290d466-04a4-4b4c-9ac9-f19ca047cb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 39])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [3, 4]])\n",
    "y = np.array([[5, 6]])\n",
    "(x * y).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e975979d-ecb6-4358-86bc-0fdc3d50ee8d",
   "metadata": {},
   "source": [
    "#### Q9: What is the output of the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1084da9c-15f8-42d0-a2ea-182ffe837f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47 98]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_as = np.array([[2, 3], # a_1\n",
    "                [3, 5] # a_2               \n",
    "               ])\n",
    "bs = np.array([[7, 11], # b_1\n",
    "               [11, 13] # b_2\n",
    "              ])\n",
    "print((_as * bs).sum(axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa7838-e5e1-483c-a270-fc57b3826b45",
   "metadata": {},
   "source": [
    "$[47, 98]$\n",
    "\n",
    "Because $[47, 98] = [2\\times 7+3\\times 11, 3\\times 11+5\\times 13]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d27100-bcff-44dd-bdaf-e5d16869b952",
   "metadata": {},
   "source": [
    "#### Q9: What is the output of the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4878077e-a09a-4a89-9d7a-5f1687db4f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [2, 3],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "mat = np.array([[1, 2],\n",
    "                 [3, 4],\n",
    "                 [5, 6]])\n",
    "column_vector = np.array([[1],\n",
    "                       [-1],\n",
    "                       [0]])\n",
    "\n",
    "mat + column_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6fc16-3de4-4ac8-86e6-03a3088d8ba6",
   "metadata": {},
   "source": [
    "## Perceptron variations\n",
    "\n",
    "$\\newcommand{\\bfa}{\\mathbf{a}}$\n",
    "#### Q10. Show that for any vector dot product with itself is same as its magnitude\n",
    "$\\bfa = [a_1, a_2, \\dots, a_n]$, it's magnitude squared is same as dot product with itself i.e. $\\|\\bfa\\|^2 = \\bfa^\\top \\bfa$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e621ade3-bd54-4772-b53c-87a23695c6bb",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-088f9689ec190305",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "A10. The mangitude of n-D vector is given by $\\|\\bfa\\| = \\sqrt{a_1^2 + a_2^2 + \\dots + a_n^2}$ and dot product the vector with itself is given by\n",
    "$\\bfa^\\top \\bfa = a_1 a_1 + a_2 a_2 + \\dots + a_n a_n = a_1^2 + a_2^2 + \\dots + a_n^2$. Squaring the magnitude gives us $\\|\\bfa\\|^2 = a_1^2 + a_2^2 + \\dots + a_n^2$, which is same as $\\bfa^\\top \\bfa$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887eff0e-b3e5-4be6-a037-bc3f1d87fc0e",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bfe}{\\mathbf{e}}$\n",
    "$\\newcommand{\\bfm}{\\mathbf{m}}$\n",
    "$\\newcommand{\\bfX}{\\mathbf{X}}$\n",
    "$\\newcommand{\\bfy}{\\mathbf{y}}$\n",
    "#### Q12. Convert the following scalar equation into vector form. \n",
    "\n",
    "Your end result should contain the vectors $\\bfm = [m; c]$, $\\bfy = [y_1; y_2; \\dots; y_n]$ and $\\bfx = [x_1; x_2, \\dots, x_n]$. You can define other vectors and matrices as needed, included a vector of ones like $\\mathbb{1}_n$.\n",
    "\n",
    "$$ e(m, c, (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)) = (y_1 - (x_1 m + c))^2 + (y_2 - (x_2 m + c))^2 + \\dots + (y_n - (x_n m + c))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960b458-dc4c-4a35-81fd-167474e73618",
   "metadata": {
    "tags": []
   },
   "source": [
    "A12. \n",
    "Recall that the magnitude of a vector $ \\|\\bfv\\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^n}$ has a similar form to the error function. This suggests that we can define an error vector with the signed error for each data point as it's elements\n",
    "\n",
    "$$ \\bfe = \\begin{bmatrix}y_1 - (mx_1 + c)\\\\ y_2 - (mx_2 + c)\\\\ \\vdots \\\\ y_n - (mx_n + c)\\end{bmatrix}$$\n",
    "\n",
    "The total error is same as minimizing the square of error vector magnitude which is further same as vector product with itself.\n",
    "\n",
    "$$  e(m, c, (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)) = \\|\\bfe\\|^2 = \\bfe^\\top \\bfe$$\n",
    "\n",
    "Let us define $\\bfx = [x_1; \\dots; x_n]$ to denote the vector of all x coordinates of the dataset and $\\bfy = [y_1; \\dots; y_n]$ to denote y coordinates. Then the error vector is:\n",
    "$$ \\bfe = \\bfy - (\\bfx m +  \\mathbf{1}_n c)$$ \n",
    "\n",
    "where $\\mathbf{1}_n$ is a n-D vector of all ones. Finally, we vectorize parameters of the line $\\bfm = [m; c]$. We will also need to horizontally concatenate $\\bfx$ and $\\mathbf{1}_n$. Let's call the result $\\bfX = [\\bfx, \\mathbf{1}_n] \\in \\bbR^{n \\times 2}$. Now, the error vector looks like this:\n",
    "\n",
    "$$ \\bfe = \\bfy - \\bfX \\bfm$$ \n",
    "\n",
    "Expanding the error magnitude:\n",
    "\n",
    "$$ \\|\\bfe\\|^2 = (\\bfy - \\bfX \\bfm)^\\top (\\bfy - \\bfX \\bfm)\n",
    "\\\\\n",
    "= \\bfy^\\top\\bfy + \\bfm^\\top \\bfX^\\top \\bfX \\bfm - 2\\bfy^\\top \\bfX \\bfm \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99590ad4",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bfz}{\\mathbf{z}}$\n",
    "#### Q13: Convert the following scalar equation into vector form.\n",
    "Convert the following scalar equation into vector form. Your end result should contain $\\bfm = [a; b; c]$, $\\bfz = [z_1; z_2; \\dots; z_n]$, $\\bfy = [y_1; y_2; \\dots; y_n]$ and $\\bfx = [x_1; x_2, \\dots, x_n]$. You can define other vectors and matrices as needed, included a vector of all ones like $\\mathbb{1}_n$.\n",
    "\n",
    "$$ e(a, b, c, (x_1, y_1, z_1), (x_2, y_2, z_2), \\dots, (x_n, y_n, z_n)) = (z_1 - (x_1 a + y_1 b + c))^2 + (z_2 - (x_2 a + y_2 b + c))^2 + \\dots + (z_n - (x_n a + y_n b + c))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0dd55",
   "metadata": {
    "tags": []
   },
   "source": [
    "A13: A variation of A12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89874c8-f201-44e4-a12b-5505c744c4bd",
   "metadata": {},
   "source": [
    "#### Q14 Convert the following vector equation into even more vectorized form. \n",
    "$\\newcommand{\\bfq}{\\mathbf{q}}$\n",
    "\n",
    "\n",
    "$$ e(m_0, \\bfm, (\\bfx_1, y_1), (\\bfx_2, y_2), \\dots, (\\bfx_n, y_n)) = (y_1 - (\\bfx_1^\\top \\bfm + m_0))^2 + (y_2 - (\\bfx_2^\\top \\bfm + m_0))^2 + \\dots + (y_n - (\\bfx_n^\\top \\bfm + m_0))^2$$\n",
    "\n",
    "where $\\bfm = [m_1; m_2; \\dots; m_p] \\in \\bbR^p$ is a p-dimensional vector and $\\bfx_i = [x_{i1}; x_{i2}; \\dots; x_{ip}] \\in \\bbR^p$ are p-dimensional vectors for all $i = \\{1, 2, \\dots n\\}$\n",
    "\n",
    "Your end result should contain $\\bfq = [m_0, m_1, m_2, \\dots, m_p] \\in \\bbR^{p+1}$, $\\bfy = [y_1; y_2; \\dots; y_n]\\in \\bbR^n$ and \n",
    "\n",
    "$$\\bfX = \n",
    "\\begin{bm}x_{11} & x_{12} & \\dots & x_{1p}\\\\\n",
    "x_{21} & x_{22} & \\dots & x_{2p} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "x_{n1} & x_{n2} & \\dots & x_{np} \\end{bm} = \\begin{bm}\n",
    "\\bfx_1^\\top \\\\\n",
    "\\bfx_2^\\top \\\\\n",
    "\\vdots\\\\\n",
    "\\bfx_n^\\top \\end{bm}\n",
    "\\in \\bbR^{n \\times p}$$. \n",
    "\n",
    "You can define other vectors and matrices as needed, included a vector of all ones like $\\mathbb{1}_n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cbf5e-9b5c-4be9-8dc2-d98a83939c55",
   "metadata": {
    "tags": []
   },
   "source": [
    "A15. \n",
    "Recall that the magnitude of a vector $ \\|\\bfv\\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^n}$ has a similar form to the error function. This suggests that we can define an error vector with the signed error for each data point as it's elements\n",
    "\n",
    "$$ \\bfe = \\begin{bmatrix}y_1 - (\\bfx_1^\\top\\bfm + m_0)\\\\ y_2 - (\\bfx_2^\\top\\bfm + m_0)\\\\ \\vdots \\\\ y_n - (\\bfx_2^\\top\\bfm_2 + m_0)\\end{bmatrix}$$\n",
    "\n",
    "The total error is same as minimizing the square of error vector magnitude which is further same as vector product with itself.\n",
    "\n",
    "$$  e(m_0, \\bfm, (\\bfx_1, y_1), (\\bfx_2, y_2), \\dots, (\\bfx_n, y_n)) = \\|\\bfe\\|^2 = \\bfe^\\top \\bfe$$\n",
    "\n",
    "Let us define $\\bfX = [\\bfx_1^\\top; \\dots; \\bfx_n^\\top]$ to denote the vector of all x coordinates of the dataset and $\\bfy = [y_1; \\dots; y_n]$ to denote y coordinates. Then the error vector is:\n",
    "$$ \\bfe = \\bfy - (\\mathbf{1}_n m_0 + \\bfX \\bfm)$$ \n",
    "\n",
    "where $\\mathbf{1}_n$ is a n-D vector of all ones. Finally, we call parameters of the line $\\bfq = [m_0; \\bfm]$. We will also need to horizontally concatenate $\\bfX$ and $\\mathbf{1}_n$. Let's call the result $\\bar{\\bfX} = [\\mathbf{1}_n, \\bfX] \\in \\bbR^{n \\times (p+1)}$. Now, the error vector looks like this:\n",
    "\n",
    "$$ \\bfe = \\bfy - \\bar{\\bfX} \\bfq$$ \n",
    "\n",
    "Expanding the error magnitude:\n",
    "$\\newcommand{\\bbfX}{\\bar{\\bfX}}$\n",
    "$$ \\|\\bfe\\|^2 = (\\bfy - \\bbfX \\bfq)^\\top (\\bfy - \\bbfX \\bfq)\n",
    "\\\\\n",
    "= \\bfy^\\top\\bfy + \\bfq^\\top \\bbfX^\\top \\bbfX \\bfq - 2\\bfy^\\top \\bbfX \\bfq \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d402c",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bfW}{\\mathbf{W}}$\n",
    "$\\DeclareMathOperator{\\Diag}{Diag}$\n",
    "#### Q16: Convert the following scalar equation into vector form.\n",
    " Your end result should contain $\\bfm = [m; c]$, the matrix $\\bfW = \\Diag([w_1; w_2; \\dots; w_n])$, $\\bfy = [y_1; y_2; \\dots; y_n]$ and $\\bfx = [x_1; x_2, \\dots, x_n]$. You can define other vectors and matrices as needed, included a vector of all ones like $\\mathbb{1}_n$.\n",
    "\n",
    "\n",
    "$$ e(m, c, (x_1, y_1, w_1), (x_2, y_2, w_2), \\dots, (x_n, y_n, w_n)) = w_1^2(y_1 - (x_1 m + c))^2 + w_2^2(y_2 - (x_2 m + c))^2 + \\dots + w_n^2(y_n - (x_n m + c))^2$$\n",
    "\n",
    "The matrix $\\bfW$ is defined as $\\Diag([w_1; w_2; \\dots; w_n])$ which indicates that $\\bfW$ is diagonal matrix of $[w_1; w_2; \\dots; w_n]$.\n",
    "\n",
    "$$ \\bfW = \\Diag([w_1; w_2; \\dots; w_n]) = \n",
    "\\begin{bm}w_1 & 0 & \\dots & 0\\\\\n",
    "0 & w_2 & \\dots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & w_n\\end{bm}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19fcf21",
   "metadata": {
    "tags": []
   },
   "source": [
    "A16:\n",
    "\n",
    "Recall that the magnitude of a vector $ \\|\\bfv\\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^n}$ has a similar form to the error function. This suggests that we can define an error vector with the signed error for each data point as it's elements\n",
    "\n",
    "$$ \\bfe = \\begin{bmatrix}y_1 - (mx_1 + c)\\\\ y_2 - (mx_2 + c)\\\\ \\vdots \\\\ y_n - (mx_n + c)\\end{bmatrix}$$\n",
    "and  let $\\bfW = \\Diag([w_1; w_2; \\dots; w_n])$.\n",
    "\n",
    "Note that \n",
    "$$ \\bfW \\bfe = \\begin{bmatrix}w_1(y_1 - (mx_1 + c))\\\\ w_2(y_2 - (mx_2 + c))\\\\ \\vdots \\\\ w_3(y_n - (mx_n + c))\\end{bmatrix}$$\n",
    "\n",
    "The total error is same as the square of error vector magnitude \n",
    "\n",
    "$$ e(m, c, (x_1, y_1, w_1), (x_2, y_2, w_2), \\dots, (x_n, y_n, w_n)) = w_1^2(y_1 - (x_1 m + c))^2 + w_2^2(y_2 - (x_2 m + c))^2 + \\dots + w_n^2(y_n - (x_n m + c))^2 = \\|\\bfW \\bfe\\|^2$$\n",
    "\n",
    "The square of error vector magnitude is same as dot product with itself,\n",
    "$$ \\|\\bfW \\bfe\\|^2 = (\\bfW\\bfe)^\\top(\\bfW\\bfe) = \\bfe^\\top \\bfW^\\top \\bfW \\bfe$$\n",
    "\n",
    "Let us define $\\bfx = [x_1; \\dots; x_n]$ to denote the vector of all x coordinates of the dataset and $\\bfy = [y_1; \\dots; y_n]$ to denote y coordinates. Then the error vector is:\n",
    "$$ \\bfe = \\bfy - (\\bfx m +  \\mathbf{1}_n c)$$ \n",
    "\n",
    "where $\\mathbf{1}_n$ is a n-D vector of all ones. Finally, we vectorize parameters of the line $\\bfm = [m; c]$. We will also need to horizontally concatenate $\\bfx$ and $\\mathbf{1}_n$. Let's call the result $\\bfX = [\\bfx, \\mathbf{1}_n] \\in \\bbR^{n \\times 2}$. Now, the error vector looks like this:\n",
    "\n",
    "$$ \\bfe = \\bfy - \\bfX \\bfm$$ \n",
    "\n",
    "Expanding the error magnitude:\n",
    "\n",
    "$$ \\|\\bfW\\bfe\\|^2 = (\\bfy - \\bfX \\bfm)^\\top \\bfW^\\top\\bfW (\\bfy - \\bfX \\bfm)\n",
    "\\\\\n",
    "= \\bfy^\\top\\bfW^\\top\\bfW\\bfy + \\bfm^\\top \\bfX^\\top \\bfW^\\top\\bfW\\bfX \\bfm - 2\\bfy^\\top \\bfW^\\top\\bfW\\bfX \\bfm \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4812bd1c",
   "metadata": {},
   "source": [
    "#### Q17: Using vector derivatives find the minimum of the following vector quadratic function \n",
    "\n",
    "\n",
    "$$\\arg~\\min_{\\bfm} e(\\bfm) = \\bfy^\\top\\bfW^\\top\\bfW\\bfy + \\bfm^\\top \\bfX^\\top \\bfW^\\top\\bfW\\bfX \\bfm - 2\\bfy^\\top \\bfW^\\top\\bfW\\bfX \\bfm $$\n",
    "\n",
    "The dimensions of the each of the variables are given $\\bfm \\in \\bbR^p$, $\\bfy \\in \\bbR^n$, $\\bfW \\in \\bbR^{n \\times n}$, $\\bfX \\in \\bbR^{n \\times p}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e375921-9d87-4cae-afda-af0d2f73b5c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "A17:\n",
    "$\\newcommand{\\p}{\\partial}$\n",
    "$\\newcommand{\\bfb}{\\mathbf{b}}$\n",
    "\\begin{align}\n",
    "\\mathbf{0}^\\top &= \\frac{\\p }{\\p \\bfm} (\\bfy^\\top\\bfW^\\top\\bfW\\bfy + \\bfm^\\top \\bfX^\\top \\bfW^\\top\\bfW\\bfX \\bfm - 2\\bfy^\\top \\bfW^\\top\\bfW\\bfX \\bfm)\\\\\n",
    "      &= 2 {\\bfm^*}^\\top \\bfX^\\top \\bfW^\\top \\bfW \\bfX  - 2\\bfy^\\top \\bfW^\\top \\bfW \\bfX\n",
    "\\end{align}\n",
    "\n",
    "This gives us the solution\n",
    "$$ \\bfm^* = (\\bfX^\\top \\bfW^\\top \\bfW \\bfX)^{-1} \\bfX^\\top \\bfW^\\top \\bfW \\bfy $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3deaf3-5ea3-4312-a50a-56c2b1a64892",
   "metadata": {},
   "source": [
    "## Vector derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67fdb4-5585-473d-9698-e246332376f7",
   "metadata": {},
   "source": [
    "#### Q17: Define a gradient, Jacobian and Hessian in terms of partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce939b75-8727-400a-9096-ee5aaa5ff3d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "$\\newcommand{\\bbR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\calH}{\\mathcal{H}}$\n",
    "$\\newcommand{\\p}{\\partial}$\n",
    "$\\newcommand{\\pfxixj}[2]{\\frac{\\p^2 f}{\\p x_{#1} \\p x_{#2}}}$\n",
    "$\\newcommand{\\calJ}{\\mathcal{J}}$\n",
    "\n",
    "Gradient is defined for a scalar-valued vector function $f(\\bfx)$ ($\\bfx \\in \\bbR^n$ and $f(\\bfx) \\in \\bbR$) as the arrangement of\n",
    "partial derivatives as a vector\n",
    "\n",
    "$$ \\nabla_\\bfx f(\\bfx) = \\begin{bmatrix} \\frac{\\p f}{\\p x_1} \\\\ \\vdots \\\\ \\frac{\\p f}{\\p x_n}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Jacobian is defined for a vector-valued vector function $\\bff(\\bfx)$ ($\\bfx \\in \\bbR^n$ and $\\bff(\\bfx)\\in\\bbR^m$) as the arrangement of the partial derviatives as the following matrix,\n",
    "\n",
    "$$ \\frac{\\p \\bff(\\bfx)}{\\p \\bfx}= \\calJ_\\bfx \\bff(\\bfx) = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\p f_1}{\\p x_1} & \\frac{\\p f_1}{\\p x_2} & \\dots & \\frac{\\p f_1}{\\p x_n}\\\\\n",
    "\\frac{\\p f_2}{\\p x_1} & \\frac{\\p f_2}{\\p x_2} & \\dots & \\frac{\\p f_2}{\\p x_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\p f_m}{\\p x_1} & \\frac{\\p f_m}{\\p x_2} & \\dots & \\frac{\\p f_m}{\\p x_n}\n",
    "\\end{bmatrix}  \\in \\bbR^{m \\times n}\n",
    "$$\n",
    "\n",
    "Note that Jacobian can be written in terms of gradients of each element of the vector function.\n",
    "\n",
    "$$ \\calJ_\\bfx \\bff(\\bfx) = \\begin{bmatrix}\n",
    "(\\nabla_\\bfx f_1(\\bfx))^\\top \\\\\n",
    "(\\nabla_\\bfx f_2(\\bfx))^\\top \\\\\n",
    "\\vdots \\\\\n",
    "(\\nabla_\\bfx f_m(\\bfx))^\\top \n",
    "\\end{bmatrix} \\in \\bbR^{m \\times n}$$\n",
    "\n",
    "Hessian matrix of a scalar-valued vector function $f: \\bbR^n \\to \\bbR$ is defined as the following arrangement of second derivatives,\n",
    "$$ \\calH f(\\bfx) = \\begin{bmatrix}\n",
    "\\pfxixj11 & \\pfxixj12 & \\dots & \\pfxixj1n \\\\\n",
    "\\pfxixj21 & \\pfxixj22 & \\dots & \\pfxixj2n \\\\\n",
    "\\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "\\pfxixj{n}1 & \\pfxixj{n}2 & \\dots & \\pfxixj{n}n \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "It is sometimes also written as $\\nabla^2 f(\\bfx) $, and hessian can be computed by taking the Jacobian of the gradient,\n",
    "$$ \\calH f(\\bfx) = \\calJ^\\top ( \\nabla f(\\bfx) )$$\n",
    "\n",
    "If the second partial derivatives are continuous then the Hessian matrix is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30144d4e-eb5a-41c8-91b3-487bc6e8e04a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Q18:\n",
    "\n",
    "$\\newcommand{\\bfb}{\\mathbf{b}}$\n",
    "Find the derivative of $f(\\bfx) = (\\bfx - \\bfa_1)^\\top A (\\bfx - \\bfa_2)$ with respecto to $\\bfx$.\n",
    "\n",
    "You can assume $A \\in \\bbR^{n\\times n}$ to be symmetric. The size of vectors are $\\bfx, \\bfa_1, \\bfa_2, \\bfa_3, \\bfb \\in \\bbR^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987e4f0-5f42-45f4-82f5-40e4cc47f1b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "A18\n",
    "\n",
    "$$ f(\\bfx) = (\\bfx - \\bfa_1)^\\top A (\\bfx - \\bfa_2)\\\\\n",
    "= \\bfx^\\top A \\bfx - \\bfa_1^\\top A \\bfx - \\bfx^\\top A \\bfa_2 + \\bfa_1^\\top A\\bfa_2\\\\\n",
    "$$\n",
    "\n",
    "Note that $\\bfx^\\top A \\bfa_2 $ is a scalar. That's why we can replace it with its transpose $\\bfx^\\top A \\bfa_2 = \\bfa_2^\\top A \\bfx$\n",
    "\n",
    "$$\n",
    "= \\bfx^\\top A \\bfx - (\\bfa_1 + \\bfa_2)^\\top A \\bfx + \\bfa_1^\\top A\\bfa_2$$ \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial \\bfx} = 2\\bfx^\\top A - (\\bfa_1 + \\bfa_2)^\\top A\\\\\n",
    "= (2\\bfx - (\\bfa_1 + \\bfa_2))^\\top A$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c0173-7114-4d07-b6e3-ec31577428d7",
   "metadata": {},
   "source": [
    "#### Q20\n",
    "\n",
    "$\\newcommand{\\bfc}{\\mathbf{c}}$\n",
    "$\\newcommand{\\p}{\\partial}$\n",
    "Show that for $\\bfc, \\bfx \\in \\bbR^n$\n",
    "$\\newcommand{\\bfc}{\\mathbf{c}}$\n",
    "$\\newcommand{\\bfA}{\\mathbf{A}}$\n",
    "\\begin{align}\n",
    "\\frac{\\p }{ \\p \\bfx} \\bfc^\\top \\bfx = \\bfc^\\top\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080da5bb-e446-4211-a497-cd8d9dbe28d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "A20:\n",
    "Let $\\bfc = [c_1, c_2, \\dots, c_n]$ and $\\bfx = [x_1, x_2, \\dots x_n]$\n",
    "\n",
    "Let $f(\\bfx) = \\bfc^\\top \\bfx = c_1 x_1 + c_2 x_2 + \\dots c_n x_n$\n",
    "$\\newcommand{\\p}{\\partial}$\n",
    "\n",
    "$$\\frac{\\p f}{\\p x_1} = c_1\\\\\n",
    "\\frac{\\p f}{\\p x_2} = c_2\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p f}{\\p x_n} = c_n\\\\\n",
    "$$\n",
    "By Jacobian convention, we arrange the partial derivatives in a row vector:\n",
    "\n",
    "$$\n",
    "\\frac{\\p }{ \\p \\bfx} \\bfc^\\top \\bfx = \n",
    "\\begin{bm} \\frac{\\p f}{\\p x_1} & \\frac{\\p f}{\\p x_2} & \\dots & \\frac{\\p f}{\\p x_n}\\end{bm}\n",
    "\\\\\n",
    "= \\begin{bm} c_1 & c_2 & \\dots & c_n\\end{bm} = \\bfc^\\top \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d0816-731b-4bca-a070-2f734cb2dcf4",
   "metadata": {},
   "source": [
    "#### Q21:\n",
    "\n",
    "Show that for $\\bfA \\in \\bbR^{n \\times n}$, $\\bfx \\in \\bbR^n$\n",
    "\\begin{align}\n",
    "\\frac{\\p }{ \\p \\bfx} \\bfA \\bfx = \\bfA\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff75c1e-b2b7-4dcc-8f84-73c44b9f9404",
   "metadata": {
    "tags": []
   },
   "source": [
    "A21:\n",
    "Let $\\bfx = [x_1; x_2; \\dots x_n]$\n",
    "\n",
    "Let $\\bfA = \\begin{bm} a_{11} & a_{12}  & \\dots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "a_{n1} & a_{n2} & \\dots & a_{nn} \\end{bm} \n",
    "= \\begin{bm} \\bfa_1^\\top \\\\ \\bfa_2^\\top \\\\ \\vdots \\\\ \\bfa_n^\\top \\end{bm}$,\n",
    "where $\\bfa_i^\\top  \\in \\bbR^{1 \\times n}$ are the row vectors of  matrix $\\bfA$.\n",
    "\n",
    "$\\newcommand{\\bff}{\\mathbf{f}}$\n",
    "Then $$ \\bfA \\bfx = \\begin{bm} \\bfa_1^\\top \\\\ \\bfa_2^\\top \\\\ \\vdots \\\\ \\bfa_n^\\top \\end{bm}\\bfx \n",
    "= \\begin{bm} \\bfa_1^\\top \\bfx \\\\ \\bfa_2^\\top \\bfx \\\\ \\vdots \\\\ \\bfa_n^\\top\\bfx  \\end{bm} $$\n",
    "\n",
    "Let \n",
    "$$\\bff(\\bfx) = \n",
    "\\begin{bm} f_1(\\bfx) \\\\ f_2(\\bfx) \\\\ \\vdots\\\\ f_n(\\bfx) \\end{bm}\n",
    "= \\bfA \\bfx = \\begin{bm} \\bfa_1^\\top \\bfx \\\\ \\bfa_2^\\top \\bfx \\\\ \\vdots \\\\ \\bfa_n^\\top\\bfx  \\end{bm}$$\n",
    "\n",
    "By Jacobian convention we arrange the partial derivatives of each function component column-wise\n",
    "\n",
    "$$ \\frac{\\p \\bff(\\bfx)}{\\p \\bfx} = \n",
    "\\begin{bm} \\frac{\\p f_1(\\bfx)}{\\p \\bfx} \\\\\n",
    "\\frac{\\p f_2(\\bfx)}{\\p \\bfx} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\p f_n(\\bfx)}{\\p \\bfx} \\end{bm} \n",
    "= \\begin{bm} \\frac{\\p \\bfa_1^\\top \\bfx}{\\p \\bfx} \\\\\n",
    "\\frac{\\p \\bfa_2^\\top \\bfx}{\\p \\bfx} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\p \\bfa_2^\\top \\bfx}{\\p \\bfx} \\end{bm} \n",
    "= \\begin{bm} \\bfa_1^\\top \\\\ \\bfa_2^\\top \\\\ \\vdots \\\\ \\bfa_n^\\top \\end{bm}\n",
    "= \\bfA\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae15c5c9-3982-46a4-bd98-1cdeb623d8e8",
   "metadata": {},
   "source": [
    "#### Q22:\n",
    "$\\newcommand{\\bfg}{\\mathbf{g}}$\n",
    "Show that for $\\bfx \\in \\bbR^n$ amd $\\bfA \\in \\bbR^{n \\times n}$\n",
    "$\\newcommand{\\bfA}{\\mathbf{A}}$\n",
    "\\begin{align}\n",
    "\\frac{\\p }{ \\p \\bfx} \\bfx^\\top \\bfA \\bfx = \\bfx^\\top (\\bfA^\\top + \\bfA)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1f07c-3891-4f30-909a-69f0e5c3f371",
   "metadata": {
    "tags": []
   },
   "source": [
    "A22:\n",
    "\n",
    "For product of any two vectors\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfx} \\bfx^\\top \\bfy = \\bfy^\\top\n",
    "\\end{align}\n",
    "If $\\bfy$ is a function of $\\bfx$, then\n",
    "\\begin{align}\n",
    "\\frac{\\p }{\\p \\bfx} \\bfx^\\top \\bfy &= \\bfy^\\top + \n",
    "\\left(\\frac{\\p }{\\p \\bfy} \\bfx^\\top \\bfy \\right) \\left(\\frac{\\p \\bfy }{\\p \\bfx}  \\right)\\\\\n",
    "&= \\bfy^\\top + \\bfx^\\top \\left(\\frac{\\p \\bfy }{\\p \\bfx}\\right)\n",
    "\\end{align}\n",
    "If $\\bfy = \\bfA \\bfx$, then \n",
    "$$\\frac{\\p \\bfy}{\\p \\bfx} = \\frac{\\p }{\\p \\bfx} \\bfA \\bfx = \\bfA$$\n",
    "and \n",
    "\n",
    "$$\\frac{\\p }{\\p \\bfx} \\bfx^\\top \\bfA \\bfx = \n",
    "\\bfy^\\top + \\bfx^\\top \\left(\\frac{\\p \\bfy }{\\p \\bfx}\\right)\n",
    "= \\bfx^\\top \\bfA^\\top + \\bfx^\\top \\bfA \n",
    "= \\bfx^\\top (\\bfA^\\top + \\bfA)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6455d414-9dbf-4328-95c9-9a5bb1b7424c",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff58c0a-8fbf-4074-a2ad-3075fbd5efb1",
   "metadata": {},
   "source": [
    "#### Q23:\n",
    "\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "You are given 2D points and corresponding labels as a training dataset $\\{ (x_1, y_1, l_1), (x_2, y_2, l_2), \\dots, (x_n, y_n, l_n) \\}$, where $x_i \\in \\bbR$, $y_i \\in \\bbR$ and the labels $l_i \\in \\{-1, 1\\}$. Use the model \n",
    "$\\hat{l}_i = \\sign(y_i - (m x_i + c))$ to construct a Hinge loss (or error) function. Find the gradient of the Hinge  loss function with respect to the vector $\\bfm = [m; c]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22e867-2ce2-4322-816c-6b653b9e33c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "A23\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$$ e(y_i, x_i; m,c) = \\begin{cases}\n",
    "0 &\\text{ if }\\sign(y_i - (m x_i + c))  = l_i\\\\\n",
    "|y_i - (m x_i + c)| &\\text{ if }  \\sign(y_i - (m x_i + c))  \\ne l_i\n",
    "\\end{cases}$$\n",
    "$\\newcommand{\\bfm}{\\mathbf{m}}$\n",
    "$$\\bfm = \\begin{bmatrix}m \\\\ c\\end{bmatrix}$$\n",
    "\n",
    "$$ e(y_i, x_i;\\bfm) = \\begin{cases}\n",
    "0 &\\text{ if } \\sign( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm)  = l_i\\\\\n",
    "|y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm| &\\text{ if }  \\sign( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm)  \\ne l_i\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "If $l_i \\in \\{-1, 1\\}$, then  $\\sign( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm)  = l_i $ is same as saying\n",
    "$ l_i( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm) > 0$.\n",
    "\n",
    "\n",
    "\n",
    "$$ e(y_i, x_i;\\bfm) = \\begin{cases}\n",
    "0 &\\text{ if } l_i( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm) > 0\\\\\n",
    "|l_i( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm)| &\\text{ if } l_i( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm) < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "Also when $z < 0$, then $|z| = -z$. \n",
    "\n",
    "$$ e(y_i, x_i;\\bfm) = \\begin{cases}\n",
    "0 &\\text{ if } l_i( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm) > 0\\\\\n",
    "-l_i( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm) &\\text{ if } l_i( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm) < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "$$ \\nabla_\\bfm e(y_i, x_i;\\bfm) = \\begin{cases}\n",
    "0 &\\text{ if } l_i( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm) > 0\\\\\n",
    "l_i(\\begin{bmatrix} x_i& 1\\end{bmatrix}) &\\text{ if } l_i( y_i - \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm) < 0\n",
    "\\end{cases} $$\n",
    "\n",
    "---\n",
    "It is acceptable to leave the answer in above form.\n",
    "\n",
    "---\n",
    "\n",
    "$$ e(y_i, x_i;\\bfm) =  \\max\\{0, - l_i (y_i -  \\begin{bmatrix} x_i& 1\\end{bmatrix}\\bfm)\\} $$\n",
    "$$ \\nabla_\\bfm e(y_i, x_i;\\bfm) = \\max\\{0, l_i(\\begin{bmatrix} x_i& 1\\end{bmatrix})\\} $$\n",
    "\n",
    "---\n",
    "It is acceptable to leave the answer in above form.\n",
    "\n",
    "---\n",
    "\n",
    "$\\newcommand{\\bfx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bfy}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bfe}{\\mathbf{e}}$\n",
    "$\\newcommand{\\bfl}{\\mathbf{l}}$\n",
    "$\\newcommand{\\bfone}{\\mathbb{1}}$\n",
    "For the entire dataset, we have $\\bfy = [y_1; \\dots; y_n]$ and $\\bfx = [x_1; \\dots; x_n]$, $\\bfl = [l_1; \\dots; l_n]$ the average error is:\n",
    "$$ e(\\bfx, \\bfy; \\bfm) = \\frac{1}{n}{\\bfone_n^\\top}\\max\\{0, - \\bfl \\odot (\\bfy - \\begin{bmatrix}\\bfx  &  \\bfone_n\\end{bmatrix}\\bfm )\\},$$\n",
    "\n",
    "where $\\odot$ is the element-wise product. and $\\bfone_n$ is a vector of ones.\n",
    "\n",
    "and the average gradient is:\n",
    "$$ \\nabla_\\bfm^\\top e(\\bfx, \\bfy; \\bfm) = \\frac{1}{n}{\\bfone_n^\\top}\\max\\{0,  \\bfl \\odot ( \\begin{bmatrix}\\bfx  &  \\bfone_n\\end{bmatrix} )\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fd2d5-7118-4204-bf6f-2fcda8578e43",
   "metadata": {},
   "source": [
    "#### Q24\n",
    "\n",
    "You are given p-D points $\\bfx_i \\in \\bbR^p$ and corresponding labels as a training dataset $\\{ (\\bfx_1, l_1), (\\bfx_2, l_2), \\dots, (\\bfx_n, l_n) \\}$, where $\\bfx_i \\in \\bbR^p$, and the labels $l_i \\in \\{-1, 1\\}$. Use the model \n",
    "$\\hat{l}_i = \\sign(\\bfx_i^\\top \\bfm + m_0))$ to construct a Hinge loss (or error) function. Find the gradient of the Hinge loss function with respect to the vector $\\bfq = [m_0; \\bfm]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7b84c-a84b-49c6-b805-e356256092c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "A24:\n",
    "\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$$ e(m_0, \\bfm; \\bfx_i) = \\begin{cases}\n",
    "0 &\\text{ if }\\sign(\\bfx_i^\\top \\bfm + m_0)  = l_i\\\\\n",
    "|\\bfx_i^\\top \\bfm + m_0| &\\text{ if }  \\sign(\\bfx_i^\\top \\bfm + m_0)  \\ne l_i\n",
    "\\end{cases}$$\n",
    "$\\newcommand{\\bfm}{\\mathbf{m}}$\n",
    "$$ e(y_i, x_i; m, c) = \\begin{cases}\n",
    "0 &\\text{ if } \\sign(\\bfx_i^\\top \\bfm + m_0)  = l_i\\\\\n",
    "|\\bfx_i^\\top \\bfm + m_0| &\\text{ if }  \\sign(\\bfx_i^\\top \\bfm + m_0)  \\ne l_i\n",
    "\\end{cases}$$\n",
    "\n",
    "$$\\bfq = \\begin{bmatrix}m_0 \\\\ \\bfm\\end{bmatrix}$$\n",
    "\n",
    "$$ e(m_0, \\bfm;\\bfx_i) = \\begin{cases}\n",
    "0 &\\text{ if } \\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}\\bfq  = l_i\\\\\n",
    "|\\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}\\bfq| &\\text{ if }  \\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}\\bfq  \\ne l_i\n",
    "\\end{cases}$$\n",
    "\n",
    "$$ \\nabla_\\bfq e(m_0, \\bfm;\\bfx_i) = \\begin{cases}\n",
    "0 &\\text{ if } \\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}\\bfq  = l_i\\\\\n",
    "|\\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}| &\\text{ if }  \\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}\\bfq  \\ne l_i\n",
    "\\end{cases}$$\n",
    "\n",
    "---\n",
    "It is acceptable to leave the answer in above form.\n",
    "\n",
    "---\n",
    "\n",
    "If $l_i \\in \\{-1, 1\\}$, then we can write\n",
    "\n",
    "$$ e(m_0, \\bfm;\\bfx_i) =  \\max\\{0, - l_i (\\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix}\\bfq)\\} $$\n",
    "$$ \\nabla_\\bfm e(m_0, \\bfm;\\bfx_i) = \\max\\{0, - l_i (\\begin{bmatrix} 1 & \\bfx_i^\\top\\end{bmatrix})\\} $$\n",
    "\n",
    "---\n",
    "It is acceptable to leave the answer in above form.\n",
    "\n",
    "---\n",
    "\n",
    "$\\newcommand{\\bfx}{\\mathbf{x}}$\n",
    "$\\newcommand{\\bfy}{\\mathbf{y}}$\n",
    "$\\newcommand{\\bfe}{\\mathbf{e}}$\n",
    "$\\newcommand{\\bfl}{\\mathbf{l}}$\n",
    "$\\newcommand{\\bfone}{\\mathbb{1}}$\n",
    "For the entire dataset, we have $\\bfX = [\\bfx_1^\\top; \\dots; \\bfx_n^\\top]$, $\\bfl = [l_1; \\dots; l_n]$ the average error is:\n",
    "$$ e(\\bfm; \\bfX, \\bfl) = \\frac{1}{n}{\\bfone_n^\\top}\\max\\{0, - \\bfl \\odot ( \\begin{bmatrix}\\bfone_n & \\bfX \\end{bmatrix}\\bfq )\\}$$\n",
    "\n",
    "and the average gradient is:\n",
    "$$ \\nabla_\\bfm^\\top e(\\bfm; \\bfX, \\bfl) = \\frac{1}{n}{\\bfone_n^\\top}\\max\\{0,  \\bfl \\odot ( \\begin{bmatrix}  \\bfone_n & \\bfX \\end{bmatrix} )\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab2f1d-511d-4573-8d3e-409267abd8dd",
   "metadata": {},
   "source": [
    "#### Q25: Define Positive definite, Negative definite and Indefinite matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831f29c-f3b0-4eb9-8cb5-3e450e4e1348",
   "metadata": {},
   "source": [
    "##### Positiive definite\n",
    "\n",
    "A square matrix $A \\in \\bbR^{n \\times n}$ is called positive definite if for all $\\bfx \\in \\bbR^n$, $\\bfx^\\top A \\bfx \\succ 0$.\n",
    "\n",
    "##### Negative definite\n",
    "\n",
    "A square matrix $A \\in \\bbR^{n \\times n}$ is called negative definite if for all $\\bfx \\in \\bbR^n$, $\\bfx^\\top A \\bfx \\prec 0$.\n",
    "\n",
    "##### Indefinite\n",
    "\n",
    "A square matrix $A \\in \\bbR^{n \\times n}$ is called indefinite if it is neither positive definite nor negative definite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a362d01f-8891-4749-adb7-46acc116d843",
   "metadata": {},
   "source": [
    "#### Q26: How would you find out if a matrix is positive definite/negative definite using eigen values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971b73d-c019-409f-91bb-202f2a8b8cca",
   "metadata": {},
   "source": [
    "A26: \n",
    "If all the eigen values are positive, then the matrix is positive definite.\n",
    "If all the eigen values are negative, then the matrix is negative definite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42db0f-430b-45af-88c2-deeb1b3b3f4a",
   "metadata": {},
   "source": [
    "#### Q27: Relationship between Hessian matrix and minimum; maximum and saddle points.\n",
    "\n",
    "Suppose you found an extreme point $\\bfx^*$ of a function $f(\\bfx)$, where the gradient is zero \n",
    "$$\\nabla_\\bfx f(\\bfx)\\vert_{\\bfx^*} = \\mathbf{0}$$\n",
    "\n",
    "You are given the Hessian matrix $\\calH f(\\bfx) \\vert_{\\bfx^*}$ at the extreme point. How would you find out if the extrement point $\\bfx^*$ is a minimum, maximum or a saddle point?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887899a7-5f94-4c85-9d01-d7b1e7c65946",
   "metadata": {},
   "source": [
    "A27: \n",
    "\n",
    "1. If all the eigen values of the Hessian matrix are positive, then $\\bfx^*$ is a miniumum.\n",
    "2. If all the eigen values of the Hessian matrix are negative, then $\\bfx^*$ is a maximum.\n",
    "3. If some of the the eigen values of the Hessian matrix are positive and others are negative, then $\\bfx^*$ is a saddle point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43adbf8d-1ee5-4cce-8d8d-36656ac2ea57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
