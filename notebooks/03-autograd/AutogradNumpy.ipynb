{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d488c07",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129dd658-67dc-4eaa-b8ce-e2ad6bf2b01f",
   "metadata": {},
   "source": [
    "# Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b5c21-6107-4e36-9281-4b5e01f7efad",
   "metadata": {},
   "source": [
    "#### Refs: \n",
    " 1. https://github.com/karpathy/micrograd/tree/master/micrograd\n",
    " 2. https://github.com/mattjj/autodidact\n",
    " 3. https://github.com/mattjj/autodidact/blob/master/autograd/numpy/numpy_vjps.py\n",
    " 4. https://auto-ed.readthedocs.io/en/latest/mod2.html#ii-more-theory\n",
    " 5. https://github.com/lindseysbrown/Auto-eD/blob/master/docs/mod2.rst?plain=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6febdc-5f93-49e3-b607-452b202ab10c",
   "metadata": {},
   "source": [
    "#### Latex macros\n",
    "$$\n",
    "% Calligraphic fonts\n",
    "\\newcommand{\\calA}{{\\cal A}}\n",
    "\\newcommand{\\calB}{{\\cal B}}\n",
    "\\newcommand{\\calC}{{\\cal C}}\n",
    "\\newcommand{\\calD}{{\\cal D}}\n",
    "\\newcommand{\\calE}{{\\cal E}}\n",
    "\\newcommand{\\calF}{{\\cal F}}\n",
    "\\newcommand{\\calG}{{\\cal G}}\n",
    "\\newcommand{\\calH}{{\\cal H}}\n",
    "\\newcommand{\\calI}{{\\cal I}}\n",
    "\\newcommand{\\calJ}{{\\cal J}}\n",
    "\\newcommand{\\calK}{{\\cal K}}\n",
    "\\newcommand{\\calL}{{\\cal L}}\n",
    "\\newcommand{\\calM}{{\\cal M}}\n",
    "\\newcommand{\\calN}{{\\cal N}}\n",
    "\\newcommand{\\calO}{{\\cal O}}\n",
    "\\newcommand{\\calP}{{\\cal P}}\n",
    "\\newcommand{\\calQ}{{\\cal Q}}\n",
    "\\newcommand{\\calR}{{\\cal R}}\n",
    "\\newcommand{\\calS}{{\\cal S}}\n",
    "\\newcommand{\\calT}{{\\cal T}}\n",
    "\\newcommand{\\calU}{{\\cal U}}\n",
    "\\newcommand{\\calV}{{\\cal V}}\n",
    "\\newcommand{\\calW}{{\\cal W}}\n",
    "\\newcommand{\\calX}{{\\cal X}}\n",
    "\\newcommand{\\calY}{{\\cal Y}}\n",
    "\\newcommand{\\calZ}{{\\cal Z}}\n",
    "% Sets:\n",
    "\\newcommand{\\setA}{\\textsf{A}}\n",
    "\\newcommand{\\setB}{\\textsf{B}}\n",
    "\\newcommand{\\setC}{\\textsf{C}}\n",
    "\\newcommand{\\setD}{\\textsf{D}}\n",
    "\\newcommand{\\setE}{\\textsf{E}}\n",
    "\\newcommand{\\setF}{\\textsf{F}}\n",
    "\\newcommand{\\setG}{\\textsf{G}}\n",
    "\\newcommand{\\setH}{\\textsf{H}}\n",
    "\\newcommand{\\setI}{\\textsf{I}}\n",
    "\\newcommand{\\setJ}{\\textsf{J}}\n",
    "\\newcommand{\\setK}{\\textsf{K}}\n",
    "\\newcommand{\\setL}{\\textsf{L}}\n",
    "\\newcommand{\\setM}{\\textsf{M}}\n",
    "\\newcommand{\\setN}{\\textsf{N}}\n",
    "\\newcommand{\\setO}{\\textsf{O}}\n",
    "\\newcommand{\\setP}{\\textsf{P}}\n",
    "\\newcommand{\\setQ}{\\textsf{Q}}\n",
    "\\newcommand{\\setR}{\\textsf{R}}\n",
    "\\newcommand{\\setS}{\\textsf{S}}\n",
    "\\newcommand{\\setT}{\\textsf{T}}\n",
    "\\newcommand{\\setU}{\\textsf{U}}\n",
    "\\newcommand{\\setV}{\\textsf{V}}\n",
    "\\newcommand{\\setW}{\\textsf{W}}\n",
    "\\newcommand{\\setX}{\\textsf{X}}\n",
    "\\newcommand{\\setY}{\\textsf{Y}}\n",
    "\\newcommand{\\setZ}{\\textsf{Z}}\n",
    "% Vectors\n",
    "\\newcommand{\\bfa}{\\mathbf{a}}\n",
    "\\newcommand{\\bfb}{\\mathbf{b}}\n",
    "\\newcommand{\\bfc}{\\mathbf{c}}\n",
    "\\newcommand{\\bfd}{\\mathbf{d}}\n",
    "\\newcommand{\\bfe}{\\mathbf{e}}\n",
    "\\newcommand{\\bff}{\\mathbf{f}}\n",
    "\\newcommand{\\bfg}{\\mathbf{g}}\n",
    "\\newcommand{\\bfh}{\\mathbf{h}}\n",
    "\\newcommand{\\bfi}{\\mathbf{i}}\n",
    "\\newcommand{\\bfj}{\\mathbf{j}}\n",
    "\\newcommand{\\bfk}{\\mathbf{k}}\n",
    "\\newcommand{\\bfl}{\\mathbf{l}}\n",
    "\\newcommand{\\bfm}{\\mathbf{m}}\n",
    "\\newcommand{\\bfn}{\\mathbf{n}}\n",
    "\\newcommand{\\bfo}{\\mathbf{o}}\n",
    "\\newcommand{\\bfp}{\\mathbf{p}}\n",
    "\\newcommand{\\bfq}{\\mathbf{q}}\n",
    "\\newcommand{\\bfr}{\\mathbf{r}}\n",
    "\\newcommand{\\bfs}{\\mathbf{s}}\n",
    "\\newcommand{\\bft}{\\mathbf{t}}\n",
    "\\newcommand{\\bfu}{\\mathbf{u}}\n",
    "\\newcommand{\\bfv}{\\mathbf{v}}\n",
    "\\newcommand{\\bfw}{\\mathbf{w}}\n",
    "\\newcommand{\\bfx}{\\mathbf{x}}\n",
    "\\newcommand{\\bfy}{\\mathbf{y}}\n",
    "\\newcommand{\\bfz}{\\mathbf{z}}\n",
    "\\newcommand{\\bfalpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\bfgamma}{\\boldsymbol{\\gamma}}\n",
    "\\newcommand{\\bfdelta}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\bfepsilon}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\bfzeta}{\\boldsymbol{\\zeta}}\n",
    "\\newcommand{\\bfeta}{\\boldsymbol{\\eta}}\n",
    "\\newcommand{\\bftheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\bfiota}{\\boldsymbol{\\iota}}\n",
    "\\newcommand{\\bfkappa}{\\boldsymbol{\\kappa}}\n",
    "\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\bfmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\bfnu}{\\boldsymbol{\\nu}}\n",
    "\\newcommand{\\bfomicron}{\\boldsymbol{\\omicron}}\n",
    "\\newcommand{\\bfpi}{\\boldsymbol{\\pi}}\n",
    "\\newcommand{\\bfrho}{\\boldsymbol{\\rho}}\n",
    "\\newcommand{\\bfsigma}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\bftau}{\\boldsymbol{\\tau}}\n",
    "\\newcommand{\\bfupsilon}{\\boldsymbol{\\upsilon}}\n",
    "\\newcommand{\\bfphi}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\bfpsi}{\\boldsymbol{\\psi}}\n",
    "\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n",
    "\\newcommand{\\bfxi}{\\boldsymbol{\\xi}}\n",
    "\\newcommand{\\bfell}{\\boldsymbol{\\ell}}\n",
    "% Matrices\n",
    "\\newcommand{\\bfA}{\\mathbf{A}}\n",
    "\\newcommand{\\bfB}{\\mathbf{B}}\n",
    "\\newcommand{\\bfC}{\\mathbf{C}}\n",
    "\\newcommand{\\bfD}{\\mathbf{D}}\n",
    "\\newcommand{\\bfE}{\\mathbf{E}}\n",
    "\\newcommand{\\bfF}{\\mathbf{F}}\n",
    "\\newcommand{\\bfG}{\\mathbf{G}}\n",
    "\\newcommand{\\bfH}{\\mathbf{H}}\n",
    "\\newcommand{\\bfI}{\\mathbf{I}}\n",
    "\\newcommand{\\bfJ}{\\mathbf{J}}\n",
    "\\newcommand{\\bfK}{\\mathbf{K}}\n",
    "\\newcommand{\\bfL}{\\mathbf{L}}\n",
    "\\newcommand{\\bfM}{\\mathbf{M}}\n",
    "\\newcommand{\\bfN}{\\mathbf{N}}\n",
    "\\newcommand{\\bfO}{\\mathbf{O}}\n",
    "\\newcommand{\\bfP}{\\mathbf{P}}\n",
    "\\newcommand{\\bfQ}{\\mathbf{Q}}\n",
    "\\newcommand{\\bfR}{\\mathbf{R}}\n",
    "\\newcommand{\\bfS}{\\mathbf{S}}\n",
    "\\newcommand{\\bfT}{\\mathbf{T}}\n",
    "\\newcommand{\\bfU}{\\mathbf{U}}\n",
    "\\newcommand{\\bfV}{\\mathbf{V}}\n",
    "\\newcommand{\\bfW}{\\mathbf{W}}\n",
    "\\newcommand{\\bfX}{\\mathbf{X}}\n",
    "\\newcommand{\\bfY}{\\mathbf{Y}}\n",
    "\\newcommand{\\bfZ}{\\mathbf{Z}}\n",
    "\\newcommand{\\bfGamma}{\\boldsymbol{\\Gamma}}\n",
    "\\newcommand{\\bfDelta}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\bfTheta}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\bfPi}{\\boldsymbol{\\Pi}}\n",
    "\\newcommand{\\bfSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\bfUpsilon}{\\boldsymbol{\\Upsilon}}\n",
    "\\newcommand{\\bfPhi}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\bfPsi}{\\boldsymbol{\\Psi}}\n",
    "\\newcommand{\\bfOmega}{\\boldsymbol{\\Omega}}\n",
    "% Blackboard Bold:\n",
    "\\newcommand{\\bbA}{\\mathbb{A}}\n",
    "\\newcommand{\\bbB}{\\mathbb{B}}\n",
    "\\newcommand{\\bbC}{\\mathbb{C}}\n",
    "\\newcommand{\\bbD}{\\mathbb{D}}\n",
    "\\newcommand{\\bbE}{\\mathbb{E}}\n",
    "\\newcommand{\\bbF}{\\mathbb{F}}\n",
    "\\newcommand{\\bbG}{\\mathbb{G}}\n",
    "\\newcommand{\\bbH}{\\mathbb{H}}\n",
    "\\newcommand{\\bbI}{\\mathbb{I}}\n",
    "\\newcommand{\\bbJ}{\\mathbb{J}}\n",
    "\\newcommand{\\bbK}{\\mathbb{K}}\n",
    "\\newcommand{\\bbL}{\\mathbb{L}}\n",
    "\\newcommand{\\bbM}{\\mathbb{M}}\n",
    "\\newcommand{\\bbN}{\\mathbb{N}}\n",
    "\\newcommand{\\bbO}{\\mathbb{O}}\n",
    "\\newcommand{\\bbP}{\\mathbb{P}}\n",
    "\\newcommand{\\bbQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\bbR}{\\mathbb{R}}\n",
    "\\newcommand{\\bbS}{\\mathbb{S}}\n",
    "\\newcommand{\\bbT}{\\mathbb{T}}\n",
    "\\newcommand{\\bbU}{\\mathbb{U}}\n",
    "\\newcommand{\\bbV}{\\mathbb{V}}\n",
    "\\newcommand{\\bbW}{\\mathbb{W}}\n",
    "\\newcommand{\\bbX}{\\mathbb{X}}\n",
    "\\newcommand{\\bbY}{\\mathbb{Y}}\n",
    "\\newcommand{\\bbZ}{\\mathbb{Z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0543d2b-206c-4d98-9b50-cc687551381f",
   "metadata": {},
   "source": [
    "### Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca81ed-2bee-470e-8f87-57d9e3128287",
   "metadata": {},
   "source": [
    "#### Scalar single-variable chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ef069-5a7d-41e6-be0d-7578a94a8d5a",
   "metadata": {},
   "source": [
    "$\\newcommand{\\p}{\\partial}$\n",
    "Recall the limit definition of derivative of a function,\n",
    "$$g'(x) = \\lim_{h \\to 0} \\frac{g(x+h) - g(x)}{h}.$$\n",
    "From the limit definition you can find the value of $g(x+h)$ as\n",
    "$$\\lim_{h \\to 0} g(x+h) = \\lim_{h \\to 0} g(x) + g'(x)h.$$\n",
    "\n",
    "You can use this rule to find the chain rule of finding the chaining of two functions together,\n",
    "\\begin{align}\n",
    "\\frac{\\p f(g(x))}{\\p x} &= \\lim_{h \\to 0} \\frac{f(g(x + h)) - f(g(x))}{h}\\\\\n",
    "&=\\lim_{h \\to 0} \\frac{f(g(x) + g'(x) h) - f(g(x))}{h}\\\\\n",
    "&=\\lim_{h \\to 0} \\frac{f(g(x)) + f'(g(x))g'(x)h - f(g(x))}{h}\\\\\n",
    "&=f'(g(x))g'(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3949879-0bda-4b5c-be51-5ea21320a132",
   "metadata": {},
   "source": [
    "#### Scalar two-variable chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d22420-174d-40b1-9bbe-32dd05c5e9b5",
   "metadata": {},
   "source": [
    "Consider a function of two variables $f(u(x),v(x))$. Find its derivative,\n",
    "\\begin{align}\n",
    "\\frac{\\p f(u(x), v(x))}{\\p x} &= \\lim_{h \\to 0} \\frac{f(u(x + h), v(x+h)) - f(u(x), v(x))}{h}\\\\\n",
    "&=\\lim_{h \\to 0} \\frac{f(u(x) + u'(x) h, v(x) + v'(x)h) - f(u(x), v(x))}{h}\n",
    "\\end{align}\n",
    "Now $f(u + \\delta u, v + \\delta v)$ should not be expanded in one step but in two steps. First keep $v + \\delta v$ as it is, and expand with respect to $u + \\delta u$\n",
    "$$\\lim_{\\delta v, \\delta u \\to 0} f(u+\\delta u, v+\\delta v) = \\lim_{\\delta v, \\delta u \\to 0} f(u, v+\\delta v) + f'_u(u, v+\\delta v) \\delta u,$$\n",
    "and then do the same with $v+\\delta v$,\n",
    "$$\\lim_{\\delta v, \\delta u \\to 0} f(u+\\delta u, v+\\delta v) = \\lim_{\\delta v, \\delta u \\to 0} f(u, v) + f'_v(u, v) \\delta v + f'_u(u, v+\\delta v) \\delta u,$$\n",
    "\n",
    "We use $\\lim_{\\delta v,\\delta u \\to 0} f'_u(u, v+\\delta v)\\delta u = \\lim_{\\delta v,\\delta u \\to 0}  f'_u(u, v)\\delta u + f''_{uv}(u, v)(\\delta v)(\\delta u) = \\lim_{\\delta u \\to 0}  f'_u(u, v)\\delta u$ to get,\n",
    "$$\\lim_{\\delta v, \\delta u \\to 0} f(u+\\delta u, v+\\delta v) = \\lim_{\\delta v, \\delta u \\to 0} f(u, v) + f'_v(u, v) \\delta v + f'_u(u, v) \\delta u.$$\n",
    "\n",
    "Going back to the chain rule,\n",
    "\\begin{align}\n",
    "\\frac{\\p f(u(x), v(x))}{\\p x} \n",
    "&= \n",
    "\\lim_{h \\to 0} \\frac{f(u(x) + u'(x) h, v(x) + v'(x)h) - f(u(x), v(x))}{h} \\\\\n",
    "&=\n",
    "\\lim_{h \\to 0} \\frac{f(u(x), v(x)) + f'_v(u(x), v(x)) v'(x)h + f'_u(u(x), v(x)) u'(x) h - f(u(x), v(x))}{h} \\\\\n",
    "&=\n",
    "\\lim_{h \\to 0} \\frac{f'_v(u(x), v(x)) v'(x)h + f'_u(u(x), v(x)) u'(x) h }{h} \\\\\n",
    "&= f'_v(u(x), v(x)) v'(x) + f'_u(u(x), v(x)) u'(x) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d138039a-6ee3-48ab-8cf7-63d014fb9432",
   "metadata": {},
   "source": [
    "#### Scalar valued vector function chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf76f98-2234-464b-b8d9-f42d2f989917",
   "metadata": {},
   "source": [
    "Consider two functions $f(\\bfg): \\bbR^m \\to \\bbR$, $\\bfg(x): \\bbR \\to \\bbR^m$ that can be composed together $f(\\bfg(x))$. We want to find the derivative of composition $f \\circ g$ by chain rule. \n",
    "\n",
    "Recall that the derivative (Jacobian) of $f(\\bfg)$ is a row vector,\n",
    "$$\\frac{\\p f(\\bfg)}{\\p \\bfg} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p f}{\\p g_1} & \\frac{\\p f}{\\p g_2} & \\dots & \\frac{\\p f}{\\p g_m}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "And the derivative (Jacobian) of $\\bfg(x)$ is a column vector,\n",
    "$$\\frac{\\p \\bfg(x)}{\\p x} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p g_1}{\\p x} \\\\\n",
    "\\frac{\\p g_2}{\\p x}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p g_m}{\\p x}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Note that a vector function is a multi-variate scalar function \n",
    "$$f(\\bfg(x)) = f(g_1(x), g_2(x), \\dots, g_m(x)).$$\n",
    "\n",
    "We can apply the multi-variate scalar function chain rule,\n",
    "\\begin{align} \\frac{\\p}{\\p x} f(\\bfg(x)) = f'_{g_1}(g_1(x), \\dots, g_m(x)) g'_1(x) + \\dots + f'_{g_m}(g_1(x), \\dots, g_m(x)) g'_m(x)\\\\\n",
    "= f'_{g_1}(\\bfg(x)) g'_1(x) + \\dots + f'_{g_m}(\\bfg(x)) g'_m(x)\n",
    ".\\end{align}\n",
    "\n",
    "The derivatives of $\\bfg$ can be separated from derivatives of $f$ as vector multiplication,\n",
    "$$ \\frac{\\p}{\\p x} f(\\bfg(x)) = \n",
    "\\begin{bmatrix}\n",
    "f'_{g_1}(\\bfg(x)) & \\dots & f'_{g_m}(\\bfg(x))\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "g'_1(x) \\\\ \\vdots \\\\  g'_m(x)\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "Hence the chain rule for vector derivatives works out for our definition of vector derivatives,\n",
    "$$\\frac{\\p }{\\p \\bfx}  f(\\bfg(x))= \\frac{\\p f(\\bfg(x))}{\\p \\bfg}\\frac{\\p \\bfg(x)}{\\p x}.$$\n",
    "\n",
    "Note that the order of multiplication matters, specifically\n",
    "$$\\frac{\\p }{\\p \\bfx}  f(\\bfg(x))\\ne \\frac{\\p \\bfg(x)}{\\p x} \\frac{\\p f(\\bfg(x))}{\\p \\bfg}.$$\n",
    "\n",
    "This is a consequence of row-vector convention. If we chose a column-vector convention the result will be completely different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b524214-26cb-42f1-b865-75e8dd2c1386",
   "metadata": {},
   "source": [
    "#### General chain rule\n",
    "\n",
    "\n",
    "Let the function be $\\bff(\\bfg): \\bbR^m \\to \\bbR^n$ and $\\bfg(\\bfx): \\bbR^p \\to \\bbR^m$, then the derivative (Jacobian) of their composition $\\bff \\circ \\bfg$ is \n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff(\\bfg(\\bfx)) = \\frac{\\p \\bff(\\bfg(\\bfx))}{\\p \\bfg}\\frac{\\p \\bfg(\\bfx)}{\\p \\bfx} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659abf54-f871-4a62-adf0-a81528d27594",
   "metadata": {},
   "source": [
    "#### Computational complexity of Forward vs Reverse mode differentiation\n",
    "\n",
    "Consider three functions, $\\bfh(\\bfx): \\bbR^m \\to \\bbR^n$, $\\bfg(\\bfh): \\bbR^n \\to \\bbR^p$ and $\\bff(\\bfg): \\bbR^p \\to \\bbR^q$ chained together for composition\n",
    "$\\bff(\\bfg(\\bfh(\\bfx))) : \\bbR^m \\to \\bbR^q$. To find the derivative (Jacobian) the composite function, we use chain rule:\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff(\\bfg(\\bfh(\\bfx))) = \n",
    "\\frac{\\p \\bff}{\\p \\bfg} \\frac{\\p \\bfg}{\\p \\bfh} \\frac{\\p \\bfh}{\\p \\bfx} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bfa62e-2625-42ad-bf3e-6024f2aa054a",
   "metadata": {},
   "source": [
    "##### Computational complexity of matrix multiplication\n",
    "\n",
    "Let's say you multiply two matrices $A \\in \\bbR^{m \\times n}$ and $B \\in \\bbR^{n \\times p}$, total number of additions and multiplications (floating point operations) can be calculated by\n",
    "$$\n",
    "C = A B = \\begin{bmatrix}\n",
    "\\bfa_1^\\top\\\\\n",
    "\\bfa_2^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\bfa_m^\\top\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\bfb_1 & \n",
    "\\bfb_2 &\n",
    "\\dots &\n",
    "\\bfb_p\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where $\\bfa_i^\\top$ are the row-vectors of matrix $A$ and $\\bfb_i$ are the column vectors of matrix $B$.\n",
    "Then matrix $C$ is written as\n",
    "$$ C = \\begin{bmatrix}\n",
    "\\bfa_1^\\top \\bfb_1 & \\bfa_1^\\top \\bfb_2 & \\dots & \\bfa_1^\\top \\bfb_p \\\\\n",
    "\\bfa_2^\\top \\bfb_1 & \\bfa_2^\\top \\bfb_2 & \\dots & \\bfa_2^\\top \\bfb_p \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\bfa_m^\\top \\bfb_1 & \\bfa_m^\\top \\bfb_2 & \\dots & \\bfa_m^\\top \\bfb_p \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "We note that $C$ matrix has $pm$ elments and each element requires computing dot product of size $n$ vectors,\n",
    "$$\\bfa_i^\\top \\bfb_j = a_{i1} b_{j1} + a_{i2} b_{j2} + \\dots + a_{in} b_{in}.$$\n",
    "\n",
    "Each dot product requires $n$ multiplications and $n-1$ additions. Hence matrix multiplication which has $pm$ dot products requires $pm(n + n -1)$ (floating point) operations.\n",
    "\n",
    "Matrix multiplication has a computation complexity of $O(pmn)$ for matrices of size $m \\times n$ and $n  \\times p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a62cf9-323e-4999-9c61-b8db8c6a22ac",
   "metadata": {},
   "source": [
    "##### Computational complexity of forward-mode differentiation\n",
    "\n",
    "In forward diff, we compute computaional complexity from input side to the output side.\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff(\\bfg(\\bfh(\\bfx))) = \n",
    "\\left(\\frac{\\p \\bff}{\\p \\bfg} \\left( \\frac{\\p \\bfg}{\\p \\bfh} \\frac{\\p \\bfh}{\\p \\bfx} \\right)\\right)$$\n",
    "\n",
    "The first two matrix multiplications  $X_{p \\times n} = \\left( \\frac{\\p \\bfg}{\\p \\bfh} \\frac{\\p \\bfh}{\\p \\bfx} \\right)$ are of the size $p \\times m$ and $m \\times n$, resulting in $O(pmn)$ complexity.\n",
    "\n",
    "The second two matrix multiplications $\\left(\\frac{\\p \\bff}{\\p \\bfg} X_{p \\times n} \\right)$ are of the size $q \\times p$ and $p \\times n$, resulting in $O(qpn)$ complexity.\n",
    "\n",
    "The total computational complexity of forward differentiation is $O(qpn + pmn) = O((qp+pm)n)$.\n",
    "\n",
    "For a longer chain of functions of Jacobians of shape $q_i \\times p_i$ with ($p_i = q_{i-1}$).\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff_n(\\dots \\bff_2(\\bff_1(\\bfx))) = \n",
    "\\frac{\\p \\bff_n}{\\p \\bff_{n-1}}_{q_n \\times p_{n}}  \n",
    "\\dots\n",
    "\\frac{\\p \\bff_2}{\\p \\bff_1}_{q_1 \\times p_1}\n",
    "\\frac{\\p \\bff_1}{\\p \\bfx}_{q_0 \\times p_0} $$\n",
    "\n",
    "We get a computational complexity that looks like  $O( (\\sum_{i=1}^n q_i p_i) p_0)$. Note that the size of input $p_0$ is the only common factor for the entire chain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12e4137-909f-4261-9585-e8d987a31a81",
   "metadata": {},
   "source": [
    "##### Computational complexity of reverse-mode diff\n",
    "\n",
    "In reverse-mode diff, we compute computaional complexity from input side to the output side.\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff(\\bfg(\\bfh(\\bfx))) = \n",
    "\\left(\\left(\\frac{\\p \\bff}{\\p \\bfg}  \\frac{\\p \\bfg}{\\p \\bfh}\\right) \\frac{\\p \\bfh}{\\p \\bfx} \\right)$$\n",
    "\n",
    "The first two matrix multiplications  $X_{q \\times p} = \\left( \\frac{\\p \\bff}{\\p \\bfg}  \\frac{\\p \\bfg}{\\p \\bfh} \\right)$ are of the size $q \\times p$ and $p \\times m$, resulting in $O(qpm)$ complexity.\n",
    "\n",
    "The second two matrix multiplications $\\left(X_{q \\times p}\\frac{\\p \\bfh}{\\p \\bfx}  \\right)$ are of the size $q \\times p$ and $p \\times n$, resulting in $O(qpn)$ complexity.\n",
    "\n",
    "The total computational complexity of forward differentiation is $O(qpm + qmn) = O(q(pm+mn))$. \n",
    "\n",
    "For a longer chain of functions of Jacobians of shape $q_i \\times p_i$ with ($p_i = q_{i-1}$).\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff_n(\\dots \\bff_2(\\bff_1(\\bfx))) = \n",
    "\\frac{\\p \\bff_n}{\\p \\bff_{n-1}}_{q_n \\times p_{n}}  \n",
    "\\dots\n",
    "\\frac{\\p \\bff_2}{\\p \\bff_1}_{q_1 \\times p_1}\n",
    "\\frac{\\p \\bff_1}{\\p \\bfx}_{q_0 \\times p_0} $$\n",
    "\n",
    "We get a computational complexity that looks like  $O( q_n (\\sum_{i=0}^{n-1} q_i p_i))$. Note that the size of output $q_n$ is the only common factor for the entire chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046cd3f-4a12-474b-8576-13830e28c3ac",
   "metadata": {},
   "source": [
    "##### Reverse-mode differentiation is called backpropagation\n",
    "\n",
    "Reverse-mode differentiation is called backpropagation in neural networks. It is more popular because most of the times you compute the derivatives of the loss function which is a scalar function with output dimension as only 1. This makes reverse-mode differentiation clearly superior for loss function gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d865b17-ab2c-4d8d-97f3-c1996b31c5e8",
   "metadata": {},
   "source": [
    "### Implementation of forward/reverse mode differentiation in Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f08369-9bb9-4ae5-9d74-adb71fb3c423",
   "metadata": {},
   "source": [
    "#### Reverse mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e251d1-0442-4cf7-a3d6-7cd2880b683f",
   "metadata": {},
   "source": [
    "Let's compute the derivatives of\n",
    "$$f(x_1, x_2) = x_1 x_2 + sin(x_1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f0347-1ad9-4695-b551-54cae2ef086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "\n",
    "x1 = t.Tensor([2]) # Initialize a tensor\n",
    "x1.requires_grad_(True) # enable gradient tracking\n",
    "x2 = t.Tensor([7])\n",
    "x2.requires_grad_(True)\n",
    "f = x1 * x2 + x1.sin() # Create computation graph \n",
    "print(\"Before backward:\", x1.grad, x2.grad) # print df/dx1 and df/dx2\n",
    "\n",
    "f.backward(t.Tensor([1])) # Intialize backward computation with dg/df = 1\n",
    "\n",
    "print(\"After backward:\", x1.grad, x2.grad) # print df/dx1 and df/dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36b2b1-de2b-414e-9709-8908fb24c76b",
   "metadata": {},
   "source": [
    "#### Forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21804f3-83d6-4c8d-9bee-11ef836cf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.autograd.forward_ad as fwAD\n",
    "x1 = t.Tensor([2]) # Initialize a tensor\n",
    "x2 = t.Tensor([7]) # Initialize a tensor\n",
    "with fwAD.dual_level():\n",
    "    x1_pd = fwAD.make_dual(x1, t.Tensor([1])) # Intialize dx1/dz = 1\n",
    "    x2_pd = fwAD.make_dual(x2, t.Tensor([0])) # Intialize dx2/dz = 0\n",
    "    \n",
    "    f = x1_pd * x2_pd + x1_pd.sin() # compute the function    \n",
    "    dfdx1 = fwAD.unpack_dual(f).tangent\n",
    "    print(dfdx1)\n",
    "\n",
    "\n",
    "    x1_pd = fwAD.make_dual(x1, t.Tensor([0])) # Intialize dx1/dz = 0\n",
    "    x2_pd = fwAD.make_dual(x2, t.Tensor([1])) # Intialize dx2/dz = 1\n",
    "    \n",
    "    f = x1_pd * x2_pd + x1_pd.sin() # compute the function    \n",
    "    dfdx2 = fwAD.unpack_dual(f).tangent\n",
    "    print(dfdx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299b642-5ae3-4a88-a666-983478cf0c8b",
   "metadata": {},
   "source": [
    "### Vector Jacobian product (vjp) for reverse-mode differentiation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6e000-7099-494d-8d21-09b5543f4bff",
   "metadata": {},
   "source": [
    "Typical output of a neural network is a loss function. Loss function is always a scalar. Most neural network libraries implement reverse-mode differentiation only for a scalar output.\n",
    "\n",
    "Hence, the first Jacobian on the output side of chain rule is a row-vector.\n",
    "$$ \\frac{\\p }{\\p \\bfx} l(\\bff(\\bfg(\\bfx)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p \\bff}{\\p \\bfg}\\frac{\\p \\bfg}{\\p \\bfx},$$\n",
    "$\\bfg(\\bfx): \\bbR^n \\to \\bbR^p$, $\\bff(\\bfg): \\bbR^p \\to \\bbR^q$ and $l(\\bff): \\bbR^q \\to \\bbR$.\n",
    "\n",
    "When you are writing a programmatic derivative function for reverse mode differentation, the function does two things:\n",
    "\n",
    "1. Compute the local Jacobian of the function for example $\\frac{\\p \\bff}{\\p \\bfg}$.\n",
    "2. Left multiply the Jacobian with a row-vector of accumulated derivative so far. For example, $\\frac{\\p l}{\\p \\bff} \\frac{\\p \\bff}{\\p \\bfg}$.\n",
    "\n",
    "The template of the function is like this:\n",
    "```python\n",
    "def g(arg1, arg2):\n",
    "    # Compute g\n",
    "    return g \n",
    "    \n",
    "def g_vjp(arg1, arg2, dl_dg):\n",
    "    # Compute vector Jacobian product with respect to each oargument\n",
    "    return dl_arg1, dl_arg2\n",
    "```\n",
    "\n",
    "If you are given a function $\\bfg(\\bfx)$, and you want to implement `vjp` function for it. It is often easier to imagine a sclar loss function $l(\\bfg(\\bfx))$ whose accumulated gradient $\\frac{\\p l}{\\p \\bfg}$ is given as an input argument. The function `vjp` returns the derivative of the loss function with respect to the inputs,\n",
    "$$\\frac{\\p}{\\p \\bfx} l(\\bfg(\\bfx)) = \\frac{\\p l}{\\p \\bfg} \\frac{\\p \\bfg}{\\p \\bfx},$$\n",
    "which looks like a vector Jacobian product, but you are free to not compute the Jacobian separately. Sometimes it is computationally harder to compute the jacobian separately then multiply it by the vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168eb04-e2df-4832-a342-78af4c0b69b8",
   "metadata": {},
   "source": [
    "### Jacobian vector product (jvp) for forward-mode differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aab0163-6c72-4e47-98cb-a0537294b597",
   "metadata": {},
   "source": [
    "It is also common to implment foward mode differentiation with only a scalar input assumption, say $t$.\n",
    "\n",
    "Say $\\bfh(\\bfx): \\bbR^m \\to \\bbR^n$, $\\bfg(\\bfh): \\bbR^n \\to \\bbR^p$ and $\\bff(\\bfg): \\bbR^p \\to \\bbR^q$ \n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff(\\bfg(\\bfh(\\bfx))) = \n",
    "\\frac{\\p \\bff}{\\p \\bfg} \\frac{\\p \\bfg}{\\p \\bfh} \\frac{\\p \\bfh}{\\p \\bfx} $$\n",
    "\n",
    "You can assume $\\bfx$ to be function of scalar $t \\in \\bbR$, $\\bfx(t)$. Then the chain rule is \n",
    "\n",
    "$$ \\frac{\\p }{\\p t} \\bff(\\bfg(\\bfh(\\bfx(t)))) = \n",
    "\\frac{\\p \\bff}{\\p \\bfg} \\frac{\\p \\bfg}{\\p \\bfh} \\frac{\\p \\bfh}{\\p \\bfx} \\frac{\\p \\bfx}{\\p t} $$\n",
    "\n",
    "You can compute the derivative with respect to one element of $\\bfx$ at a time by setting that element's derivative to be 1 and the rest to be zero. For example, if you want to compute the $\\frac{\\p \\bff}{\\p x_2}$ then set \n",
    "$$\\frac{\\p \\bfx}{\\p t} = \\begin{bmatrix}\n",
    "0\\\\ 1\\\\ 0\\\\ \\vdots\\\\ 0\n",
    "\\end{bmatrix}$$.\n",
    "\n",
    "For forward pass you typically implement a function called `jvp` which stands for Jacobian vector product:\n",
    "1. The Jacobian is the local derivative. For example $\\frac{\\p \\bfh}{\\p \\bfx}$\n",
    "2. Multiplication of the jacobian with an incoming accumulated gradient which is a column-vector. For example, $\\frac{\\p \\bfx}{\\p t}$.\n",
    "\n",
    "The template of the function is like this:\n",
    "```python\n",
    "def g(arg1, arg2):\n",
    "    # Compute g\n",
    "    return g \n",
    "    \n",
    "def g_jvp(arg1, arg2, darg1_dt, darg2_dt):\n",
    "    # Compute Jacobian vector product with respect to t\n",
    "    return dg_dt\n",
    "```\n",
    "\n",
    "If you are given a function $\\bfg(\\bfx)$, and you want to implement `jvp` function for it. It is often easier to imagine a sclar input variable $\\bfg(\\bfx(t)))$ whose accumulated gradient $\\frac{\\p \\bfx}{\\p t}$ are given as an input argument. The function `jvp` returns the derivative of the output with respect to the scalar input $t$,\n",
    "$$\\frac{\\p}{\\p t} \\bfg(\\bfx(t)) = \\frac{\\p \\bfg}{\\p \\bfx}\\frac{\\p \\bfx}{\\p t},$$\n",
    "which looks like a  Jacobian vector product, but you are free to not compute the Jacobian separately. Sometimes it is computationally harder to compute the jacobian separately then multiply it by the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa141ae8-7bc1-41e1-84a5-2c1c7bc86f65",
   "metadata": {},
   "source": [
    "### Implementing numpy backpropagation for various operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed467c90-12b0-4333-888a-fd9264620c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refs: \n",
    "# 1. https://github.com/karpathy/micrograd/tree/master/micrograd\n",
    "# 2. https://github.com/mattjj/autodidact\n",
    "# 3. https://github.com/mattjj/autodidact/blob/master/autograd/numpy/numpy_vjps.py\n",
    "from collections import namedtuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193046a-cc86-4e3f-b986-ab3b59b19eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unbroadcast function is explained in the end because expaining it now\n",
    "# does not make narrative sense\n",
    "def unbroadcast(target, g, axis=0):\n",
    "    \"\"\"Remove broadcasted dimensions by summing along them.\n",
    "    When computing gradients of a broadcasted value, this is the right thing to\n",
    "    do when computing the total derivative and accounting for cloning.\n",
    "    \"\"\"\n",
    "    while np.ndim(g) > np.ndim(target):\n",
    "        g = g.sum(axis=axis)\n",
    "    for axis, size in enumerate(target.shape):\n",
    "        if size == 1:\n",
    "            g = g.sum(axis=axis, keepdims=True)\n",
    "    if np.iscomplexobj(g) and not np.iscomplex(target):\n",
    "        g = g.real()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4e2c61-8a28-4ab8-8df8-9b809255c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtuple are just like dictionaries but are faster\n",
    "Op = namedtuple('Op', ['apply',\n",
    "                   'vjp',\n",
    "                   'name',\n",
    "                   'nargs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178a445-25a7-40e5-82c4-9c03e9c3385f",
   "metadata": {},
   "source": [
    "## Vector Jacobian Product for addition\n",
    "$\\newcommand{\\bfa}{\\mathbf{a}}$\n",
    "$\\newcommand{\\bfb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bfA}{\\mathbf{A}}$\n",
    "$\\newcommand{\\bfB}{\\mathbf{B}}$\n",
    "$\\newcommand{\\bfC}{\\mathbf{C}}$\n",
    "$\\newcommand{\\bfF}{\\mathbf{F}}$\n",
    "$\\newcommand{\\bff}{\\mathbf{f}}$\n",
    "$\\newcommand{\\bbR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\bfI}{\\mathbf{I}}$\n",
    "$\\newcommand{\\bfzero}{\\mathbf{0}}$\n",
    "$\\newcommand{\\p}{\\partial}$\n",
    "\n",
    "$$\\bff(\\bfa, \\bfb) = \\bfa + \\bfb$$ where $\\bfa, \\bfb, \\bff \\in \\bbR^n$\n",
    "\n",
    "Let $l(\\bff(\\bfa, \\bfb)) \\in \\bbR$ be the eventual scalar output. We find $\\frac{\\p l}{\\p \\bfa}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product.\n",
    "$$\\frac{\\p }{\\p \\bfa} l(\\bff(\\bfa, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfa}(\\bfa + \\bfb)\n",
    "= \\frac{\\p l}{\\p \\bff} (\\bfI_{n \\times n} + 0_{n \\times n}) = \\frac{\\p l}{\\p \\bff}$$ \n",
    "\n",
    "Similarly, \n",
    "$$\\frac{\\p }{\\p \\bfb} l(\\bff(\\bfa, \\bfb)) = \\frac{\\p l}{\\p \\bff}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a152b-313b-46a5-99b7-13c0b47d457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vjp(dldf, a, b):\n",
    "    dlda = unbroadcast(a, dldf)\n",
    "    dldb = unbroadcast(b, dldf)\n",
    "    return dlda, dldb\n",
    "    \n",
    "add = Op(\n",
    "    apply=np.add,\n",
    "    vjp=add_vjp,\n",
    "    name='+',\n",
    "    nargs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1abf2-d748-4b2e-b7b5-6cb748f73d79",
   "metadata": {},
   "source": [
    "## VJP for element-wise multiplication\n",
    "\n",
    "<!-- $$\\bff(\\alpha, \\bfb) = \\alpha \\bfb$$ where $\\alpha \\in \\bbR,  and \\bfb, \\bff \\in \\bbR^n$\n",
    "\n",
    "Let $l(\\bff(\\alpha, \\bfb)) \\in \\bbR$ be the eventual scalar output. We find $\\frac{\\p l}{\\p \\alpha}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product.\n",
    "$$\\frac{\\p }{\\p \\alpha} l(\\bff(\\alpha, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\alpha}(\\alpha \\bfb)\n",
    "= \\frac{\\p l}{\\p \\bff} \\bfb$$ \n",
    "\n",
    "Similarly, \n",
    "$$\\frac{\\p }{\\p \\bfb} l(\\bff(\\alpha, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\alpha I_{n \\times n}$$ -->\n",
    "\n",
    "$$f(\\alpha, \\beta) = \\alpha \\beta$$ where $\\alpha, \\beta, f \\in \\bbR$\n",
    "\n",
    "Let $l(f(\\alpha, \\beta)) \\in \\bbR$ be the eventual scalar output. We find $\\frac{\\p l}{\\p \\alpha}$ and $\\frac{\\p l}{\\p \\beta}$ for Vector Jacobian product.\n",
    "\n",
    "$$\\frac{\\p }{\\p \\alpha} l(f(\\alpha, \\beta)) = \\frac{\\p l}{\\p f}\\frac{\\p }{\\p \\alpha}(\\alpha \\beta)\n",
    "= \\frac{\\p l}{\\p f} \\beta$$ \n",
    "$$\\frac{\\p }{\\p \\beta} l(f(\\alpha, \\beta)) = \\frac{\\p l}{\\p f}\\frac{\\p }{\\p \\beta}(\\alpha \\beta)\n",
    "= \\frac{\\p l}{\\p f} \\alpha$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867a65a-b191-4349-9d93-cbc32e436ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_vjp(dldf, a, b):\n",
    "    dlda = unbroadcast(a, dldf * b)\n",
    "    dldb = unbroadcast(b, dldf * a)\n",
    "    return dlda, dldb\n",
    "\n",
    "mul = Op(\n",
    "    apply=np.multiply,\n",
    "    vjp=mul_vjp,\n",
    "    name='*',\n",
    "    nargs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00554cf4-2dee-42d2-a900-e69e1621d323",
   "metadata": {},
   "source": [
    "## VJP for matrix-matrix, matrix-vector and vector-vector multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa238d3-6dbf-4805-b1ea-b0f8f3929785",
   "metadata": {},
   "source": [
    "### Case 1: VJP for vector-vector multiplication\n",
    "\n",
    "$$f(\\bfa, \\bfb) = \\bfa^\\top \\bfb$$ where $f \\in \\bbR$,  and $\\bfb, \\bfa \\in \\bbR^n$\n",
    "\n",
    "Let $l(f(\\bfa, \\bfb)) \\in \\bbR$ be the eventual scalar output. We find $\\frac{\\p l}{\\p \\bfa}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product.\n",
    "$$\\frac{\\p }{\\p \\bfa} l(f(\\bfa, \\bfb)) = \\frac{\\p l}{\\p f}\\frac{\\p }{\\p \\bfa}(\\bfa^\\top \\bfb)\n",
    "= \\frac{\\p l}{\\p f} \\bfb^\\top$$ \n",
    "\n",
    "Similarly, \n",
    "$$\\frac{\\p }{\\p \\bfb} l(f(\\bfa, \\bfb)) = \\frac{\\p l}{\\p f}\\bfa^\\top$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96eaea-a043-4da4-a395-0adc4e3eef09",
   "metadata": {},
   "source": [
    "### Case 2: VJP for matrix-vector multiplication\n",
    "\n",
    "Let\n",
    "$$\\bff(\\bfA, \\bfb) = \\bfA \\bfb$$ where $\\bff \\in \\bbR^m$, $\\bfb \\in \\bbR^n$, and $\\bfA \\in \\bbR^{m \\times n}$\n",
    "\n",
    "Let $l(\\bff(\\bfA, \\bfb)) \\in \\bbR$ be the eventual scalar output. We want to findfind $\\frac{\\p l}{\\p \\bfA}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product.\n",
    "\n",
    "\n",
    "Let \n",
    "$$ \\bfA = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\dots & a_{1n}\\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\dots & a_{mn}\n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix}\n",
    "\\bfa_1^\\top\\\\\n",
    "\\bfa_2^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\bfa_m^\\top\n",
    "\\end{bmatrix}$$, where each $\\bfa_i^\\top \\in \\bbR^{1 \\times n}$ and $a_{ij} \\in \\bbR$.\n",
    "\n",
    "Define matrix derivative of  scalar to be:\n",
    "$$ \\frac{\\p l}{\\p \\bfA} = \\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p a_{11}} & \\frac{\\p l}{\\p a_{12}} & \\dots & \\frac{\\p l}{\\p a_{1n}}\\\\\n",
    "\\frac{\\p l}{\\p a_{21}} & \\frac{\\p l}{\\p a_{22}} & \\dots & \\frac{\\p l}{\\p a_{2n}}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\p l}{\\p a_{m1}} & \\frac{\\p l}{\\p a_{m2}} & \\dots & \\frac{\\p l}{\\p a_{mn}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p \\bfa_1}\\\\\n",
    "\\frac{\\p l}{\\p \\bfa_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p l}{\\p \\bfa_m}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\frac{\\p }{\\p \\bfA} l(\\bff(\\bfa, \\bfb)) = \\frac{\\p l}{\\p \\bff} \\frac{\\p }{\\p \\bfA} (\\bfA\\bfb)$$.\n",
    "\n",
    "Note that $$\\bfA\\bfb = \n",
    "\\begin{bmatrix}\n",
    "\\bfa_1^\\top\\\\\n",
    "\\bfa_2^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\bfa_m^\\top\n",
    "\\end{bmatrix}\\bfb \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\bfa_1^\\top\\bfb\\\\\n",
    "\\bfa_2^\\top\\bfb\\\\\n",
    "\\vdots\\\\\n",
    "\\bfa_m^\\top\\bfb\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since $\\bfa_i^\\top \\bfb$ is a scalar, it is easier to find its derivative with respect to the matrix $\\bfA$.\n",
    "\n",
    "$$\\frac{\\p }{\\p \\bfA}\\bfa_i^\\top \\bfb \n",
    "= \\begin{bmatrix}\n",
    "\\frac{\\p \\bfa_i^\\top \\bfb }{\\p \\bfa_1}\\\\\n",
    "\\frac{\\p \\bfa_i^\\top \\bfb }{\\p \\bfa_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p \\bfa_i^\\top \\bfb }{\\p \\bfa_i}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p \\bfa_i^\\top \\bfb }{\\p \\bfa_m}\n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix}\n",
    "\\bfzero^\\top_{n}\\\\\n",
    "\\bfzero^\\top_{n}\\\\\n",
    "\\vdots\\\\\n",
    "\\bfb^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\bfzero^\\top_{n}\n",
    "\\end{bmatrix}\n",
    "\\in \\bbR^{m \\times n}$$\n",
    "\n",
    "Let \n",
    "$$\\frac{\\p l}{\\p \\bff} = \\begin{bmatrix} \\frac{\\p l}{\\p f_1} & \\frac{\\p l}{\\p f_2} & \\dots & \\frac{\\p l}{\\p f_m} \\end{bmatrix}$$\n",
    "\n",
    "Then $$\\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\\bfa_i^\\top \\bfb = \n",
    "\\begin{bmatrix} \\frac{\\p l}{\\p f_1} & \\frac{\\p l}{\\p f_2} & \\dots & \\frac{\\p l}{\\p f_m} \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\bfzero^\\top_{n}\\\\\n",
    "\\bfzero^\\top_{n}\\\\\n",
    "\\vdots\\\\\n",
    "\\bfb^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\bfzero^\\top_{n}\n",
    "\\end{bmatrix}\n",
    "= \\frac{\\p l}{\\p f_i}\\bfb^\\top \\in \\bbR^{1 \\times n}$$\n",
    "\n",
    "Returning to our original quest for \n",
    "$$ \\frac{\\p }{\\p \\bfA} l(\\bff(\\bfA, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\\bfA \\bfb = \n",
    "\\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\n",
    "\\begin{bmatrix}\n",
    "\\bfa_1^\\top\\bfb\\\\\n",
    "\\bfa_2^\\top\\bfb\\\\\n",
    "\\vdots\\\\\n",
    "\\bfa_m^\\top\\bfb\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\\bfa_1^\\top\\bfb\\\\\n",
    "\\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\\bfa_2^\\top\\bfb\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\\bfa_m^\\top\\bfb\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p f_1}\\bfb^\\top\\\\\n",
    "\\frac{\\p l}{\\p f_2}\\bfb^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p l}{\\p f_m}\\bfb^\\top\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that \n",
    "$$  \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p f_1}\\bfb^\\top\\\\\n",
    "\\frac{\\p l}{\\p f_2}\\bfb^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p l}{\\p f_m}\\bfb^\\top\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix} \\frac{\\p l}{\\p f_1} \\\\ \\frac{\\p l}{\\p f_2} \\\\ \\dots \\\\ \\frac{\\p l}{\\p f_m} \\end{bmatrix} \\bfb^\\top \n",
    "= \\left(\\frac{\\p l}{\\p \\bff}\\right)^\\top \\bfb^\\top\n",
    "$$\n",
    "\n",
    "We can group the terms inside a single transpose.\n",
    "\n",
    "Which results in\n",
    "$$ \\frac{\\p }{\\p \\bfA} l(\\bff(\\bfA, \\bfb)) = \\left(\\bfb\\frac{\\p l}{\\p \\bff}\\right)^\\top$$\n",
    "\n",
    "The derivative with respect to $\\bfb$ is simpler:\n",
    "$$ \\frac{\\p }{\\p \\bfb} l(\\bff(\\bfA, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfb}(\\bfA\\bfb) = \\frac{\\p l}{\\p \\bff} \\bfA$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a268a7-b908-4e33-ae93-e1263dacf2bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Case 3: VJP for matrix-matrix multiplication\n",
    "\n",
    "Let\n",
    "$$\\bfF(\\bfA, \\bfB) = \\bfA \\bfB$$ where $\\bfF \\in \\bbR^{m \\times p}$, $\\bfB \\in \\bbR^{n \\times p}$, and $\\bfA \\in \\bbR^{m \\times n}$\n",
    "\n",
    "Let $l(\\bfF(\\bfA, \\bfB)) \\in \\bbR$ be the eventual scalar output. We want to find $\\frac{\\p l}{\\p \\bfA}$ and $\\frac{\\p l}{\\p \\bfB}$ for Vector Jacobian product.\n",
    "\n",
    "\n",
    "Note that a matrix-matrix multiplication can be written in terms horizontal stacking of matrix-vector multiplications. Specifically, write $\\bfF$ and $\\bfB$ in terms of their column vectors:\n",
    "$$\\bfB = \\begin{bmatrix} \\bfb_1 & \\bfb_2 & \\dots & \\bfb_p \\end{bmatrix}$$\n",
    "$$\\bfF = \\begin{bmatrix} \\bff_1 & \\bff_2 & \\dots & \\bff_p \\end{bmatrix}.$$\n",
    "\n",
    "Then for all $i$ \n",
    "$$\\bff_i = \\bfA\\bfb_i$$\n",
    "\n",
    "From the VJP of matrix-vector multiplication, we can write\n",
    "$$ \\frac{\\p l}{\\p \\bff_i}\\frac{\\p }{\\p \\bfA}\\bff_i =\n",
    "\\frac{\\p l}{\\p \\bff_i}\\frac{\\p }{\\p \\bfA}(\\bfA \\bfb_i) = \\left(\\bfb_i\\frac{\\p l}{\\p \\bff_i}\\right)^\\top \\in \\bbR^{ m \\times n }$$\n",
    "and for all $i \\ne j$\n",
    "$$ \\frac{\\p l}{\\p \\bff_j}\\frac{\\p }{\\p \\bfA}(\\bfA \\bfb_i) = \\bfzero_{m \\times n}$$\n",
    "\n",
    "Instead of writing $l(\\bfF)$, we can also write $l(\\bff_1, \\bff_2, \\dots, \\bff_p)$, then by chain rule of functions with multiple arguments, we have,\n",
    "$$\\frac{\\p }{\\p \\bfA}l(\\bfF(\\bfA, \\bfB)) = \\frac{\\p }{\\p \\bfA} l(\\bff_1, \\bff_2, \\dots, \\bff_p)\n",
    "= \\frac{\\p l}{\\p \\bff_1} \\frac{\\p \\bff_1}{\\p \\bfA} + \\frac{\\p l}{\\p \\bff_2} \\frac{\\p \\bff_2}{\\p \\bfA} + \\dots + \\frac{\\p l}{\\p \\bff_p} \\frac{\\p \\bff_p}{\\p \\bfA}$$\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfA}l(\\bfF(\\bfA, \\bfB)) = \n",
    " \\left(\\bfb_1\\frac{\\p l}{\\p \\bff_1}\\right)^\\top \n",
    " + \\left(\\bfb_2\\frac{\\p l}{\\p \\bff_2}\\right)^\\top \n",
    " + \\dots\n",
    "+ \\left(\\bfb_p\\frac{\\p l}{\\p \\bff_p}\\right)^\\top \n",
    "=\n",
    " \\left(\\bfb_1\\frac{\\p l}{\\p \\bff_1}\n",
    " + \\bfb_2\\frac{\\p l}{\\p \\bff_2}\n",
    " + \\dots\n",
    "+ \\bfb_p\\frac{\\p l}{\\p \\bff_p}\\right)^\\top \n",
    "$$\n",
    "\n",
    "It turns out that some of outer products can be compactly written as matrix-matrix multiplication:\n",
    "$$ \\bfb_1\\frac{\\p l}{\\p \\bff_1}\n",
    " + \\bfb_2\\frac{\\p l}{\\p \\bff_2}\n",
    " + \\dots\n",
    "+ \\bfb_p\\frac{\\p l}{\\p \\bff_p} = \n",
    "\\begin{bmatrix}\n",
    "\\bfb_1 & \\bfb_2 & \\dots & \\bfb_p \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p \\bff_1}\\\\\n",
    "\\frac{\\p l}{\\p \\bff_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p l}{\\p \\bff_p}\\end{bmatrix} = \\bfB \\left(\\frac{\\p l}{\\p \\bfF}\\right)^\\top$$\n",
    "\n",
    "Hence,\n",
    "$$ \\frac{\\p }{\\p \\bfA}l(\\bfF(\\bfA, \\bfB)) = \\frac{\\p l}{\\p \\bfF}\\bfB^\\top $$\n",
    "\n",
    "\n",
    "The vector Jacobian product for $\\bfB$ can be found by applying the above rule to $\\bfF_2(\\bfA, \\bfC) = \\bfF^\\top(\\bfA, \\bfB) = \\bfB^\\top \\bfA^\\top = \\bfC \\bfA^\\top$ where $\\bfC = \\bfB^\\top$ and $\\bfF_2 = \\bfF^\\top$.\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfC}l(\\bfF_2(\\bfA, \\bfC)) = \\frac{\\p l}{\\p \\bfF_2}\\bfA $$\n",
    "\n",
    "Take transpose of both sides\n",
    "$$ \\frac{\\p }{\\p \\bfC^\\top}l(\\bfF_2^\\top(\\bfA, \\bfC)) = \\bfA^\\top\\frac{\\p l}{\\p \\bfF_2^\\top} $$\n",
    "\n",
    "Put back, $\\bfC = \\bfB^\\top$ and $\\bfF_2 = \\bfF^\\top$,\n",
    "$$ \\frac{\\p }{\\p \\bfB}l(\\bfF(\\bfA, \\bfB)) = \\bfA^\\top\\frac{\\p l}{\\p \\bfF} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7953b-d59c-4877-a52f-388f1653b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_vjp(dldF, A, B):\n",
    "    G = dldF\n",
    "    if G.ndim == 0:\n",
    "        # Case 1: vector-vector multiplication\n",
    "        assert A.ndim == 1 and B.ndim == 1\n",
    "        dldA = G*B\n",
    "        dldB = G*A\n",
    "        return (unbroadcast(A, dldA),\n",
    "                unbroadcast(B, dldB))\n",
    "    \n",
    "    assert not (A.ndim == 1 and B.ndim == 1)\n",
    "\n",
    "    # 1. If both arguments are 2-D they are multiplied like conventional matrices.\n",
    "    # 2. If either argument is N-D, N > 2, it is treated as a stack of matrices \n",
    "    # residing in the last two indexes and broadcast accordingly.\n",
    "    if A.ndim >= 2 and B.ndim >= 2:\n",
    "        dldA = G @ B.swapaxes(-2, -1)\n",
    "        dldB = A.swapaxes(-2, -1) @ G\n",
    "    if A.ndim == 1:\n",
    "        # 3. If the first argument is 1-D, it is promoted to a matrix by prepending a\n",
    "        #    1 to its dimensions. After matrix multiplication the prepended 1 is removed.\n",
    "        A_ = A[np.newaxis, :]\n",
    "        G_ = G[np.newaxis, :]\n",
    "        dldA = G @ B.swapaxes(-2, -1) \n",
    "        dldB = A_.swapaxes(-2, -1) @ G_ # outer product\n",
    "    elif B.ndim == 1:\n",
    "        # 4. If the second argument is 1-D, it is promoted to a matrix by appending \n",
    "        #    a 1 to its dimensions. After matrix multiplication the appended 1 is removed.\n",
    "        B_ = B[:, np.newaxis]\n",
    "        G_ = G[:, np.newaxis]\n",
    "        dldA = G_ @ B_.swapaxes(-2, -1) # outer product\n",
    "        dldB = A.swapaxes(-2, -1) @ G\n",
    "    return (unbroadcast(A, dldA), \n",
    "            unbroadcast(B, dldB))\n",
    "        \n",
    "\n",
    "matmul = Op(\n",
    "    apply=np.matmul,\n",
    "    vjp=matmul_vjp,\n",
    "    name='@',\n",
    "    nargs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47606608-c165-459b-b6a6-3bda378de5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_vjp(dldf, x):\n",
    "    dldx = dldf * np.exp(x)\n",
    "    return (unbroadcast(x, dldx),)\n",
    "exp = Op(\n",
    "    apply=np.exp,\n",
    "    vjp=exp_vjp,\n",
    "    name='exp',\n",
    "    nargs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ec346-2524-44a1-9a2a-56a4db710369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_vjp(dldf, x):\n",
    "    dldx = dldf / x\n",
    "    return (unbroadcast(x, dldx),)\n",
    "log = Op(\n",
    "    apply=np.log,\n",
    "    vjp=log_vjp,\n",
    "    name='log',\n",
    "    nargs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192c432-d8f9-4ffe-9853-abdf1adb81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_vjp(dldf, x, axis=None, **kwargs):\n",
    "    if axis is not None:\n",
    "        dldx = np.expand_dims(dldf, axis=axis) * np.ones_like(x)\n",
    "    else:\n",
    "        dldx = dldf * np.ones_like(x)\n",
    "    return (unbroadcast(x, dldx),)\n",
    "\n",
    "sum_ = Op(\n",
    "    apply=np.sum,\n",
    "    vjp=sum_vjp,\n",
    "    name='sum',\n",
    "    nargs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a608920-289a-484c-9f94-587d37775ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_vjp(dldf, a, b):\n",
    "    dlda = dldf * np.where(a > b, 1, 0)\n",
    "    dldb = dldf * np.where(a > b, 0, 1)\n",
    "    return unbroadcast(a, dlda), unbroadcast(b, dldb)\n",
    "\n",
    "maximum = Op(\n",
    "    apply=np.maximum,\n",
    "    vjp=maximum_vjp,\n",
    "    name='maximum',\n",
    "    nargs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2b3d8-ef4e-407a-9ff6-c04d057d14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoOp = Op(apply=None, name='', vjp=None, nargs=0)\n",
    "class Tensor:\n",
    "    __array_priority__ = 100\n",
    "    def __init__(self, value, grad=None, parents=(), op=NoOp, kwargs={}, requires_grad=True):\n",
    "        self.value = np.asarray(value)\n",
    "        self.grad = grad\n",
    "        self.parents = parents\n",
    "        self.op = op\n",
    "        self.kwargs = kwargs\n",
    "        self.requires_grad = requires_grad\n",
    "    \n",
    "    shape = property(lambda self: self.value.shape)\n",
    "    ndim  = property(lambda self: self.value.ndim)\n",
    "    size  = property(lambda self: self.value.size)\n",
    "    dtype = property(lambda self: self.value.dtype)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        cls = type(self)\n",
    "        other = other if isinstance(other, cls) else cls(other)\n",
    "        return cls(add.apply(self.value, other.value),\n",
    "                   parents=(self, other),\n",
    "                   op=add)\n",
    "    __radd__ = __add__\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        cls = type(self)\n",
    "        other = other if isinstance(other, cls) else cls(other)\n",
    "        return cls(mul.apply(self.value, other.value),\n",
    "                   parents=(self, other),\n",
    "                   op=mul)\n",
    "    __rmul__ = __mul__\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        cls = type(self)\n",
    "        other = other if isinstance(other, cls) else cls(other)\n",
    "        return cls(matmul.apply(self.value, other.value),\n",
    "                  parents=(self, other),\n",
    "                  op=matmul)\n",
    "    \n",
    "    def exp(self):\n",
    "        cls = type(self)\n",
    "        return cls(exp.apply(self.value),\n",
    "                parents=(self,),\n",
    "                op=exp)\n",
    "    \n",
    "    def log(self):\n",
    "        cls = type(self)\n",
    "        return cls(log.apply(self.value),\n",
    "                parents=(self, ),\n",
    "                op=log)\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        cls = type(self)\n",
    "        other = other if isinstance(other, cls) else cls(other)\n",
    "        return (self.log() * other).exp()\n",
    "    \n",
    "    def __div__(self, other):\n",
    "        return self * (other**(-1))\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (other * (-1))\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self*(-1)\n",
    "    \n",
    "    def sum(self, axis=None):\n",
    "        cls = type(self)\n",
    "        return cls(sum_.apply(self.value, axis=axis),\n",
    "                   parents=(self,),\n",
    "                   op=sum_,\n",
    "                   kwargs=dict(axis=axis))\n",
    "\n",
    "    def maximum(self, other):\n",
    "        cls = type(self)\n",
    "        other = other if isinstance(other, cls) else cls(other)\n",
    "        return cls(maximum.apply(self.value, other.value),\n",
    "                   parents=(self, other),\n",
    "                   op=maximum)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        cls = type(self)\n",
    "        return f\"{cls.__name__}(value={self.value}, op={self.op.name})\" if self.parents else f\"{cls.__name__}(value={self.value})\"\n",
    "        #return f\"{cls.__name__}(value={self.value}, parents={self.parents}, op={self.op}\"\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        self.grad = grad if self.grad is None else (self.grad+grad)\n",
    "        if self.requires_grad and self.parents:\n",
    "            p_vals = [p.value for p in self.parents]\n",
    "            assert len(p_vals) == self.op.nargs\n",
    "            p_grads = self.op.vjp(grad, *p_vals, **self.kwargs)\n",
    "            for p, g in zip(self.parents, p_grads):\n",
    "                p.backward(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b842c-e6b2-4f6c-bfa8-30bc4a6e44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor([1, 2]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58255827-ca28-4f20-96e1-2ff13841ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "except ImportError as e:\n",
    "    import subprocess\n",
    "    subprocess.call(\"pip install --user graphviz\".split())\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for p in v.parents:\n",
    "                edges.add((p, v))\n",
    "                build(p)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        vstr = np.array2string(np.asarray(n.value), precision=4)\n",
    "        gradstr= np.array2string(np.asarray(n.grad), precision=4)\n",
    "        dot.node(name=str(id(n)), label = f\"{{v={vstr} | g={gradstr}}}\", shape='record')\n",
    "        if n.parents:\n",
    "            dot.node(name=str(id(n)) + n.op.name, label=n.op.name)\n",
    "            dot.edge(str(id(n)) + n.op.name, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2.op.name)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d75c1b-9f27-4e29-9234-381ca63c2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very simple example\n",
    "x = Tensor([[1.0, 2.0],\n",
    "            [2.0, -1.0]])\n",
    "y = (x * 2 - 1).maximum(0).sum(axis=-1)\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ad733-0c8c-4bbd-a119-441ccab34161",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(np.ones_like(y))\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5940f5-711e-4fd9-9e01-0deca7a01247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_np(x):\n",
    "    b = [1, 0]\n",
    "    return (x @ b)*np.exp((-x*x).sum(axis=-1))\n",
    "\n",
    "def f_T(x):\n",
    "    b = [1, 0]\n",
    "    return (x @ b)*(-x*x).sum(axis=-1).exp()\n",
    "\n",
    "def grad_f(x):\n",
    "    xT = Tensor(x)\n",
    "    y = f_T(xT)\n",
    "    y.backward(np.ones_like(y.value))\n",
    "    return xT.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f7b0b-66c7-45a2-b088-7ebce07d84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xT = Tensor([1, 2])\n",
    "out = f_T(xT)\n",
    "out.backward(1)\n",
    "print(xT.grad)\n",
    "draw_dot(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f016010-9e21-4e43-ac28-8f68608f3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_jacobian(f, x, h=1e-10):\n",
    "    n = x.shape[-1]\n",
    "    eye = np.eye(n)\n",
    "    x_plus_dx = x + h * eye # n x n\n",
    "    num_jac = (f(x_plus_dx) - f(x)) / h # limit definition of the formula # n x m\n",
    "    if num_jac.ndim >= 2:\n",
    "        num_jac = num_jac.swapaxes(-1, -2) # m x n\n",
    "    return num_jac\n",
    "\n",
    "# Compare our grad_f with numerical gradient\n",
    "def check_numerical_jacobian(f, jac_f,  nD=2, **kwargs):\n",
    "    x = np.random.rand(nD)\n",
    "    print(x)\n",
    "    num_jac = numerical_jacobian(f, x, **kwargs)\n",
    "    print(num_jac)\n",
    "    print(jac_f(x))\n",
    "    return np.allclose(num_jac, jac_f(x), atol=1e-06, rtol=1e-4) # m x n\n",
    "\n",
    "## Throw error if grad_f is wrong\n",
    "assert check_numerical_jacobian(f_np, grad_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a968495-531f-401f-b940-86ecc6e33379",
   "metadata": {},
   "source": [
    "### Customizing backward step (vector-Jacobian product) in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb3e78-78f8-4fd2-ae3f-2334393b5c7d",
   "metadata": {},
   "source": [
    "Consider the derivative of Sigmoid activation function\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)}$$\n",
    "\n",
    "$$\\frac{\\p }{\\p x} \\sigma(x) = -\\frac{1}{(1+\\exp(-x))^2} (-\\exp(-x))  $$\n",
    "\n",
    "The above derivative is computed by chain rule. However, there is much simpler expression that can avoid unnecessary computations,\n",
    "$$\\frac{\\p }{\\p x} \\sigma(x) = \\frac{1}{1+\\exp(-x)} \\frac{\\exp(-x)}{1+\\exp(-x)}  $$\n",
    "$$\\frac{\\p }{\\p x} \\sigma(x) = \\frac{1}{1+\\exp(-x)} \\left(1-\\frac{1}{1+\\exp(-x)}\\right)  $$\n",
    "$$\\frac{\\p }{\\p x} \\sigma(x) = \\sigma(x) (1-\\sigma(x))  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82da01-0c68-4049-8a99-9f68dc01addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "# https://pytorch.org/docs/stable/notes/autograd.html\n",
    "import torch as t\n",
    "\n",
    "class SigmoidCustom(t.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # Because we are saving one of the inputs use `save_for_backward`\n",
    "        # Save non-tensors and non-inputs/non-outputs directly on ctx\n",
    "        \n",
    "        sigmoid_x = 1/(1+(-x).exp())\n",
    "        ctx.save_for_backward(x, sigmoid_x)\n",
    "        return sigmoid_x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        # A function support double backward automatically if autograd\n",
    "        # is able to record the computations performed in backward\n",
    "        x, sigmoid_x = ctx.saved_tensors\n",
    "        jacobian = sigmoid_x * (1-sigmoid_x)\n",
    "        return grad_out * jacobian # vector jacobian product\n",
    "\n",
    "def sigmoid_c(x):\n",
    "    return SigmoidCustom.apply(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6f9cf-4e10-40d5-838b-ec66bfc16872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = t.tensor([100.], requires_grad=True)\n",
    "def s(x):\n",
    "    return 1/(1+(-x).exp())\n",
    "out = s(s(s(x)))\n",
    "out.backward(t.tensor([1.]))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cd45e-5cbc-47bc-8091-667c31fa04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x = t.tensor([100.], requires_grad=True)\n",
    "out = sigmoid_c(sigmoid_c(sigmoid_c(x)))\n",
    "out.backward(t.tensor([1.]))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b1520-9848-4d18-a03e-5b7495c902bc",
   "metadata": {},
   "source": [
    "#### Whats the deal with the unbroadcast function\n",
    "\n",
    "\n",
    "To understand unbroadcast function, first it is important to understand [numpy broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html#basics-broadcasting). Once you understand that then we can proceed to understand unbroadcast function\n",
    "\n",
    "Once you understand broadcasting, we can appreciate the unbroadcast function. Suppose you have `A.shape = (100, 1)` and `B.shape = (1, 100)` and you add them together\n",
    "\n",
    "F = A + B\n",
    "\n",
    "Then `F.shape = (100, 100)`. Note that as if we created copies of A and B along the relevant dimensions.\n",
    "\n",
    "Let's think about backpropagation now. The incoming grad will have a dimension of `dl__dF.shape = (100, 100)` but the returned `dl__dA` must have shape `dl__dA.shape = (100, 1)` and `dl__dB.shape = (1, 100)`.\n",
    "\n",
    "From the chain rule of addition we only know that `dF_{ij}/dA_{ij} = 1` and `dF_{ij}/dB_{ij} = 1`. What happens when you create copies of a variable and they contribute to a function? Let's consider a very simple function, of two variables:\n",
    "\n",
    "$$f(y₁ , y₂) = y₁ y₂$$\n",
    "\n",
    "What if the $y₁$ and $y₂$ were copies of the same variable?\n",
    "$$y₁(x) = x$$\n",
    "$$y₂(x) = x$$\n",
    "\n",
    "We can use the chain rule to find out the result\n",
    "\n",
    "$$∂ₓ f(y₁(x), y₂(x))  = ∂f/∂y₁  ∂ₓy₁ + ∂f/∂y₂ ∂ₓy₂$$\n",
    "\n",
    "If we know that $y₁$ and $y₂$ are just copies of $x$, then $∂ₓy₁ = 1$ and  $∂ₓy₂ = 1$ .\n",
    "\n",
    "$$∂ₓ f(y₁, y₂)  = ∂f/∂y₁ + ∂f/∂y₂$$\n",
    "\n",
    "In other words, if input variables are copied multiple times and then the function is computed using the copies, then the derivative is the sum of the derivatives with respect to the copies.\n",
    "\n",
    "That's what the unbroadcast function does. Because broadcasting creates the copies of the inputs along the mismatching axis, unbroadcasting sums the derivatives along the mismatching axis.\n",
    "\n",
    "Coming back to the `F = A + B` example. We get `dl__dF.shape =  (100, 100)`. `A.shape = (100, 1)` was copied 100 times along the axis=1 (2nd dimension). So `dl__dA = dl__dF.sum(axis=1)` . Same for the B as well. `dl__dB = dl__dF.sum(axis=0)`.\n",
    "\n",
    "That's what unbroadcast function does but in a more generalizable way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e0517-477f-457c-9bd5-7480705a04c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
