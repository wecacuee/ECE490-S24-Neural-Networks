{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a6f23b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129dd658-67dc-4eaa-b8ce-e2ad6bf2b01f",
   "metadata": {},
   "source": [
    "# Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b5c21-6107-4e36-9281-4b5e01f7efad",
   "metadata": {},
   "source": [
    "#### Refs: \n",
    " 1. https://github.com/karpathy/micrograd/tree/master/micrograd\n",
    " 2. https://github.com/mattjj/autodidact\n",
    " 3. https://github.com/mattjj/autodidact/blob/master/autograd/numpy/numpy_vjps.py\n",
    " 4. https://auto-ed.readthedocs.io/en/latest/mod2.html#ii-more-theory\n",
    " 5. https://github.com/lindseysbrown/Auto-eD/blob/master/docs/mod2.rst?plain=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6febdc-5f93-49e3-b607-452b202ab10c",
   "metadata": {},
   "source": [
    "#### Latex macros\n",
    "$$\n",
    "% Calligraphic fonts\n",
    "\\newcommand{\\calA}{{\\cal A}}\n",
    "\\newcommand{\\calB}{{\\cal B}}\n",
    "\\newcommand{\\calC}{{\\cal C}}\n",
    "\\newcommand{\\calD}{{\\cal D}}\n",
    "\\newcommand{\\calE}{{\\cal E}}\n",
    "\\newcommand{\\calF}{{\\cal F}}\n",
    "\\newcommand{\\calG}{{\\cal G}}\n",
    "\\newcommand{\\calH}{{\\cal H}}\n",
    "\\newcommand{\\calI}{{\\cal I}}\n",
    "\\newcommand{\\calJ}{{\\cal J}}\n",
    "\\newcommand{\\calK}{{\\cal K}}\n",
    "\\newcommand{\\calL}{{\\cal L}}\n",
    "\\newcommand{\\calM}{{\\cal M}}\n",
    "\\newcommand{\\calN}{{\\cal N}}\n",
    "\\newcommand{\\calO}{{\\cal O}}\n",
    "\\newcommand{\\calP}{{\\cal P}}\n",
    "\\newcommand{\\calQ}{{\\cal Q}}\n",
    "\\newcommand{\\calR}{{\\cal R}}\n",
    "\\newcommand{\\calS}{{\\cal S}}\n",
    "\\newcommand{\\calT}{{\\cal T}}\n",
    "\\newcommand{\\calU}{{\\cal U}}\n",
    "\\newcommand{\\calV}{{\\cal V}}\n",
    "\\newcommand{\\calW}{{\\cal W}}\n",
    "\\newcommand{\\calX}{{\\cal X}}\n",
    "\\newcommand{\\calY}{{\\cal Y}}\n",
    "\\newcommand{\\calZ}{{\\cal Z}}\n",
    "% Sets:\n",
    "\\newcommand{\\setA}{\\textsf{A}}\n",
    "\\newcommand{\\setB}{\\textsf{B}}\n",
    "\\newcommand{\\setC}{\\textsf{C}}\n",
    "\\newcommand{\\setD}{\\textsf{D}}\n",
    "\\newcommand{\\setE}{\\textsf{E}}\n",
    "\\newcommand{\\setF}{\\textsf{F}}\n",
    "\\newcommand{\\setG}{\\textsf{G}}\n",
    "\\newcommand{\\setH}{\\textsf{H}}\n",
    "\\newcommand{\\setI}{\\textsf{I}}\n",
    "\\newcommand{\\setJ}{\\textsf{J}}\n",
    "\\newcommand{\\setK}{\\textsf{K}}\n",
    "\\newcommand{\\setL}{\\textsf{L}}\n",
    "\\newcommand{\\setM}{\\textsf{M}}\n",
    "\\newcommand{\\setN}{\\textsf{N}}\n",
    "\\newcommand{\\setO}{\\textsf{O}}\n",
    "\\newcommand{\\setP}{\\textsf{P}}\n",
    "\\newcommand{\\setQ}{\\textsf{Q}}\n",
    "\\newcommand{\\setR}{\\textsf{R}}\n",
    "\\newcommand{\\setS}{\\textsf{S}}\n",
    "\\newcommand{\\setT}{\\textsf{T}}\n",
    "\\newcommand{\\setU}{\\textsf{U}}\n",
    "\\newcommand{\\setV}{\\textsf{V}}\n",
    "\\newcommand{\\setW}{\\textsf{W}}\n",
    "\\newcommand{\\setX}{\\textsf{X}}\n",
    "\\newcommand{\\setY}{\\textsf{Y}}\n",
    "\\newcommand{\\setZ}{\\textsf{Z}}\n",
    "% Vectors\n",
    "\\newcommand{\\bfa}{\\mathbf{a}}\n",
    "\\newcommand{\\bfb}{\\mathbf{b}}\n",
    "\\newcommand{\\bfc}{\\mathbf{c}}\n",
    "\\newcommand{\\bfd}{\\mathbf{d}}\n",
    "\\newcommand{\\bfe}{\\mathbf{e}}\n",
    "\\newcommand{\\bff}{\\mathbf{f}}\n",
    "\\newcommand{\\bfg}{\\mathbf{g}}\n",
    "\\newcommand{\\bfh}{\\mathbf{h}}\n",
    "\\newcommand{\\bfi}{\\mathbf{i}}\n",
    "\\newcommand{\\bfj}{\\mathbf{j}}\n",
    "\\newcommand{\\bfk}{\\mathbf{k}}\n",
    "\\newcommand{\\bfl}{\\mathbf{l}}\n",
    "\\newcommand{\\bfm}{\\mathbf{m}}\n",
    "\\newcommand{\\bfn}{\\mathbf{n}}\n",
    "\\newcommand{\\bfo}{\\mathbf{o}}\n",
    "\\newcommand{\\bfp}{\\mathbf{p}}\n",
    "\\newcommand{\\bfq}{\\mathbf{q}}\n",
    "\\newcommand{\\bfr}{\\mathbf{r}}\n",
    "\\newcommand{\\bfs}{\\mathbf{s}}\n",
    "\\newcommand{\\bft}{\\mathbf{t}}\n",
    "\\newcommand{\\bfu}{\\mathbf{u}}\n",
    "\\newcommand{\\bfv}{\\mathbf{v}}\n",
    "\\newcommand{\\bfw}{\\mathbf{w}}\n",
    "\\newcommand{\\bfx}{\\mathbf{x}}\n",
    "\\newcommand{\\bfy}{\\mathbf{y}}\n",
    "\\newcommand{\\bfz}{\\mathbf{z}}\n",
    "\\newcommand{\\bfalpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\bfgamma}{\\boldsymbol{\\gamma}}\n",
    "\\newcommand{\\bfdelta}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\bfepsilon}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\bfzeta}{\\boldsymbol{\\zeta}}\n",
    "\\newcommand{\\bfeta}{\\boldsymbol{\\eta}}\n",
    "\\newcommand{\\bftheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\bfiota}{\\boldsymbol{\\iota}}\n",
    "\\newcommand{\\bfkappa}{\\boldsymbol{\\kappa}}\n",
    "\\newcommand{\\bflambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\bfmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\bfnu}{\\boldsymbol{\\nu}}\n",
    "\\newcommand{\\bfomicron}{\\boldsymbol{\\omicron}}\n",
    "\\newcommand{\\bfpi}{\\boldsymbol{\\pi}}\n",
    "\\newcommand{\\bfrho}{\\boldsymbol{\\rho}}\n",
    "\\newcommand{\\bfsigma}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\bftau}{\\boldsymbol{\\tau}}\n",
    "\\newcommand{\\bfupsilon}{\\boldsymbol{\\upsilon}}\n",
    "\\newcommand{\\bfphi}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\bfchi}{\\boldsymbol{\\chi}}\n",
    "\\newcommand{\\bfpsi}{\\boldsymbol{\\psi}}\n",
    "\\newcommand{\\bfomega}{\\boldsymbol{\\omega}}\n",
    "\\newcommand{\\bfxi}{\\boldsymbol{\\xi}}\n",
    "\\newcommand{\\bfell}{\\boldsymbol{\\ell}}\n",
    "% Matrices\n",
    "\\newcommand{\\bfA}{\\mathbf{A}}\n",
    "\\newcommand{\\bfB}{\\mathbf{B}}\n",
    "\\newcommand{\\bfC}{\\mathbf{C}}\n",
    "\\newcommand{\\bfD}{\\mathbf{D}}\n",
    "\\newcommand{\\bfE}{\\mathbf{E}}\n",
    "\\newcommand{\\bfF}{\\mathbf{F}}\n",
    "\\newcommand{\\bfG}{\\mathbf{G}}\n",
    "\\newcommand{\\bfH}{\\mathbf{H}}\n",
    "\\newcommand{\\bfI}{\\mathbf{I}}\n",
    "\\newcommand{\\bfJ}{\\mathbf{J}}\n",
    "\\newcommand{\\bfK}{\\mathbf{K}}\n",
    "\\newcommand{\\bfL}{\\mathbf{L}}\n",
    "\\newcommand{\\bfM}{\\mathbf{M}}\n",
    "\\newcommand{\\bfN}{\\mathbf{N}}\n",
    "\\newcommand{\\bfO}{\\mathbf{O}}\n",
    "\\newcommand{\\bfP}{\\mathbf{P}}\n",
    "\\newcommand{\\bfQ}{\\mathbf{Q}}\n",
    "\\newcommand{\\bfR}{\\mathbf{R}}\n",
    "\\newcommand{\\bfS}{\\mathbf{S}}\n",
    "\\newcommand{\\bfT}{\\mathbf{T}}\n",
    "\\newcommand{\\bfU}{\\mathbf{U}}\n",
    "\\newcommand{\\bfV}{\\mathbf{V}}\n",
    "\\newcommand{\\bfW}{\\mathbf{W}}\n",
    "\\newcommand{\\bfX}{\\mathbf{X}}\n",
    "\\newcommand{\\bfY}{\\mathbf{Y}}\n",
    "\\newcommand{\\bfZ}{\\mathbf{Z}}\n",
    "\\newcommand{\\bfGamma}{\\boldsymbol{\\Gamma}}\n",
    "\\newcommand{\\bfDelta}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\bfTheta}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\bfLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\bfPi}{\\boldsymbol{\\Pi}}\n",
    "\\newcommand{\\bfSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\bfUpsilon}{\\boldsymbol{\\Upsilon}}\n",
    "\\newcommand{\\bfPhi}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\bfPsi}{\\boldsymbol{\\Psi}}\n",
    "\\newcommand{\\bfOmega}{\\boldsymbol{\\Omega}}\n",
    "% Blackboard Bold:\n",
    "\\newcommand{\\bbA}{\\mathbb{A}}\n",
    "\\newcommand{\\bbB}{\\mathbb{B}}\n",
    "\\newcommand{\\bbC}{\\mathbb{C}}\n",
    "\\newcommand{\\bbD}{\\mathbb{D}}\n",
    "\\newcommand{\\bbE}{\\mathbb{E}}\n",
    "\\newcommand{\\bbF}{\\mathbb{F}}\n",
    "\\newcommand{\\bbG}{\\mathbb{G}}\n",
    "\\newcommand{\\bbH}{\\mathbb{H}}\n",
    "\\newcommand{\\bbI}{\\mathbb{I}}\n",
    "\\newcommand{\\bbJ}{\\mathbb{J}}\n",
    "\\newcommand{\\bbK}{\\mathbb{K}}\n",
    "\\newcommand{\\bbL}{\\mathbb{L}}\n",
    "\\newcommand{\\bbM}{\\mathbb{M}}\n",
    "\\newcommand{\\bbN}{\\mathbb{N}}\n",
    "\\newcommand{\\bbO}{\\mathbb{O}}\n",
    "\\newcommand{\\bbP}{\\mathbb{P}}\n",
    "\\newcommand{\\bbQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\bbR}{\\mathbb{R}}\n",
    "\\newcommand{\\bbS}{\\mathbb{S}}\n",
    "\\newcommand{\\bbT}{\\mathbb{T}}\n",
    "\\newcommand{\\bbU}{\\mathbb{U}}\n",
    "\\newcommand{\\bbV}{\\mathbb{V}}\n",
    "\\newcommand{\\bbW}{\\mathbb{W}}\n",
    "\\newcommand{\\bbX}{\\mathbb{X}}\n",
    "\\newcommand{\\bbY}{\\mathbb{Y}}\n",
    "\\newcommand{\\bbZ}{\\mathbb{Z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0543d2b-206c-4d98-9b50-cc687551381f",
   "metadata": {},
   "source": [
    "### Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca81ed-2bee-470e-8f87-57d9e3128287",
   "metadata": {},
   "source": [
    "#### Scalar single-variable chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ef069-5a7d-41e6-be0d-7578a94a8d5a",
   "metadata": {},
   "source": [
    "$\\newcommand{\\p}{\\partial}$\n",
    "Recall the limit definition of derivative of a function,\n",
    "$$g'(x) = \\lim_{h \\to 0} \\frac{g(x+h) - g(x)}{h}.$$\n",
    "From the limit definition you can find the value of $g(x+h)$ as\n",
    "$$\\lim_{h \\to 0} g(x+h) = \\lim_{h \\to 0} g(x) + g'(x)h.$$\n",
    "\n",
    "You can use this rule to find the chain rule of finding the chaining of two functions together,\n",
    "\\begin{align}\n",
    "\\frac{\\p f(g(x))}{\\p x} &= \\lim_{h \\to 0} \\frac{f(g(x + h)) - f(g(x))}{h}\\\\\n",
    "&=\\lim_{h \\to 0} \\frac{f(g(x) + g'(x) h) - f(g(x))}{h}\\\\\n",
    "&=\\lim_{h \\to 0} \\frac{f(g(x)) + f'(g(x))g'(x)h - f(g(x))}{h}\\\\\n",
    "&=f'(g(x))g'(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3949879-0bda-4b5c-be51-5ea21320a132",
   "metadata": {},
   "source": [
    "#### Scalar two-variable chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d22420-174d-40b1-9bbe-32dd05c5e9b5",
   "metadata": {},
   "source": [
    "Consider a function of two variables $f(u(x),v(x))$. Find its derivative,\n",
    "\\begin{align}\n",
    "\\frac{\\p f(u(x), v(x))}{\\p x} &= \\lim_{h \\to 0} \\frac{f(u(x + h), v(x+h)) - f(u(x), v(x))}{h}\\\\\n",
    "&=\\lim_{h \\to 0} \\frac{f(u(x) + u'(x) h, v(x) + v'(x)h) - f(u(x), v(x))}{h}\n",
    "\\end{align}\n",
    "Now $f(u + \\delta u, v + \\delta v)$ should not be expanded in one step but in two steps. First keep $v + \\delta v$ as it is, and expand with respect to $u + \\delta u$\n",
    "$$\\lim_{\\delta v, \\delta u \\to 0} f(u+\\delta u, v+\\delta v) = \\lim_{\\delta v, \\delta u \\to 0} f(u, v+\\delta v) + f'_u(u, v+\\delta v) \\delta u,$$\n",
    "and then do the same with $v+\\delta v$,\n",
    "$$\\lim_{\\delta v, \\delta u \\to 0} f(u+\\delta u, v+\\delta v) = \\lim_{\\delta v, \\delta u \\to 0} f(u, v) + f'_v(u, v) \\delta v + f'_u(u, v+\\delta v) \\delta u,$$\n",
    "\n",
    "We use $\\lim_{\\delta v \\to 0} f'_u(u, v+\\delta v) = \\lim_{\\delta v \\to 0}  f'_u(u, v)$ to get,\n",
    "$$\\lim_{\\delta v, \\delta u \\to 0} f(u+\\delta u, v+\\delta v) = \\lim_{\\delta v, \\delta u \\to 0} f(u, v) + f'_v(u, v) \\delta v + f'_u(u, v) \\delta u.$$\n",
    "\n",
    "Going back to the chain rule,\n",
    "\\begin{align}\n",
    "\\frac{\\p f(u(x), v(x))}{\\p x} \n",
    "&= \n",
    "\\lim_{h \\to 0} \\frac{f(u(x) + u'(x) h, v(x) + v'(x)h) - f(u(x), v(x))}{h} \\\\\n",
    "&=\n",
    "\\lim_{h \\to 0} \\frac{f(u(x), v(x)) + f'_v(u(x), v(x)) v'(x)h + f'_u(u(x), v(x)) u'(x) h - f(u(x), v(x))}{h} \\\\\n",
    "&=\n",
    "\\lim_{h \\to 0} \\frac{f'_v(u(x), v(x)) v'(x)h + f'_u(u(x), v(x)) u'(x) h }{h} \\\\\n",
    "&= f'_v(u(x), v(x)) v'(x) + f'_u(u(x), v(x)) u'(x) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d138039a-6ee3-48ab-8cf7-63d014fb9432",
   "metadata": {},
   "source": [
    "#### Scalar valued vector function chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf76f98-2234-464b-b8d9-f42d2f989917",
   "metadata": {},
   "source": [
    "Consider two functions $f(\\bfg): \\bbR^m \\to \\bbR$, $\\bfg(x): \\bbR \\to \\bbR^m$ that can be composed together $f(\\bfg(x))$. We want to find the derivative of composition $f \\circ g$ by chain rule. \n",
    "\n",
    "Recall that the derivative (Jacobian) of $f(\\bfy)$ is a row vector,\n",
    "$$\\frac{\\p f(\\bfg)}{\\p \\bfg} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p f}{\\p g_1} & \\frac{\\p f}{\\p g_2} & \\dots & \\frac{\\p f}{\\p g_m}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "And the derivative (Jacobian) of $\\bfg(x)$ is a column vector,\n",
    "$$\\frac{\\p \\bfg(x)}{\\p x} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p g_1}{\\p x} \\\\\n",
    "\\frac{\\p g_2}{\\p x}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p g_m}{\\p x}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Note that a vector function is a multi-variate scalar function \n",
    "$$f(\\bfg(x)) = f(g_1(x), g_2(x), \\dots, g_m(x)).$$\n",
    "\n",
    "We can apply the multi-variate scalar function chain rule,\n",
    "\\begin{align} \\frac{\\p}{\\p x} f(\\bfg(x)) = f'_{g_1}(g_1(x), \\dots, g_m(x)) g'_1(x) + \\dots + f'_{g_m}(g_1(x), \\dots, g_m(x)) g'_m(x)\\\\\n",
    "= f'_{g_1}(\\bfg(x)) g'_1(x) + \\dots + f'_{g_m}(\\bfg(x)) g'_m(x)\n",
    ".\\end{align}\n",
    "\n",
    "The derivatives of $\\bfg$ can be separated from derivatives of $f$ as vector multiplication,\n",
    "$$ \\frac{\\p}{\\p x} f(\\bfg(x)) = \n",
    "\\begin{bmatrix}\n",
    "f'_{g_1}(\\bfg(x)) & \\dots & f'_{g_m}(\\bfg(x))\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "g'_1(x) \\\\ \\vdots \\\\  g'_m(x)\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "Hence the chain rule for vector derivatives works out for our definition of vector derivatives,\n",
    "$$\\frac{\\p }{\\p \\bfx}  f(\\bfg(x))= \\frac{\\p f(\\bfg(x))}{\\p \\bfg}\\frac{\\p \\bfg(x)}{\\p x}.$$\n",
    "\n",
    "Note that the order of multiplication matters, specifically\n",
    "$$\\frac{\\p }{\\p \\bfx}  f(\\bfg(x))\\ne \\frac{\\p \\bfg(x)}{\\p x} \\frac{\\p f(\\bfg(x))}{\\p \\bfg}.$$\n",
    "\n",
    "This is a consequence of row-vector convention. If we chose a column-vector convention the result will be completely different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b524214-26cb-42f1-b865-75e8dd2c1386",
   "metadata": {},
   "source": [
    "#### General chain rule\n",
    "\n",
    "\n",
    "Let the function be $\\bff(\\bfg): \\bbR^m \\to \\bbR^n$ and $\\bfg(\\bfx): \\bbR^p \\to \\bbR^m$, then the derivative (Jacobian) of their composition $\\bff \\circ \\bfg$ is \n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfx} \\bff(\\bfg(\\bfx)) = \\frac{\\p \\bff(\\bfg(\\bfx))}{\\p \\bfg}\\frac{\\p \\bfg(\\bfx)}{\\p \\bfx} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794af26-027d-42a4-b0be-9f89cd3efd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refs: \n",
    "# 1. https://github.com/karpathy/micrograd/tree/master/micrograd\n",
    "# 2. https://github.com/mattjj/autodidact\n",
    "# 3. https://github.com/mattjj/autodidact/blob/master/autograd/numpy/numpy_vjps.py\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def unbroadcast(target, g, axis=0):\n",
    "    \"\"\"Remove broadcasted dimensions by summing along them.\n",
    "    When computing gradients of a broadcasted value, this is the right thing to\n",
    "    do when computing the total derivative and accounting for cloning.\n",
    "    \"\"\"\n",
    "    while np.ndim(g) > np.ndim(target):\n",
    "        g = g.sum(axis=axis)\n",
    "    for axis, size in enumerate(target.shape):\n",
    "        if size == 1:\n",
    "            g = g.sum(axis=axis, keepdims=True)\n",
    "    if np.iscomplexobj(g) and not np.iscomplex(target):\n",
    "        g = g.real()\n",
    "    return g\n",
    "\n",
    "Op = namedtuple('Op', ['apply',\n",
    "                   'vjp',\n",
    "                   'name',\n",
    "                   'nargs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178a445-25a7-40e5-82c4-9c03e9c3385f",
   "metadata": {},
   "source": [
    "## Vector Jacobian Product for addition\n",
    "$\\newcommand{\\bfa}{\\mathbf{a}}$\n",
    "$\\newcommand{\\bfb}{\\mathbf{b}}$\n",
    "$\\newcommand{\\bfA}{\\mathbf{A}}$\n",
    "$\\newcommand{\\bfB}{\\mathbf{B}}$\n",
    "$\\newcommand{\\bfC}{\\mathbf{C}}$\n",
    "$\\newcommand{\\bfF}{\\mathbf{F}}$\n",
    "$\\newcommand{\\bff}{\\mathbf{f}}$\n",
    "$\\newcommand{\\bbR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\bfI}{\\mathbf{I}}$\n",
    "$\\newcommand{\\bfzero}{\\mathbf{0}}$\n",
    "$\\newcommand{\\p}{\\partial}$\n",
    "\n",
    "$$\\bff(\\bfa, \\bfb) = \\bfa + \\bfb$$ where $\\bfa, \\bfb, \\bff \\in \\bbR^n$\n",
    "\n",
    "Let $l(\\bff(\\bfa, \\bfb)) \\in \\bbR$ be the eventual scalar output. We find $\\frac{\\p l}{\\p \\bfa}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product.\n",
    "$$\\frac{\\p }{\\p \\bfa} l(\\bff(\\bfa, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfa}(\\bfa + \\bfb)\n",
    "= \\frac{\\p l}{\\p \\bff} (\\bfI_{n \\times n} + 0_{n \\times n}) = \\frac{\\p l}{\\p \\bff}$$ \n",
    "\n",
    "Similarly, \n",
    "$$\\frac{\\p }{\\p \\bfb} l(\\bff(\\bfa, \\bfb)) = \\frac{\\p l}{\\p \\bff}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a152b-313b-46a5-99b7-13c0b47d457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vjp(dldf, a, b):\n",
    "    dlda = unbroadcast(a, dldf)\n",
    "    dldb = unbroadcast(b, dldf)\n",
    "    return dlda, dldb\n",
    "    \n",
    "add = Op(\n",
    "    apply=np.add,\n",
    "    vjp=add_vjp,\n",
    "    name='+',\n",
    "    nargs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc1abf2-d748-4b2e-b7b5-6cb748f73d79",
   "metadata": {},
   "source": [
    "## VJP for element-wise multiplication\n",
    "\n",
    "<!-- $$\\bff(\\alpha, \\bfb) = \\alpha \\bfb$$ where $\\alpha \\in \\bbR,  and \\bfb, \\bff \\in \\bbR^n$\n",
    "\n",
    "Let $l(\\bff(\\alpha, \\bfb)) \\in \\bbR$ be the eventual scalar output. We find $\\frac{\\p l}{\\p \\alpha}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product.\n",
    "$$\\frac{\\p }{\\p \\alpha} l(\\bff(\\alpha, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\alpha}(\\alpha \\bfb)\n",
    "= \\frac{\\p l}{\\p \\bff} \\bfb$$ \n",
    "\n",
    "Similarly, \n",
    "$$\\frac{\\p }{\\p \\bfb} l(\\bff(\\alpha, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\alpha I_{n \\times n}$$ -->\n",
    "\n",
    "$$f(\\alpha, \\beta) = \\alpha \\beta$$ where $\\alpha, \\beta, f \\in \\bbR$\n",
    "\n",
    "Let $l(f(\\alpha, \\beta)) \\in \\bbR$ be the eventual scalar output. We find $\\frac{\\p l}{\\p \\alpha}$ and $\\frac{\\p l}{\\p \\beta}$ for Vector Jacobian product.\n",
    "\n",
    "$$\\frac{\\p }{\\p \\alpha} l(f(\\alpha, \\beta)) = \\frac{\\p l}{\\p f}\\frac{\\p }{\\p \\alpha}(\\alpha \\beta)\n",
    "= \\frac{\\p l}{\\p f} \\beta$$ \n",
    "$$\\frac{\\p }{\\p \\beta} l(f(\\alpha, \\beta)) = \\frac{\\p l}{\\p f}\\frac{\\p }{\\p \\beta}(\\alpha \\beta)\n",
    "= \\frac{\\p l}{\\p f} \\alpha$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867a65a-b191-4349-9d93-cbc32e436ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_vjp(dldf, a, b):\n",
    "    dlda = unbroadcast(a, dldf * b)\n",
    "    dldb = unbroadcast(b, dldf * a)\n",
    "    return dlda, dldb\n",
    "\n",
    "mul = Op(\n",
    "    apply=np.multiply,\n",
    "    vjp=mul_vjp,\n",
    "    name='*',\n",
    "    nargs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00554cf4-2dee-42d2-a900-e69e1621d323",
   "metadata": {},
   "source": [
    "## VJP for matrix-matrix, matrix-vector and vector-vector multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa238d3-6dbf-4805-b1ea-b0f8f3929785",
   "metadata": {},
   "source": [
    "### Case 1: VJP for vector-vector multiplication\n",
    "\n",
    "$$f(\\bfa, \\bfb) = \\bfa^\\top \\bfb$$ where $f \\in \\bbR$,  and $\\bfb, \\bfa \\in \\bbR^n$\n",
    "\n",
    "Let $l(f(\\bfa, \\bfb)) \\in \\bbR$ be the eventual scalar output. We find $\\frac{\\p l}{\\p \\bfa}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product.\n",
    "$$\\frac{\\p }{\\p \\bfa} l(f(\\bfa, \\bfb)) = \\frac{\\p l}{\\p f}\\frac{\\p }{\\p \\bfa}(\\bfa^\\top \\bfb)\n",
    "= \\frac{\\p l}{\\p f} \\bfb^\\top$$ \n",
    "\n",
    "Similarly, \n",
    "$$\\frac{\\p }{\\p \\bfb} l(f(\\bfa, \\bfb)) = \\frac{\\p l}{\\p f}\\bfa^\\top$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96eaea-a043-4da4-a395-0adc4e3eef09",
   "metadata": {},
   "source": [
    "### Case 2: VJP for matrix-vector multiplication\n",
    "\n",
    "Let\n",
    "$$\\bff(\\bfA, \\bfb) = \\bfA \\bfb$$ where $\\bff \\in \\bbR^m$, $\\bfb \\in \\bbR^n$, and $\\bfA \\in \\bbR^{m \\times n}$\n",
    "\n",
    "Let $l(\\bff(\\bfA, \\bfb)) \\in \\bbR$ be the eventual scalar output. We want to findfind $\\frac{\\p l}{\\p \\bfA}$ and $\\frac{\\p l}{\\p \\bfb}$ for Vector Jacobian product.\n",
    "\n",
    "\n",
    "Let \n",
    "$$ \\bfA = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\dots & a_{1n}\\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\dots & a_{mn}\n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix}\n",
    "\\bfa_1^\\top\\\\\n",
    "\\bfa_2^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\bfa_m^\\top\n",
    "\\end{bmatrix}$$, where each $\\bfa_i^\\top \\in \\bbR^{1 \\times n}$ and $a_{ij} \\in \\bbR$.\n",
    "\n",
    "Define matrix derivative of  scalar to be:\n",
    "$$ \\frac{\\p l}{\\p \\bfA} = \\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p a_{11}} & \\frac{\\p l}{\\p a_{12}} & \\dots & \\frac{\\p l}{\\p a_{1n}}\\\\\n",
    "\\frac{\\p l}{\\p a_{21}} & \\frac{\\p l}{\\p a_{22}} & \\dots & \\frac{\\p l}{\\p a_{2n}}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\p l}{\\p a_{m1}} & \\frac{\\p l}{\\p a_{m2}} & \\dots & \\frac{\\p l}{\\p a_{mn}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p \\bfa_1}\\\\\n",
    "\\frac{\\p l}{\\p \\bfa_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p l}{\\p \\bfa_m}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\\frac{\\p }{\\p \\bfA} l(\\bff(\\bfa, \\bfb)) = \\frac{\\p l}{\\p \\bff} \\frac{\\p }{\\p \\bfA} (\\bfA\\bfb)$$.\n",
    "\n",
    "Note that $$\\bfA\\bfb = \n",
    "\\begin{bmatrix}\n",
    "\\bfa_1^\\top\\\\\n",
    "\\bfa_2^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\bfa_m^\\top\n",
    "\\end{bmatrix}\\bfb \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\bfa_1^\\top\\bfb\\\\\n",
    "\\bfa_2^\\top\\bfb\\\\\n",
    "\\vdots\\\\\n",
    "\\bfa_m^\\top\\bfb\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since $\\bfa_i^\\top \\bfb$ is a scalar, it is easier to find its derivative with respect to the matrix $\\bfA$.\n",
    "\n",
    "$$\\frac{\\p }{\\p \\bfA}\\bfa_i^\\top \\bfb \n",
    "= \\begin{bmatrix}\n",
    "\\frac{\\p \\bfa_i^\\top \\bfb }{\\p \\bfa_1}\\\\\n",
    "\\frac{\\p \\bfa_i^\\top \\bfb }{\\p \\bfa_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p \\bfa_i^\\top \\bfb }{\\p \\bfa_i}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p \\bfa_i^\\top \\bfb }{\\p \\bfa_m}\n",
    "\\end{bmatrix} \n",
    "= \\begin{bmatrix}\n",
    "\\bfzero^\\top_{n}\\\\\n",
    "\\bfzero^\\top_{n}\\\\\n",
    "\\vdots\\\\\n",
    "\\bfb^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\bfzero^\\top_{n}\n",
    "\\end{bmatrix}\n",
    "\\in \\bbR^{m \\times n}$$\n",
    "\n",
    "Let \n",
    "$$\\frac{\\p l}{\\p \\bff} = \\begin{bmatrix} \\frac{\\p l}{\\p f_1} & \\frac{\\p l}{\\p f_2} & \\dots & \\frac{\\p l}{\\p f_m} \\end{bmatrix}$$\n",
    "\n",
    "Then $$\\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\\bfa_i^\\top \\bfb = \n",
    "\\begin{bmatrix} \\frac{\\p l}{\\p f_1} & \\frac{\\p l}{\\p f_2} & \\dots & \\frac{\\p l}{\\p f_m} \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\bfzero^\\top_{n}\\\\\n",
    "\\bfzero^\\top_{n}\\\\\n",
    "\\vdots\\\\\n",
    "\\bfb^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\bfzero^\\top_{n}\n",
    "\\end{bmatrix}\n",
    "= \\frac{\\p l}{\\p f_i}\\bfb^\\top \\in \\bbR^{1 \\times n}$$\n",
    "\n",
    "Returning to our original quest for \n",
    "$$ \\frac{\\p }{\\p \\bfA} l(\\bff(\\bfA, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\\bfA \\bfb = \n",
    "\\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\n",
    "\\begin{bmatrix}\n",
    "\\bfa_1^\\top\\bfb\\\\\n",
    "\\bfa_2^\\top\\bfb\\\\\n",
    "\\vdots\\\\\n",
    "\\bfa_m^\\top\\bfb\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\\bfa_1^\\top\\bfb\\\\\n",
    "\\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\\bfa_2^\\top\\bfb\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfA}\\bfa_m^\\top\\bfb\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p f_1}\\bfb^\\top\\\\\n",
    "\\frac{\\p l}{\\p f_2}\\bfb^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p l}{\\p f_m}\\bfb^\\top\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that \n",
    "$$  \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p f_1}\\bfb^\\top\\\\\n",
    "\\frac{\\p l}{\\p f_2}\\bfb^\\top\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p l}{\\p f_m}\\bfb^\\top\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix} \\frac{\\p l}{\\p f_1} \\\\ \\frac{\\p l}{\\p f_2} \\\\ \\dots \\\\ \\frac{\\p l}{\\p f_m} \\end{bmatrix} \\bfb^\\top \n",
    "= \\left(\\frac{\\p l}{\\p \\bff}\\right)^\\top \\bfb^\\top\n",
    "$$\n",
    "\n",
    "We can group the terms inside a single transpose.\n",
    "\n",
    "Which results in\n",
    "$$ \\frac{\\p }{\\p \\bfA} l(\\bff(\\bfA, \\bfb)) = \\left(\\bfb\\frac{\\p l}{\\p \\bff}\\right)^\\top$$\n",
    "\n",
    "The derivative with respect to $\\bfb$ is simpler:\n",
    "$$ \\frac{\\p }{\\p \\bfb} l(\\bff(\\bfA, \\bfb)) = \\frac{\\p l}{\\p \\bff}\\frac{\\p }{\\p \\bfb}(\\bfA\\bfb) = \\frac{\\p l}{\\p \\bff} \\bfA$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a268a7-b908-4e33-ae93-e1263dacf2bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Case 3: VJP for matrix-matrix multiplication\n",
    "\n",
    "Let\n",
    "$$\\bfF(\\bfA, \\bfB) = \\bfA \\bfB$$ where $\\bfF \\in \\bbR^{m \\times p}$, $\\bfB \\in \\bbR^{n \\times p}$, and $\\bfA \\in \\bbR^{m \\times n}$\n",
    "\n",
    "Let $l(\\bfF(\\bfA, \\bfB)) \\in \\bbR$ be the eventual scalar output. We want to find $\\frac{\\p l}{\\p \\bfA}$ and $\\frac{\\p l}{\\p \\bfB}$ for Vector Jacobian product.\n",
    "\n",
    "\n",
    "Note that a matrix-matrix multiplication can be written in terms horizontal stacking of matrix-vector multiplications. Specifically, write $\\bfF$ and $\\bfB$ in terms of their column vectors:\n",
    "$$\\bfB = \\begin{bmatrix} \\bfb_1 & \\bfb_2 & \\dots & \\bfb_p \\end{bmatrix}$$\n",
    "$$\\bfF = \\begin{bmatrix} \\bff_1 & \\bff_2 & \\dots & \\bff_p \\end{bmatrix}.$$\n",
    "\n",
    "Then for all $i$ \n",
    "$$\\bff_i = \\bfA\\bfb_i$$\n",
    "\n",
    "From the VJP of matrix-vector multiplication, we can write\n",
    "$$ \\frac{\\p l}{\\p \\bff_i}\\frac{\\p }{\\p \\bfA}\\bff_i =\n",
    "\\frac{\\p l}{\\p \\bff_i}\\frac{\\p }{\\p \\bfA}(\\bfA \\bfb_i) = \\left(\\bfb_i\\frac{\\p l}{\\p \\bff_i}\\right)^\\top \\in \\bbR^{ m \\times n }$$\n",
    "and for all $i \\ne j$\n",
    "$$ \\frac{\\p l}{\\p \\bff_j}\\frac{\\p }{\\p \\bfA}(\\bfA \\bfb_i) = \\bfzero_{m \\times n}$$\n",
    "\n",
    "Instead of writing $l(\\bfF)$, we can also write $l(\\bff_1, \\bff_2, \\dots, \\bff_p)$, then by chain rule of functions with multiple arguments, we have,\n",
    "$$\\frac{\\p }{\\p \\bfA}l(\\bfF(\\bfA, \\bfB)) = \\frac{\\p }{\\p \\bfA} l(\\bff_1, \\bff_2, \\dots, \\bff_p)\n",
    "= \\frac{\\p l}{\\p \\bff_1} \\frac{\\p \\bff_1}{\\p \\bfA} + \\frac{\\p l}{\\p \\bff_2} \\frac{\\p \\bff_2}{\\p \\bfA} + \\dots + \\frac{\\p l}{\\p \\bff_p} \\frac{\\p \\bff_p}{\\p \\bfA}$$\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfA}l(\\bfF(\\bfA, \\bfB)) = \n",
    " \\left(\\bfb_1\\frac{\\p l}{\\p \\bff_1}\\right)^\\top \n",
    " + \\left(\\bfb_2\\frac{\\p l}{\\p \\bff_2}\\right)^\\top \n",
    " + \\dots\n",
    "+ \\left(\\bfb_p\\frac{\\p l}{\\p \\bff_p}\\right)^\\top \n",
    "=\n",
    " \\left(\\bfb_1\\frac{\\p l}{\\p \\bff_1}\n",
    " + \\bfb_2\\frac{\\p l}{\\p \\bff_2}\n",
    " + \\dots\n",
    "+ \\bfb_p\\frac{\\p l}{\\p \\bff_p}\\right)^\\top \n",
    "$$\n",
    "\n",
    "It turns out that some of outer products can be compactly written as matrix-matrix multiplication:\n",
    "$$ \\bfb_1\\frac{\\p l}{\\p \\bff_1}\n",
    " + \\bfb_2\\frac{\\p l}{\\p \\bff_2}\n",
    " + \\dots\n",
    "+ \\bfb_p\\frac{\\p l}{\\p \\bff_p} = \n",
    "\\begin{bmatrix}\n",
    "\\bfb_1 & \\bfb_2 & \\dots & \\bfb_p \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\p l}{\\p \\bff_1}\\\\\n",
    "\\frac{\\p l}{\\p \\bff_2}\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\p l}{\\p \\bff_p}\\end{bmatrix} = \\bfB \\left(\\frac{\\p l}{\\p \\bfF}\\right)^\\top$$\n",
    "\n",
    "Hence,\n",
    "$$ \\frac{\\p }{\\p \\bfA}l(\\bfF(\\bfA, \\bfB)) = \\frac{\\p l}{\\p \\bfF}\\bfB^\\top $$\n",
    "\n",
    "\n",
    "The vector Jacobian product for $\\bfB$ can be found by applying the above rule to $\\bfF_2(\\bfA, \\bfC) = \\bfF^\\top(\\bfA, \\bfB) = \\bfB^\\top \\bfA^\\top = \\bfC \\bfA^\\top$ where $\\bfC = \\bfB^\\top$ and $\\bfF_2 = \\bfF^\\top$.\n",
    "\n",
    "$$ \\frac{\\p }{\\p \\bfC}l(\\bfF_2(\\bfA, \\bfC)) = \\frac{\\p l}{\\p \\bfF_2}\\bfA $$\n",
    "\n",
    "Take transpose of both sides\n",
    "$$ \\frac{\\p }{\\p \\bfC^\\top}l(\\bfF_2^\\top(\\bfA, \\bfC)) = \\bfA^\\top\\frac{\\p l}{\\p \\bfF_2^\\top} $$\n",
    "\n",
    "Put back, $\\bfC = \\bfB^\\top$ and $\\bfF_2 = \\bfF^\\top$,\n",
    "$$ \\frac{\\p }{\\p \\bfB}l(\\bfF(\\bfA, \\bfB)) = \\bfA^\\top\\frac{\\p l}{\\p \\bfF} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7953b-d59c-4877-a52f-388f1653b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_vjp(dldF, A, B):\n",
    "    G = dldF\n",
    "    if G.ndim == 0:\n",
    "        # Case 1: vector-vector multiplication\n",
    "        assert A.ndim == 1 and B.ndim == 1\n",
    "        dldA = G*B\n",
    "        dldB = G*A\n",
    "        return (unbroadcast(A, dldA),\n",
    "                unbroadcast(B, dldB))\n",
    "    \n",
    "    assert not (A.ndim == 1 and B.ndim == 1)\n",
    "\n",
    "    # 1. If both arguments are 2-D they are multiplied like conventional matrices.\n",
    "    # 2. If either argument is N-D, N > 2, it is treated as a stack of matrices \n",
    "    # residing in the last two indexes and broadcast accordingly.\n",
    "    if A.ndim >= 2 and B.ndim >= 2:\n",
    "        dldA = G @ B.swapaxes(-2, -1)\n",
    "        dldB = A.swapaxes(-2, -1) @ G\n",
    "    if A.ndim == 1:\n",
    "        # 3. If the first argument is 1-D, it is promoted to a matrix by prepending a\n",
    "        #    1 to its dimensions. After matrix multiplication the prepended 1 is removed.\n",
    "        A_ = A[np.newaxis, :]\n",
    "        G_ = G[np.newaxis, :]\n",
    "        dldA = G @ B.swapaxes(-2, -1) \n",
    "        dldB = A_.swapaxes(-2, -1) @ G_ # outer product\n",
    "    elif B.ndim == 1:\n",
    "        # 4. If the second argument is 1-D, it is promoted to a matrix by appending \n",
    "        #    a 1 to its dimensions. After matrix multiplication the appended 1 is removed.\n",
    "        B_ = B[:, np.newaxis]\n",
    "        G_ = G[:, np.newaxis]\n",
    "        dldA = G_ @ B_.swapaxes(-2, -1) # outer product\n",
    "        dldB = A.swapaxes(-2, -1) @ G\n",
    "    return (unbroadcast(A, dldA), \n",
    "            unbroadcast(B, dldB))\n",
    "        \n",
    "\n",
    "matmul = Op(\n",
    "    apply=np.matmul,\n",
    "    vjp=matmul_vjp,\n",
    "    name='@',\n",
    "    nargs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47606608-c165-459b-b6a6-3bda378de5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_vjp(dldf, x):\n",
    "    dldx = dldf * np.exp(x)\n",
    "    return (unbroadcast(x, dldx),)\n",
    "exp = Op(\n",
    "    apply=np.exp,\n",
    "    vjp=exp_vjp,\n",
    "    name='exp',\n",
    "    nargs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ec346-2524-44a1-9a2a-56a4db710369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_vjp(dldf, x):\n",
    "    dldx = dldf / x\n",
    "    return (unbroadcast(x, dldx),)\n",
    "log = Op(\n",
    "    apply=np.log,\n",
    "    vjp=log_vjp,\n",
    "    name='log',\n",
    "    nargs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192c432-d8f9-4ffe-9853-abdf1adb81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_vjp(dldf, x, axis=None, **kwargs):\n",
    "    if axis is not None:\n",
    "        dldx = np.expand_dims(dldf, axis=axis) * np.ones_like(x)\n",
    "    else:\n",
    "        dldx = dldf * np.ones_like(x)\n",
    "    return (unbroadcast(x, dldx),)\n",
    "\n",
    "sum_ = Op(\n",
    "    apply=np.sum,\n",
    "    vjp=sum_vjp,\n",
    "    name='sum',\n",
    "    nargs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a608920-289a-484c-9f94-587d37775ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_vjp(dldf, a, b):\n",
    "    dlda = dldf * np.where(a > b, 1, 0)\n",
    "    dldb = dldf * np.where(a > b, 0, 1)\n",
    "    return unbroadcast(a, dlda), unbroadcast(b, dldb)\n",
    "\n",
    "maximum = Op(\n",
    "    apply=np.maximum,\n",
    "    vjp=maximum_vjp,\n",
    "    name='maximum',\n",
    "    nargs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed2b3d8-ef4e-407a-9ff6-c04d057d14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NoOp = Op(apply=None, name='', vjp=None, nargs=0)\n",
    "class Tensor:\n",
    "    __array_priority__ = 100\n",
    "    def __init__(self, value, grad=None, parents=(), op=NoOp, kwargs={}, requires_grad=True):\n",
    "        self.value = np.asarray(value)\n",
    "        self.grad = grad\n",
    "        self.parents = parents\n",
    "        self.op = op\n",
    "        self.kwargs = kwargs\n",
    "        self.requires_grad = requires_grad\n",
    "    \n",
    "    shape = property(lambda self: self.value.shape)\n",
    "    ndim  = property(lambda self: self.value.ndim)\n",
    "    size  = property(lambda self: self.value.size)\n",
    "    dtype = property(lambda self: self.value.dtype)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        cls = type(self)\n",
    "        other = other if isinstance(other, cls) else cls(other)\n",
    "        return cls(add.apply(self.value, other.value),\n",
    "                   parents=(self, other),\n",
    "                   op=add)\n",
    "    __radd__ = __add__\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        cls = type(self)\n",
    "        other = other if isinstance(other, cls) else cls(other)\n",
    "        return cls(mul.apply(self.value, other.value),\n",
    "                   parents=(self, other),\n",
    "                   op=mul)\n",
    "    __rmul__ = __mul__\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        cls = type(self)\n",
    "        other = other if isinstance(other, cls) else cls(other)\n",
    "        return cls(matmul.apply(self.value, other.value),\n",
    "                  parents=(self, other),\n",
    "                  op=matmul)\n",
    "    \n",
    "    def exp(self):\n",
    "        cls = type(self)\n",
    "        return cls(exp.apply(self.value),\n",
    "                parents=(self,),\n",
    "                op=exp)\n",
    "    \n",
    "    def log(self):\n",
    "        cls = type(self)\n",
    "        return cls(log.apply(self.value),\n",
    "                parents=(self, ),\n",
    "                op=log)\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        cls = type(self)\n",
    "        other = other if isinstance(other, cls) else cls(other)\n",
    "        return (self.log() * other).exp()\n",
    "    \n",
    "    def __div__(self, other):\n",
    "        return self * (other**(-1))\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (other * (-1))\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self*(-1)\n",
    "    \n",
    "    def sum(self, axis=None):\n",
    "        cls = type(self)\n",
    "        return cls(sum_.apply(self.value, axis=axis),\n",
    "                   parents=(self,),\n",
    "                   op=sum_,\n",
    "                   kwargs=dict(axis=axis))\n",
    "\n",
    "    def maximum(self, other):\n",
    "        cls = type(self)\n",
    "        other = other if isinstance(other, cls) else cls(other)\n",
    "        return cls(maximum.apply(self.value, other.value),\n",
    "                   parents=(self, other),\n",
    "                   op=maximum)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        cls = type(self)\n",
    "        return f\"{cls.__name__}(value={self.value}, op={self.op.name})\" if self.parents else f\"{cls.__name__}(value={self.value})\"\n",
    "        #return f\"{cls.__name__}(value={self.value}, parents={self.parents}, op={self.op}\"\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        self.grad = grad if self.grad is None else (self.grad+grad)\n",
    "        if self.requires_grad and self.parents:\n",
    "            p_vals = [p.value for p in self.parents]\n",
    "            assert len(p_vals) == self.op.nargs\n",
    "            p_grads = self.op.vjp(grad, *p_vals, **self.kwargs)\n",
    "            for p, g in zip(self.parents, p_grads):\n",
    "                p.backward(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b842c-e6b2-4f6c-bfa8-30bc4a6e44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor([1, 2]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58255827-ca28-4f20-96e1-2ff13841ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "except ImportError as e:\n",
    "    import subprocess\n",
    "    subprocess.call(\"pip install --user graphviz\".split())\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for p in v.parents:\n",
    "                edges.add((p, v))\n",
    "                build(p)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        vstr = np.array2string(np.asarray(n.value), precision=4)\n",
    "        gradstr= np.array2string(np.asarray(n.grad), precision=4)\n",
    "        dot.node(name=str(id(n)), label = f\"{{v={vstr} | g={gradstr}}}\", shape='record')\n",
    "        if n.parents:\n",
    "            dot.node(name=str(id(n)) + n.op.name, label=n.op.name)\n",
    "            dot.edge(str(id(n)) + n.op.name, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2.op.name)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d75c1b-9f27-4e29-9234-381ca63c2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very simple example\n",
    "x = Tensor([[1.0, 2.0],\n",
    "            [2.0, -1.0]])\n",
    "y = (x * 2 - 1).maximum(0).sum(axis=-1)\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ad733-0c8c-4bbd-a119-441ccab34161",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(np.ones_like(y))\n",
    "draw_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5940f5-711e-4fd9-9e01-0deca7a01247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_np(x):\n",
    "    b = [1, 0]\n",
    "    return (x @ b)*np.exp((-x*x).sum(axis=-1))\n",
    "\n",
    "def f_T(x):\n",
    "    b = [1, 0]\n",
    "    return (x @ b)*(-x*x).sum(axis=-1).exp()\n",
    "\n",
    "def grad_f(x):\n",
    "    xT = Tensor(x)\n",
    "    y = f_T(xT)\n",
    "    y.backward(np.ones_like(y.value))\n",
    "    return xT.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f7b0b-66c7-45a2-b088-7ebce07d84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xT = Tensor([1, 2])\n",
    "out = f_T(xT)\n",
    "out.backward(1)\n",
    "print(xT.grad)\n",
    "draw_dot(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f016010-9e21-4e43-ac28-8f68608f3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_jacobian(f, x, h=1e-10):\n",
    "    n = x.shape[-1]\n",
    "    eye = np.eye(n)\n",
    "    x_plus_dx = x + h * eye # n x n\n",
    "    num_jac = (f(x_plus_dx) - f(x)) / h # limit definition of the formula # n x m\n",
    "    if num_jac.ndim >= 2:\n",
    "        num_jac = num_jac.swapaxes(-1, -2) # m x n\n",
    "    return num_jac\n",
    "\n",
    "# Compare our grad_f with numerical gradient\n",
    "def check_numerical_jacobian(f, jac_f,  nD=2, **kwargs):\n",
    "    x = np.random.rand(nD)\n",
    "    print(x)\n",
    "    num_jac = numerical_jacobian(f, x, **kwargs)\n",
    "    print(num_jac)\n",
    "    print(jac_f(x))\n",
    "    return np.allclose(num_jac, jac_f(x), atol=1e-06, rtol=1e-4) # m x n\n",
    "\n",
    "## Throw error if grad_f is wrong\n",
    "assert check_numerical_jacobian(f_np, grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f9cab-6e88-4ad1-959e-d598b9475c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb99fe-9658-4ab0-92e2-969ba95960e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
